{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6be9dbc-d935-4f79-831e-82ce2e8edf6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f4d76e-dc74-403b-b43b-60e135942006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526fff5f-812b-493f-8e56-71b35f85a4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "# Initialiser DBUtils\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Récupérer les secrets depuis le scope\n",
    "user = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"user\")\n",
    "password = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"password\")\n",
    "account = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"account\")\n",
    "warehouse = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"warehouse\")\n",
    "database = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"database\")\n",
    "schema = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"schema\")\n",
    "role = dbutils.secrets.get(scope=\"my_snowflake_scope\", key=\"role\")\n",
    "\n",
    "# Connexion à Snowflake\n",
    "connection = snowflake.connector.connect(\n",
    "    user=user,\n",
    "    password=password,\n",
    "    account=account,\n",
    "    warehouse=warehouse,\n",
    "    database=database,\n",
    "    schema=schema,\n",
    "    role=role\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2046fa33-6495-4082-9023-bfd6cc165324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "# Initialiser DBUtils\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Récupérer les secrets depuis le scope\n",
    "ADLS_STORAGE_ACCOUNT_NAME = dbutils.secrets.get(scope=\"adls_scope\", key=\"storage_account_name\")\n",
    "ADLS_ACCOUNT_KEY = dbutils.secrets.get(scope=\"adls_scope\", key=\"account_key\")\n",
    "ADLS_CONTAINER_NAME = dbutils.secrets.get(scope=\"adls_scope\", key=\"container_name\")\n",
    "ADLS_FOLDER_PATH = dbutils.secrets.get(scope=\"adls_scope\", key=\"folder_path\")\n",
    "\n",
    "# Construire l'URL Delta Source\n",
    "DELTA_SOURCE = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65284846-d711-4021-87e5-afebb478f4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define checkpoint file path\n",
    "CHECKPOINT_FILE_PATH = \"./delta_checkpoint.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62913b91-0520-4d83-9477-c924bb795f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    ADLS_ACCOUNT_KEY,\n",
    ")\n",
    "spark.conf.set(\"spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472cc852-5259-4b7a-a820-cd8c0cbdd441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Latest timtestamp from delta history: \n"
     ]
    }
   ],
   "source": [
    "# Read the checkpoint file if it exists, otherwise set the LAST_READ_TIMESTAMP to 0\n",
    "delta_table = DeltaTable.forPath(spark, DELTA_SOURCE)\n",
    "try:\n",
    "    with open(CHECKPOINT_FILE_PATH, \"r\") as checkpoint_file:\n",
    "        LAST_READ_TIMESTAMP = checkpoint_file.read()\n",
    "    print(\"Getting Latest timtestamp from checkpoint file: \")\n",
    "except FileNotFoundError:\n",
    "    print(\"Getting Latest timtestamp from delta history: \")\n",
    "    LAST_READ_TIMESTAMP = (\n",
    "        delta_table.history().select(F.min(\"timestamp\").alias(\"timestamp\")).collect()\n",
    "    )\n",
    "    LAST_READ_TIMESTAMP = str(LAST_READ_TIMESTAMP[0][\"timestamp\"])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8f4723-77c6-4943-a878-36fec446b955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST_READ_TIMESTAMP:  2024-12-03 16:27:48\n"
     ]
    }
   ],
   "source": [
    "print(\"LAST_READ_TIMESTAMP: \", LAST_READ_TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d345cbd7-0ebc-408f-ab5e-c30eef9325a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_data(connection, competences, contrat_nom, societe_nom, formation_nom, experience_dur, salaire, type_de_trav):\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Insérer les compétences dans la table, en laissant l'auto-incrémentation gérer l'ID\n",
    "        for competence_nom in competences:\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO my_project_schema.competence (NOM)\n",
    "                SELECT '{competence_nom}'\n",
    "                WHERE NOT EXISTS (\n",
    "                    SELECT 1 FROM my_project_schema.competence WHERE nom = '{competence_nom}'\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "        # Insérer les autres entités (contrat, société, formation, etc.)\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO my_project_schema.contrat (contrat_nom)\n",
    "            SELECT '{contrat_nom}'\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM my_project_schema.contrat WHERE contrat_nom = '{contrat_nom}'\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO my_project_schema.entreprise (nom)\n",
    "            SELECT '{societe_nom}'\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM my_project_schema.entreprise WHERE nom = '{societe_nom}'\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO my_project_schema.formation (nom)\n",
    "            SELECT '{formation_nom}'\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM my_project_schema.formation WHERE nom = '{formation_nom}'\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # Récupérer les IDs des entités\n",
    "        cursor.execute(f\"SELECT entreprise_id FROM my_project_schema.entreprise WHERE nom = '{societe_nom}'\")\n",
    "        entreprise_id = cursor.fetchone()[0]\n",
    "\n",
    "        cursor.execute(f\"SELECT formation_id FROM my_project_schema.formation WHERE nom = '{formation_nom}'\")\n",
    "        formation_id = cursor.fetchone()[0]\n",
    "\n",
    "        cursor.execute(f\"SELECT contrat_id FROM my_project_schema.contrat WHERE contrat_nom = '{contrat_nom}'\")\n",
    "        contrat_id = cursor.fetchone()[0]\n",
    "\n",
    "        cursor.execute(f\"SELECT TYPETRAV_ID FROM my_project_schema.type_trav WHERE nom = '{type_de_trav}'\")\n",
    "        type_trav_id = cursor.fetchone()[0]\n",
    "\n",
    "        # Insérer l'offre\n",
    "        query = \"\"\"\n",
    "            INSERT INTO my_project_schema.offre_fait (\n",
    "                entreprise_id, experience_dur, formation_id, contrat_id, salaire, typetrav_id\n",
    "            )\n",
    "            SELECT %s, %s, %s, %s, %s, %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (entreprise_id, experience_dur, formation_id, contrat_id, salaire, type_trav_id))\n",
    "\n",
    "        # Récupérer l'ID de l'offre insérée (l'ID le plus récent)\n",
    "        cursor.execute(\"SELECT COALESCE(MAX(OFFRE_ID), 0) FROM my_project_schema.offre_fait\")\n",
    "        offre_id = cursor.fetchone()[0]\n",
    "\n",
    "        # Insérer les associations entre l'offre et les compétences\n",
    "        for competence_nom in competences:\n",
    "            # Récupérer l'ID de la compétence\n",
    "            cursor.execute(f\"SELECT competence_id FROM my_project_schema.competence WHERE nom = '{competence_nom}'\")\n",
    "            competence_id = cursor.fetchone()[0]\n",
    "\n",
    "            # Insérer l'association entre l'offre et la compétence\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO my_project_schema.offre_competence (offre_id, competence_id)\n",
    "                SELECT {offre_id}, {competence_id}\n",
    "                WHERE NOT EXISTS (\n",
    "                    SELECT 1 FROM my_project_schema.offre_comp WHERE offre_id = {offre_id} AND competence_id = {competence_id}\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "        # Commit des modifications\n",
    "        connection.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'insertion des données : {e}\")\n",
    "        connection.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_langue_association(connection, langues):\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Récupérer l'ID de l'offre la plus récemment insérée\n",
    "        cursor.execute(\"SELECT COALESCE(MAX(OFFRE_ID), 0) FROM my_project_schema.offre_fait\")\n",
    "        offre_id = cursor.fetchone()[0]\n",
    "\n",
    "        # Insérer les associations des langues avec l'offre\n",
    "        for langue_nom in langues:\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT langue_id FROM my_project_schema.langue WHERE nom = '{langue_nom}'\n",
    "            \"\"\")\n",
    "            langue_id = cursor.fetchone()[0]\n",
    "\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO my_project_schema.langue_offr (offre_id, langue_id)\n",
    "                SELECT {offre_id}, {langue_id}\n",
    "                WHERE NOT EXISTS (\n",
    "                    SELECT 1 FROM my_project_schema.langue_offr WHERE offre_id = {offre_id} AND langue_id = {langue_id}\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "        # Commit des modifications\n",
    "        connection.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'insertion des associations : {e}\")\n",
    "        connection.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc08709a-0d58-4d90-a31a-dd49cfd333ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_delta_changes():\n",
    "    delta_df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .option(\"startingTimestamp\", LAST_READ_TIMESTAMP)\n",
    "        .load(DELTA_SOURCE)\n",
    "    )\n",
    "\n",
    "    if delta_df.count() != 0:\n",
    "        excluded_columns = [\"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "        selected_columns = [\n",
    "            column for column in delta_df.columns if column not in excluded_columns\n",
    "        ]\n",
    "        print(f\"Colonnes sélectionnées : {selected_columns}\")\n",
    "        \n",
    "        # Filtrer les colonnes nécessaires et convertir en Pandas DataFrame\n",
    "        pandas_df = delta_df.select(*selected_columns).toPandas()\n",
    "        for index, row in pandas_df.iterrows():\n",
    "    # Assurez-vous que les noms de colonnes existent dans le DataFrame\n",
    "          # Extraire les paramètres de la ligne\n",
    "         competences = row['competences']\n",
    "         experience_demande = row['experience_demande']\n",
    "         formation_requise = row['formation_requise']\n",
    "         type_de_contrat = row['type_de_contrat']\n",
    "         type_offre = row['type']\n",
    "         salaire = row['salaire']\n",
    "         societe = row['societe']\n",
    "\n",
    "    # Imprimer les paramètres\n",
    "         print(\"Données à insérer :\")\n",
    "         print(f\"Compétences: {competences}, Expérience demandée: {experience_demande}, Formation requise: {formation_requise}, \"\n",
    "          f\"Type de contrat: {type_de_contrat}, Type d'offre: {type_offre}, Salaire: {salaire}, Société: {societe}\")\n",
    "         if row['experience_demande']==None:\n",
    "             row['experience_demande']=0\n",
    "       \n",
    "             \n",
    "    \n",
    "         insert_data(connection,row['competences'],row['type_de_contrat'],row['societe'],\n",
    "                     row['formation_requise'], , row['experience_demande'],row['salaire'],row[ 'type'])\n",
    "         insert_langue_association(connection ,row[\"langues\"])\n",
    "         updated_timestamp = delta_table.history().select(F.max(\"timestamp\").alias(\"timestamp\")).collect()[0][\"timestamp\"]\n",
    "         updated_timestamp += datetime.timedelta(seconds=1)\n",
    "         updated_timestamp = str(updated_timestamp)\n",
    "         print(\"Updated timestamp: \", updated_timestamp)\n",
    "         with open(CHECKPOINT_FILE_PATH, \"w\") as file:\n",
    "            file.write(updated_timestamp)\n",
    "\n",
    "    else:\n",
    "        print(\"Aucune nouvelle donnée dans les fichiers Delta.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfca174b-91bc-4076-be5e-f841a9d47531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes sélectionnées : ['titre_du_poste', 'societe', 'competences', 'lieu', 'type_offre', 'durée', 'type_de_contrat', 'email', 'telephone', 'type', 'langues', 'salaire', 'date_de_debut', 'secteur_dactivite', 'experience_demande', 'formation_requise', 'avantages', 'site_web']\n",
      "Données à insérer :\n",
      "Compétences: ['Python' 'SQL' 'Apache Airflow' 'Talend' 'MongoDB' 'Cassandra' 'AWS'\n",
      " 'Google Cloud'], Expérience demandée: None, Formation requise: Étudiant en informatique ou domaine connexe, Type de contrat: Stage PFE, Type d'offre: stage, Salaire: 1 200 EUR par mois, Société: DataTech Solutions\n",
      "Erreur lors de l'insertion des données : 100038 (22018): Numeric value '1 200 EUR par mois' is not recognized\n"
     ]
    }
   ],
   "source": [
    "display_delta_changes()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook sans titre 2024-12-02 16:44:01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
