[
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "concurrent.futures",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "config",
        "description": "config",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "psycopg2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psycopg2",
        "description": "psycopg2",
        "detail": "psycopg2",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "udf",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "BinaryType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "data.config",
        "description": "data.config",
        "peekOfCode": "topic = \"distributed-video1\"\nend_topic = \"final_result_topic\"",
        "detail": "data.config",
        "documentation": {}
    },
    {
        "label": "end_topic",
        "kind": 5,
        "importPath": "data.config",
        "description": "data.config",
        "peekOfCode": "end_topic = \"final_result_topic\"",
        "detail": "data.config",
        "documentation": {}
    },
    {
        "label": "process_frame",
        "kind": 2,
        "importPath": "data.producer",
        "description": "data.producer",
        "peekOfCode": "def process_frame(producer, topic, frame):\n    # Encodage de l'image en format .jpg\n    ret, buffer = cv2.imencode('.jpg', frame)\n    # Si l'encodage est réussi, on envoie l'image encodée au sujet Kafka\n    if ret:\n        producer.send(topic, buffer.tobytes())\n# Fonction qui lit et publie les trames vidéo dans un sujet Kafka\ndef publish_video(producer, topic, video_file=\"match_endirect.mp4\"):\n    # Ouverture du fichier vidéo\n    video = cv2.VideoCapture(video_file)",
        "detail": "data.producer",
        "documentation": {}
    },
    {
        "label": "publish_video",
        "kind": 2,
        "importPath": "data.producer",
        "description": "data.producer",
        "peekOfCode": "def publish_video(producer, topic, video_file=\"match_endirect.mp4\"):\n    # Ouverture du fichier vidéo\n    video = cv2.VideoCapture(video_file)\n    print('Publishing video...')\n    # Récupérer le nombre d'images par seconde (FPS) de la vidéo\n    fps = video.get(cv2.CAP_PROP_FPS)\n    # Calculer le délai à appliquer entre chaque trame pour respecter le taux de FPS\n    delay = 1 / fps\n    # Utilisation d'un pool de threads pour traiter plusieurs trames en parallèle\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:",
        "detail": "data.producer",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.competition",
        "description": "database.tables.remplissage_database.competition",
        "peekOfCode": "def insert_data(match):\n    try:\n        # Insérer les données de la compétition\n        cursor.execute('''\n            INSERT INTO competitions (competition_id, country_name, competition_name)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (competition_id) DO NOTHING\n        ''', (match['competition']['competition_id'], match['competition']['country_name'], match['competition']['competition_name']))\n        print(\"Données de la compétition insérées.\")\n    except Exception as e:",
        "detail": "database.tables.remplissage_database.competition",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.competition",
        "description": "database.tables.remplissage_database.competition",
        "peekOfCode": "conn = psycopg2.connect(\n    dbname='football',\n    user='postgres',\n    password='papapapa',\n    host='localhost',\n    port='5432'\n)\ncursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match):",
        "detail": "database.tables.remplissage_database.competition",
        "documentation": {}
    },
    {
        "label": "cursor",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.competition",
        "description": "database.tables.remplissage_database.competition",
        "peekOfCode": "cursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match):\n    try:\n        # Insérer les données de la compétition\n        cursor.execute('''\n            INSERT INTO competitions (competition_id, country_name, competition_name)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (competition_id) DO NOTHING\n        ''', (match['competition']['competition_id'], match['competition']['country_name'], match['competition']['competition_name']))",
        "detail": "database.tables.remplissage_database.competition",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.competition",
        "description": "database.tables.remplissage_database.competition",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)\n                    # Vérifiez si les données sont dans une liste, sinon créez une liste avec un seul élément",
        "detail": "database.tables.remplissage_database.competition",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.managers",
        "description": "database.tables.remplissage_database.managers",
        "peekOfCode": "def insert_data(match, folder_name):\n    try:\n                # Insérer les données du manager\n        cursor.execute('''\n            INSERT INTO managers (id, name, nickname, dob, country_id)\n            VALUES (%s, %s, %s, %s, %s)\n            ON CONFLICT (id) DO NOTHING\n        ''', (match['home_team']['managers'][0]['id'], match['home_team']['managers'][0]['name'], match['home_team']['managers'][0]['nickname'], match['home_team']['managers'][0]['dob'], match['home_team']['managers'][0]['country']['id']))\n        print(\"Données du manager insérées.\")\n    except KeyError as e:",
        "detail": "database.tables.remplissage_database.managers",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.managers",
        "description": "database.tables.remplissage_database.managers",
        "peekOfCode": "conn = psycopg2.connect(\n    dbname='football',\n    user='postgres',\n    password='papapapa',\n    host='localhost',\n    port='5432'\n)\ncursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match, folder_name):",
        "detail": "database.tables.remplissage_database.managers",
        "documentation": {}
    },
    {
        "label": "cursor",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.managers",
        "description": "database.tables.remplissage_database.managers",
        "peekOfCode": "cursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match, folder_name):\n    try:\n                # Insérer les données du manager\n        cursor.execute('''\n            INSERT INTO managers (id, name, nickname, dob, country_id)\n            VALUES (%s, %s, %s, %s, %s)\n            ON CONFLICT (id) DO NOTHING\n        ''', (match['home_team']['managers'][0]['id'], match['home_team']['managers'][0]['name'], match['home_team']['managers'][0]['nickname'], match['home_team']['managers'][0]['dob'], match['home_team']['managers'][0]['country']['id']))",
        "detail": "database.tables.remplissage_database.managers",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.managers",
        "description": "database.tables.remplissage_database.managers",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    folder_name = os.path.basename(subdir)  # Récupérer le nom du dossier actuel\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)",
        "detail": "database.tables.remplissage_database.managers",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.match",
        "description": "database.tables.remplissage_database.match",
        "peekOfCode": "def insert_data(match, folder_name):\n    try:\n        # Insérer les données du match, en utilisant .get() pour fournir 'None' si la clé n'existe pas\n        cursor.execute('''\n            INSERT INTO matches (match_id, match_date, kick_off, competition_id, season_id, home_team_id, away_team_id, home_score, away_score, match_status, match_status_360, match_week, stadium_id, competition_stage_id, referee_id)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (match_id) DO NOTHING\n        ''', (\n            match.get('match_id', None),\n            match.get('match_date', None),",
        "detail": "database.tables.remplissage_database.match",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.match",
        "description": "database.tables.remplissage_database.match",
        "peekOfCode": "conn = psycopg2.connect(\n    dbname='football',\n    user='postgres',\n    password='papapapa',\n    host='localhost',\n    port='5432'\n)\ncursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match, folder_name):",
        "detail": "database.tables.remplissage_database.match",
        "documentation": {}
    },
    {
        "label": "cursor",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.match",
        "description": "database.tables.remplissage_database.match",
        "peekOfCode": "cursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match, folder_name):\n    try:\n        # Insérer les données du match, en utilisant .get() pour fournir 'None' si la clé n'existe pas\n        cursor.execute('''\n            INSERT INTO matches (match_id, match_date, kick_off, competition_id, season_id, home_team_id, away_team_id, home_score, away_score, match_status, match_status_360, match_week, stadium_id, competition_stage_id, referee_id)\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (match_id) DO NOTHING\n        ''', (",
        "detail": "database.tables.remplissage_database.match",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.match",
        "description": "database.tables.remplissage_database.match",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    folder_name = os.path.basename(subdir)  # Récupérer le nom du dossier actuel\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)",
        "detail": "database.tables.remplissage_database.match",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.ref",
        "description": "database.tables.remplissage_database.ref",
        "peekOfCode": "def insert_data(match, folder_name):\n    try:\n        if \"referee\" in match:\n         cursor.execute('''\n            INSERT INTO referee (referee_id, referee_name, country_id)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (referee_id) DO NOTHING\n        ''', (match['referee']['id'], match['referee']['name'], match['referee']['country']['id']))\n         print(\"Données de l'arbitre insérées.\")\n        # Insérer les données de l'arbitre",
        "detail": "database.tables.remplissage_database.ref",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.ref",
        "description": "database.tables.remplissage_database.ref",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)\n                    # Vérifiez si les données sont dans une liste, sinon créez une liste avec un seul élément",
        "detail": "database.tables.remplissage_database.ref",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.season_country",
        "description": "database.tables.remplissage_database.season_country",
        "peekOfCode": "def insert_data(match):\n    try:\n        # Insérer les données du pays\n        cursor.execute('''\n            INSERT INTO countries (country_id, name)\n            VALUES (%s, %s)\n            ON CONFLICT (country_id) DO NOTHING\n        ''', (match['home_team']['country']['id'], match['home_team']['country']['name']))\n        # Insérer les données de la saison\n        cursor.execute('''",
        "detail": "database.tables.remplissage_database.season_country",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.season_country",
        "description": "database.tables.remplissage_database.season_country",
        "peekOfCode": "conn = psycopg2.connect(\n    dbname='football',\n    user='postgres',\n    password='papapapa',\n    host='localhost',\n    port='5432'\n)\ncursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match):",
        "detail": "database.tables.remplissage_database.season_country",
        "documentation": {}
    },
    {
        "label": "cursor",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.season_country",
        "description": "database.tables.remplissage_database.season_country",
        "peekOfCode": "cursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match):\n    try:\n        # Insérer les données du pays\n        cursor.execute('''\n            INSERT INTO countries (country_id, name)\n            VALUES (%s, %s)\n            ON CONFLICT (country_id) DO NOTHING\n        ''', (match['home_team']['country']['id'], match['home_team']['country']['name']))",
        "detail": "database.tables.remplissage_database.season_country",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.season_country",
        "description": "database.tables.remplissage_database.season_country",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)\n                    # Vérifiez si les données sont dans une liste, sinon créez une liste avec un seul élément",
        "detail": "database.tables.remplissage_database.season_country",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.stadium",
        "description": "database.tables.remplissage_database.stadium",
        "peekOfCode": "def insert_data(match, folder_name):\n    try:\n        if \"stadium\" in match:        \n            cursor.execute('''\n            INSERT INTO stadiums (stadium_id, stadium_name, country_id)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (stadium_id) DO NOTHING\n        ''', (match['stadium']['id'], match['stadium']['name'], match['stadium']['country']['id']))\n            print(\"Données du stade insérées.\")\n    except psycopg2.Error as e:",
        "detail": "database.tables.remplissage_database.stadium",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.stadium",
        "description": "database.tables.remplissage_database.stadium",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)\n                    # Vérifiez si les données sont dans une liste, sinon créez une liste avec un seul élément",
        "detail": "database.tables.remplissage_database.stadium",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.stage",
        "description": "database.tables.remplissage_database.stage",
        "peekOfCode": "def insert_data(match, folder_name):\n    try:# Insérer les données du match\n        cursor.execute('''\n            INSERT INTO competition_stage (id, name)\n            VALUES (%s, %s)\n            ON CONFLICT (id) DO NOTHING\n        ''', (match['competition_stage']['id'], match['competition_stage'][\"name\"]))\n    except KeyError as e:\n        print(f\"Erreur lors de l'insertion des données pour le match {match['match_id']} dans le dossier {folder_name}: clé manquante {e}\")\n        conn.rollback()  # Annuler la transaction",
        "detail": "database.tables.remplissage_database.stage",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.stage",
        "description": "database.tables.remplissage_database.stage",
        "peekOfCode": "conn = psycopg2.connect(\n    dbname='football',\n    user='postgres',\n    password='papapapa',\n    host='localhost',\n    port='5432'\n)\ncursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match, folder_name):",
        "detail": "database.tables.remplissage_database.stage",
        "documentation": {}
    },
    {
        "label": "cursor",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.stage",
        "description": "database.tables.remplissage_database.stage",
        "peekOfCode": "cursor = conn.cursor()\n# Fonction pour insérer les données dans la base de données\ndef insert_data(match, folder_name):\n    try:# Insérer les données du match\n        cursor.execute('''\n            INSERT INTO competition_stage (id, name)\n            VALUES (%s, %s)\n            ON CONFLICT (id) DO NOTHING\n        ''', (match['competition_stage']['id'], match['competition_stage'][\"name\"]))\n    except KeyError as e:",
        "detail": "database.tables.remplissage_database.stage",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.stage",
        "description": "database.tables.remplissage_database.stage",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    folder_name = os.path.basename(subdir)  # Récupérer le nom du dossier actuel\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)",
        "detail": "database.tables.remplissage_database.stage",
        "documentation": {}
    },
    {
        "label": "insert_data",
        "kind": 2,
        "importPath": "database.tables.remplissage_database.team",
        "description": "database.tables.remplissage_database.team",
        "peekOfCode": "def insert_data(match, folder_name):\n    try:\n        # Vérifiez si 'away_team' existe dans le match\n        if 'away_team' in match:\n            cursor.execute('''\n                INSERT INTO teams (team_id, team_name, team_gender, country_id)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (team_id) DO NOTHING\n            ''', (\n                match['away_team']['away_team_id'], ",
        "detail": "database.tables.remplissage_database.team",
        "documentation": {}
    },
    {
        "label": "main_folder",
        "kind": 5,
        "importPath": "database.tables.remplissage_database.team",
        "description": "database.tables.remplissage_database.team",
        "peekOfCode": "main_folder = r'C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\matches'\n# Parcours des sous-dossiers et des fichiers JSON\nfor subdir, dirs, files in os.walk(main_folder):\n    for file in files:\n        if file.endswith('.json'):\n            file_path = os.path.join(subdir, file)\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                try:\n                    data = json.load(json_file)\n                    # Vérifiez si les données sont dans une liste, sinon créez une liste avec un seul élément",
        "detail": "database.tables.remplissage_database.team",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "spark_streaming.config",
        "description": "spark_streaming.config",
        "peekOfCode": "topic = \"distributed-video1\"\nend_topic = \"final_result_topic\"",
        "detail": "spark_streaming.config",
        "documentation": {}
    },
    {
        "label": "end_topic",
        "kind": 5,
        "importPath": "spark_streaming.config",
        "description": "spark_streaming.config",
        "peekOfCode": "end_topic = \"final_result_topic\"",
        "detail": "spark_streaming.config",
        "documentation": {}
    },
    {
        "label": "process_image_cons",
        "kind": 2,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "def process_image_cons(image_bytes):\n    np_array = np.frombuffer(image_bytes, np.uint8)\n    frame = cv2.imdecode(np_array, cv2.IMREAD_COLOR)\n    if frame is not None:\n        # Obtenir les dimensions de l'image\n        height, width, _ = frame.shape\n        # Définir les coordonnées du rectangle (ici, centré dans l'image)\n        start_point = (width // 4, height // 4)  # Coin supérieur gauche\n        end_point = (width * 3 // 4, height * 3 // 4)  # Coin inférieur droit\n        # Dessiner le rectangle sur l'image",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "process_batch",
        "kind": 2,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "def process_batch(df, epoch_id):\n    print(f\"Traitement du micro-lot {epoch_id}\")\n    for row in df.collect():\n        if row['img_bytes']:  # Vérifiez que les octets de l'image ne sont pas vides\n            # Générer un timestamp pour le nom de fichier\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")  # Format : YYYYMMDD_HHMMSS_microsecond\n            filename = os.path.join(output_folder, f\"{timestamp}.jpg\")  # Créer le nom de fichier avec le timestamp\n            with open(filename, 'wb') as f:\n                f.write(row['img_bytes'])\n            print(f\"Image sauvegardée : {filename}\")",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "output_folder",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "output_folder = 'images_traited'\nos.makedirs(output_folder, exist_ok=True)\nos.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\nspark = SparkSession.builder \\\n    .appName(\"Kafka Spark Structured Streaming App\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.jars.packages\", \n        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,\" \n        \"org.apache.kafka:kafka-clients:3.4.1,\" \n        \"org.apache.commons:commons-pool2:2.11.1\") \\",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYSPARK_PIN_THREAD\"]",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\nspark = SparkSession.builder \\\n    .appName(\"Kafka Spark Structured Streaming App\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.jars.packages\", \n        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,\" \n        \"org.apache.kafka:kafka-clients:3.4.1,\" \n        \"org.apache.commons:commons-pool2:2.11.1\") \\\n    .getOrCreate()\nif spark:",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "spark = SparkSession.builder \\\n    .appName(\"Kafka Spark Structured Streaming App\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.jars.packages\", \n        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,\" \n        \"org.apache.kafka:kafka-clients:3.4.1,\" \n        \"org.apache.commons:commons-pool2:2.11.1\") \\\n    .getOrCreate()\nif spark:\n    print(\"Session is created\")",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "process_image_udf",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "process_image_udf = udf(process_image_cons, BinaryType())\nkafka_df = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:29092\") \\\n    .option(\"subscribe\", cfg.topic) \\\n    .load()\nkafka_df = kafka_df.selectExpr(\"CAST(value AS BINARY) as img_bytes\")\nprocessed_df = kafka_df.withColumn(\"img_bytes\", process_image_udf(col(\"img_bytes\")))\ndef process_batch(df, epoch_id):\n    print(f\"Traitement du micro-lot {epoch_id}\")",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "kafka_df",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "kafka_df = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:29092\") \\\n    .option(\"subscribe\", cfg.topic) \\\n    .load()\nkafka_df = kafka_df.selectExpr(\"CAST(value AS BINARY) as img_bytes\")\nprocessed_df = kafka_df.withColumn(\"img_bytes\", process_image_udf(col(\"img_bytes\")))\ndef process_batch(df, epoch_id):\n    print(f\"Traitement du micro-lot {epoch_id}\")\n    for row in df.collect():",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "kafka_df",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "kafka_df = kafka_df.selectExpr(\"CAST(value AS BINARY) as img_bytes\")\nprocessed_df = kafka_df.withColumn(\"img_bytes\", process_image_udf(col(\"img_bytes\")))\ndef process_batch(df, epoch_id):\n    print(f\"Traitement du micro-lot {epoch_id}\")\n    for row in df.collect():\n        if row['img_bytes']:  # Vérifiez que les octets de l'image ne sont pas vides\n            # Générer un timestamp pour le nom de fichier\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")  # Format : YYYYMMDD_HHMMSS_microsecond\n            filename = os.path.join(output_folder, f\"{timestamp}.jpg\")  # Créer le nom de fichier avec le timestamp\n            with open(filename, 'wb') as f:",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "processed_df",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "processed_df = kafka_df.withColumn(\"img_bytes\", process_image_udf(col(\"img_bytes\")))\ndef process_batch(df, epoch_id):\n    print(f\"Traitement du micro-lot {epoch_id}\")\n    for row in df.collect():\n        if row['img_bytes']:  # Vérifiez que les octets de l'image ne sont pas vides\n            # Générer un timestamp pour le nom de fichier\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")  # Format : YYYYMMDD_HHMMSS_microsecond\n            filename = os.path.join(output_folder, f\"{timestamp}.jpg\")  # Créer le nom de fichier avec le timestamp\n            with open(filename, 'wb') as f:\n                f.write(row['img_bytes'])",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "spark_streaming.kafka_spark",
        "description": "spark_streaming.kafka_spark",
        "peekOfCode": "query = processed_df.writeStream \\\n    .outputMode(\"append\") \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attendre que le stream soit arrêté\nquery.awaitTermination()",
        "detail": "spark_streaming.kafka_spark",
        "documentation": {}
    }
]