[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "pyspark.sql.functions",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "year",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "month",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "dayofmonth",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "hour",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "udf",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "from_json",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "config",
        "description": "config",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "OFFRE_SCHEMA",
        "importPath": "schema",
        "description": "schema",
        "isExtraImport": true,
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "OFFRE_SCHEMA",
        "importPath": "schema",
        "description": "schema",
        "isExtraImport": true,
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "process_message_groq",
        "importPath": "extractor",
        "description": "extractor",
        "isExtraImport": true,
        "detail": "extractor",
        "documentation": {}
    },
    {
        "label": "PdfReader",
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "isExtraImport": true,
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceInstructEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "pinecone",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pinecone",
        "description": "pinecone",
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "Pinecone",
        "importPath": "pinecone",
        "description": "pinecone",
        "isExtraImport": true,
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "ServerlessSpec",
        "importPath": "pinecone",
        "description": "pinecone",
        "isExtraImport": true,
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "PineconeApiException",
        "importPath": "pinecone",
        "description": "pinecone",
        "isExtraImport": true,
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "redirect",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "url_for",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "flash",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "redirect",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "url_for",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "flash",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "importPath": "ecommbot.retrieval_generation",
        "description": "ecommbot.retrieval_generation",
        "isExtraImport": true,
        "detail": "ecommbot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "importPath": "ecommbot.converteur",
        "description": "ecommbot.converteur",
        "isExtraImport": true,
        "detail": "ecommbot.converteur",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "search_similar_vectors_candidat",
        "importPath": "utils.similar_vectors",
        "description": "utils.similar_vectors",
        "isExtraImport": true,
        "detail": "utils.similar_vectors",
        "documentation": {}
    },
    {
        "label": "search_similar_vectors_job",
        "importPath": "utils.similar_vectors",
        "description": "utils.similar_vectors",
        "isExtraImport": true,
        "detail": "utils.similar_vectors",
        "documentation": {}
    },
    {
        "label": "search_similar_vectors_candidat",
        "importPath": "utils.similar_vectors",
        "description": "utils.similar_vectors",
        "isExtraImport": true,
        "detail": "utils.similar_vectors",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "importPath": "utils.prepare_data",
        "description": "utils.prepare_data",
        "isExtraImport": true,
        "detail": "utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "importPath": "utils.prepare_data",
        "description": "utils.prepare_data",
        "isExtraImport": true,
        "detail": "utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "json_to_csv",
        "importPath": "utils.prepare_data",
        "description": "utils.prepare_data",
        "isExtraImport": true,
        "detail": "utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "generee_offres",
        "importPath": "utils.prepare_data",
        "description": "utils.prepare_data",
        "isExtraImport": true,
        "detail": "utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "importPath": "utils.prepare_data",
        "description": "utils.prepare_data",
        "isExtraImport": true,
        "detail": "utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "json_to_csv",
        "importPath": "utils.prepare_data",
        "description": "utils.prepare_data",
        "isExtraImport": true,
        "detail": "utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "snowflake.connector",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "snowflake.connector",
        "description": "snowflake.connector",
        "detail": "snowflake.connector",
        "documentation": {}
    },
    {
        "label": "redis",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "redis",
        "description": "redis",
        "detail": "redis",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "importPath": "recruitboot.ingest",
        "description": "recruitboot.ingest",
        "isExtraImport": true,
        "detail": "recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "importPath": "recruitboot.ingest",
        "description": "recruitboot.ingest",
        "isExtraImport": true,
        "detail": "recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "importPath": "recruitboot.retrieval_generation",
        "description": "recruitboot.retrieval_generation",
        "isExtraImport": true,
        "detail": "recruitboot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "importPath": "recruitboot.converteur",
        "description": "recruitboot.converteur",
        "isExtraImport": true,
        "detail": "recruitboot.converteur",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "pyspark.ml.clustering",
        "description": "pyspark.ml.clustering",
        "isExtraImport": true,
        "detail": "pyspark.ml.clustering",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "importPath": "reader_data",
        "description": "reader_data",
        "isExtraImport": true,
        "detail": "reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "importPath": "reader_data",
        "description": "reader_data",
        "isExtraImport": true,
        "detail": "reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "importPath": "transformer",
        "description": "transformer",
        "isExtraImport": true,
        "detail": "transformer",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "StringIndexer",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "VectorAssembler",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "pyspark.ml",
        "description": "pyspark.ml",
        "isExtraImport": true,
        "detail": "pyspark.ml",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "pyspark.ml.classification",
        "description": "pyspark.ml.classification",
        "isExtraImport": true,
        "detail": "pyspark.ml.classification",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "def create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    # Vérifier si la table Delta existe\n    try:\n        delta_table = DeltaTable.forPath(spark, path)",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    # Sauvegarder les données traitées dans la table Delta avec la fusion de schémas\n    df.write.format(\"delta\") \\\n        .mode(\"append\") \\\n        .option(\"mergeSchema\", \"true\") \\\n        .save(output_path)\n    print(\"Data written to Delta Table\")\n# ===================================================================================\n#                           MAIN ENTRYPOINT\n# ===================================================================================",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYSPARK_PIN_THREAD\"]",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n# Définir le schéma pour le JSON\nPERSON_SCHEMA = StructType([\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),\n        StructField(\"postal_code\", StringType(), True)",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",\n]\ndef create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "spark = create_or_get_spark(\n    app_name=\"json_to_delta\", packages=PACKAGES, cluster_manager=\"local[*]\"\n)\n# Configurer la connexion à Azure\nspark.conf.set(\n    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n    ADLS_ACCOUNT_KEY,\n)\nprint(\"Spark Session Created\")\n# Lire les données JSON depuis la variable",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "code-snippets.spark_TO_delta(inspiratio)",
        "description": "code-snippets.spark_TO_delta(inspiratio)",
        "peekOfCode": "df = spark.read.json(spark.sparkContext.parallelize([data]), schema=PERSON_SCHEMA)\nprint(\"JSON data loaded\")\n# Traiter les données\nprint(\"Data processed\")\n# Créer une table Delta vide (si elle n'existe pas encore)\ncreate_empty_delta_table(\n    spark=spark,\n    schema=PERSON_SCHEMA,\n    path=OUTPUT_PATH,\n    partition_cols=[\"first_name\"],",
        "detail": "code-snippets.spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.producer.config",
        "description": "CV_Processus.producer.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.producer.config",
        "documentation": {}
    },
    {
        "label": "producer",
        "kind": 5,
        "importPath": "CV_Processus.producer.producer",
        "description": "CV_Processus.producer.producer",
        "peekOfCode": "producer = KafkaProducer(bootstrap_servers='localhost:29092')\n# 2. Spécifier le dossier contenant les fichiers PDF\ndossier_data = os.path.join(os.path.dirname(__file__), \"data\")\n# 3. Parcourir tous les fichiers dans le dossier et envoyer chaque PDF\nfor fichier in os.listdir(dossier_data):\n    if fichier.endswith('.pdf'):  # Filtrer uniquement les fichiers PDF\n        chemin_fichier = os.path.join(dossier_data, fichier)\n        with open(chemin_fichier, 'rb') as fichier_pdf:\n            contenu_pdf = fichier_pdf.read()\n            # 4. Envoyer les données dans Kafka",
        "detail": "CV_Processus.producer.producer",
        "documentation": {}
    },
    {
        "label": "dossier_data",
        "kind": 5,
        "importPath": "CV_Processus.producer.producer",
        "description": "CV_Processus.producer.producer",
        "peekOfCode": "dossier_data = os.path.join(os.path.dirname(__file__), \"data\")\n# 3. Parcourir tous les fichiers dans le dossier et envoyer chaque PDF\nfor fichier in os.listdir(dossier_data):\n    if fichier.endswith('.pdf'):  # Filtrer uniquement les fichiers PDF\n        chemin_fichier = os.path.join(dossier_data, fichier)\n        with open(chemin_fichier, 'rb') as fichier_pdf:\n            contenu_pdf = fichier_pdf.read()\n            # 4. Envoyer les données dans Kafka\n            producer.send(config.topic, contenu_pdf)\n            print(f\"Fichier envoyé : {fichier}\", flush=True)  # Affichage immédiat",
        "detail": "CV_Processus.producer.producer",
        "documentation": {}
    },
    {
        "label": "create_spark_session",
        "kind": 2,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "def create_spark_session(app_name: str, packages: List[str]) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.jars.packages\", jars)\n        .config(f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", STORAGE_ACCOUNT_KEY)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n        .getOrCreate()\n    )",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "def ats_extractor(resume_data, api_key):\n    try:\n        groq_client = Groq(api_key=api_key)\n        messages = [\n            {\"role\": \"system\", \"content\": LLM_PROMPT},\n            {\"role\": \"user\", \"content\": resume_data}\n        ]\n        response = groq_client.chat.completions.create(\n            model=\"llama3-8b-8192\",\n            messages=messages,",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "extract_text_from_bytes",
        "kind": 2,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "def extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:\n            text = \"\".join([page.get_text(\"text\") for page in doc])\n            return text\n    except Exception as e:\n        return f\"Error extracting text: {e}\"\n# UDF to Process PDF Data\n@udf(returnType=StringType())",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "process_message_udf",
        "kind": 2,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "def process_message_udf(value):\n    pdf_bytes = value\n    text = extract_text_from_bytes(pdf_bytes)\n    extracted_data = ats_extractor(text, GROQ_API_KEY)\n    return json.dumps(extracted_data)\n# Create Delta Table if it Doesn't Exist\ndef create_delta_table(spark: SparkSession, schema: StructType, path: str):\n    try:\n        DeltaTable.forPath(spark, path)\n    except Exception:",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "create_delta_table",
        "kind": 2,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "def create_delta_table(spark: SparkSession, schema: StructType, path: str):\n    try:\n        DeltaTable.forPath(spark, path)\n    except Exception:\n        DeltaTable.createIfNotExists(spark) \\\n            .location(path) \\\n            .addColumns(schema) \\\n            .execute()\n# ===================================================================================\n#       KAFKA STREAM PROCESSING",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "STORAGE_ACCOUNT_NAME = \"adl.............\"\nSTORAGE_ACCOUNT_KEY = \"b.................................=\"\nCONTAINER_NAME = \"cv1\"\nDELTA_TABLE_PATH = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/delta-table\"\n# Kafka Configuration\nKAFKA_BOOTSTRAP_SERVERS = \"192.168.1.13:29093\"\nKAFKA_TOPIC = \"TopicCV\"\n# Groq API Key\nGROQ_API_KEY = \"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\"\n# Delta Table Schema",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "STORAGE_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "STORAGE_ACCOUNT_KEY = \"b.................................=\"\nCONTAINER_NAME = \"cv1\"\nDELTA_TABLE_PATH = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/delta-table\"\n# Kafka Configuration\nKAFKA_BOOTSTRAP_SERVERS = \"192.168.1.13:29093\"\nKAFKA_TOPIC = \"TopicCV\"\n# Groq API Key\nGROQ_API_KEY = \"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\"\n# Delta Table Schema\nPERSON_SCHEMA = StructType([",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "CONTAINER_NAME",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "CONTAINER_NAME = \"cv1\"\nDELTA_TABLE_PATH = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/delta-table\"\n# Kafka Configuration\nKAFKA_BOOTSTRAP_SERVERS = \"192.168.1.13:29093\"\nKAFKA_TOPIC = \"TopicCV\"\n# Groq API Key\nGROQ_API_KEY = \"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\"\n# Delta Table Schema\nPERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "DELTA_TABLE_PATH",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "DELTA_TABLE_PATH = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/delta-table\"\n# Kafka Configuration\nKAFKA_BOOTSTRAP_SERVERS = \"192.168.1.13:29093\"\nKAFKA_TOPIC = \"TopicCV\"\n# Groq API Key\nGROQ_API_KEY = \"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\"\n# Delta Table Schema\nPERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = \"192.168.1.13:29093\"\nKAFKA_TOPIC = \"TopicCV\"\n# Groq API Key\nGROQ_API_KEY = \"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\"\n# Delta Table Schema\nPERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "KAFKA_TOPIC",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "KAFKA_TOPIC = \"TopicCV\"\n# Groq API Key\nGROQ_API_KEY = \"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\"\n# Delta Table Schema\nPERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "GROQ_API_KEY",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "GROQ_API_KEY = \"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\"\n# Delta Table Schema\nPERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "spark = create_spark_session(\"Kafka_to_Delta\", [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\"\n])\n# ===================================================================================\n#       FUNCTIONS\n# ===================================================================================\n# LLM Extraction Prompt",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "LLM_PROMPT",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "LLM_PROMPT = '''\nYou are an AI system designed to extract structured information from resumes. \nExtract the following fields strictly following this JSON schema:\n{\n  \"first_name\": \"Le prénom du candidat.\",\n  \"last_name\": \"Le nom de famille du candidat.\",\n  \"full_name\": \"Le nom complet du candidat (prénom et nom).\",\n  \"title\": \"Titre professionnel du candidat (ex: Stagiaire, Ingénieur, Développeur).\",\n  \"address\": {\n    \"formatted_location\": \"Adresse complète du candidat au format texte.\",",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n    .option(\"subscribe\", KAFKA_TOPIC) \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .option(\"failOnDataLoss\", \"false\") \\\n    .load()\nprocessed_df = df.selectExpr(\"CAST(value AS BINARY) as value\") \\\n    .withColumn(\"processed_data\", from_json(process_message_udf(col(\"value\")), PERSON_SCHEMA))\n# Forcer la correspondance du schéma\nfinal_df = processed_df.select(",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "processed_df",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "processed_df = df.selectExpr(\"CAST(value AS BINARY) as value\") \\\n    .withColumn(\"processed_data\", from_json(process_message_udf(col(\"value\")), PERSON_SCHEMA))\n# Forcer la correspondance du schéma\nfinal_df = processed_df.select(\n    col(\"processed_data.first_name\").alias(\"first_name\"),\n    col(\"processed_data.last_name\").alias(\"last_name\"),\n    col(\"processed_data.full_name\").alias(\"full_name\"),\n    col(\"processed_data.title\").alias(\"title\"),\n    col(\"processed_data.address\").alias(\"address\"),\n    col(\"processed_data.objective\").alias(\"objective\"),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "final_df",
        "kind": 5,
        "importPath": "CV_Processus.consumer",
        "description": "CV_Processus.consumer",
        "peekOfCode": "final_df = processed_df.select(\n    col(\"processed_data.first_name\").alias(\"first_name\"),\n    col(\"processed_data.last_name\").alias(\"last_name\"),\n    col(\"processed_data.full_name\").alias(\"full_name\"),\n    col(\"processed_data.title\").alias(\"title\"),\n    col(\"processed_data.address\").alias(\"address\"),\n    col(\"processed_data.objective\").alias(\"objective\"),\n    col(\"processed_data.date_of_birth\").alias(\"date_of_birth\"),\n    col(\"processed_data.place_of_birth\").alias(\"place_of_birth\"),\n    col(\"processed_data.phones\").alias(\"phones\"),",
        "detail": "CV_Processus.consumer",
        "documentation": {}
    },
    {
        "label": "STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "STORAGE_ACCOUNT_NAME = \"a...............\"\nSTORAGE_ACCOUNT_KEY = \"b...........................................................=\"\nCONTAINER_NAME = \"cv1\"\nPARQUET_FILE_PATH = \"delta-table/part-00000-e8831429-466c-47a9-9517-06aecdc0b839-c000.snappy.parquet\"\n# Créer la session Spark\nspark = SparkSession.builder \\\n    .appName(\"Lecture Parquet depuis ADLS\") \\\n    .config(f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", STORAGE_ACCOUNT_KEY) \\\n    .getOrCreate()\n# Chemin du fichier Parquet",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "STORAGE_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "STORAGE_ACCOUNT_KEY = \"b...........................................................=\"\nCONTAINER_NAME = \"cv1\"\nPARQUET_FILE_PATH = \"delta-table/part-00000-e8831429-466c-47a9-9517-06aecdc0b839-c000.snappy.parquet\"\n# Créer la session Spark\nspark = SparkSession.builder \\\n    .appName(\"Lecture Parquet depuis ADLS\") \\\n    .config(f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", STORAGE_ACCOUNT_KEY) \\\n    .getOrCreate()\n# Chemin du fichier Parquet\nparquet_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{PARQUET_FILE_PATH}\"",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "CONTAINER_NAME",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "CONTAINER_NAME = \"cv1\"\nPARQUET_FILE_PATH = \"delta-table/part-00000-e8831429-466c-47a9-9517-06aecdc0b839-c000.snappy.parquet\"\n# Créer la session Spark\nspark = SparkSession.builder \\\n    .appName(\"Lecture Parquet depuis ADLS\") \\\n    .config(f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", STORAGE_ACCOUNT_KEY) \\\n    .getOrCreate()\n# Chemin du fichier Parquet\nparquet_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{PARQUET_FILE_PATH}\"\n# Lire les données Parquet",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "PARQUET_FILE_PATH",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "PARQUET_FILE_PATH = \"delta-table/part-00000-e8831429-466c-47a9-9517-06aecdc0b839-c000.snappy.parquet\"\n# Créer la session Spark\nspark = SparkSession.builder \\\n    .appName(\"Lecture Parquet depuis ADLS\") \\\n    .config(f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", STORAGE_ACCOUNT_KEY) \\\n    .getOrCreate()\n# Chemin du fichier Parquet\nparquet_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{PARQUET_FILE_PATH}\"\n# Lire les données Parquet\ndf = spark.read.parquet(parquet_path)",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "spark = SparkSession.builder \\\n    .appName(\"Lecture Parquet depuis ADLS\") \\\n    .config(f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", STORAGE_ACCOUNT_KEY) \\\n    .getOrCreate()\n# Chemin du fichier Parquet\nparquet_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{PARQUET_FILE_PATH}\"\n# Lire les données Parquet\ndf = spark.read.parquet(parquet_path)\n# Afficher un aperçu des données dans le DataFrame\ndf.show(truncate=False)",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "parquet_path",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "parquet_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{PARQUET_FILE_PATH}\"\n# Lire les données Parquet\ndf = spark.read.parquet(parquet_path)\n# Afficher un aperçu des données dans le DataFrame\ndf.show(truncate=False)\n# Afficher le schéma pour référence\ndf.printSchema()",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "CV_Processus.Test",
        "description": "CV_Processus.Test",
        "peekOfCode": "df = spark.read.parquet(parquet_path)\n# Afficher un aperçu des données dans le DataFrame\ndf.show(truncate=False)\n# Afficher le schéma pour référence\ndf.printSchema()",
        "detail": "CV_Processus.Test",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "offre_Process.data.config",
        "description": "offre_Process.data.config",
        "peekOfCode": "api_key = \"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\"\n# Required connection configs for Kafka producer, consumer, and admin\nserveur_kafka=\"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nusername=\"OM3FCB4RLKF3L2AQ\"\npassword=\"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\ntopic=\"offres_trav\"",
        "detail": "offre_Process.data.config",
        "documentation": {}
    },
    {
        "label": "envoyer_vers_kafka",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):\n    try:\n        # Configuration du producteur Kafka avec SASL_SSL\n        producer = KafkaProducer(\n            bootstrap_servers=[serveur_kafka],\n            value_serializer=lambda m: json.dumps(m).encode('utf-8'),\n            security_protocol=\"SASL_SSL\",\n            sasl_mechanism=\"PLAIN\",\n            sasl_plain_username=api_key,\n            sasl_plain_password=api_secret,",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "simulation_offres",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def simulation_offres(api_key, api_secret, serveur_kafka, topic):\n    while True:\n        try:\n            # Génération simulée d'une offre complète\n            offre = {\n                \"Titre du poste\": random.choice(titres_poste),\n                \"Société\": random.choice(societes),\n                \"Lieu\": random.choice(lieux),\n                \"Type de contrat\": \"Stage PFE de 6 mois\",\n                \"Description du poste\": \"Assistance à la conception, à la mise en œuvre et à l'optimisation de pipelines de données.\",",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "compteur_offres",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "compteur_offres = 0\nserveur_kafka = cfg.serveur_kafka\ntopic = cfg.topic\napi_key = cfg.username\napi_secret = cfg.password\n# Liste d'exemples pour chaque section de l'offre\ntitres_poste = [\"Stagiaire Data Engineer (PFE)\", \"Data Analyst Junior\", \"Développeur Backend\", \"Ingénieur Machine Learning\"]\nsocietes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "serveur_kafka",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "serveur_kafka = cfg.serveur_kafka\ntopic = cfg.topic\napi_key = cfg.username\napi_secret = cfg.password\n# Liste d'exemples pour chaque section de l'offre\ntitres_poste = [\"Stagiaire Data Engineer (PFE)\", \"Data Analyst Junior\", \"Développeur Backend\", \"Ingénieur Machine Learning\"]\nsocietes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "topic = cfg.topic\napi_key = cfg.username\napi_secret = cfg.password\n# Liste d'exemples pour chaque section de l'offre\ntitres_poste = [\"Stagiaire Data Engineer (PFE)\", \"Data Analyst Junior\", \"Développeur Backend\", \"Ingénieur Machine Learning\"]\nsocietes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "api_key = cfg.username\napi_secret = cfg.password\n# Liste d'exemples pour chaque section de l'offre\ntitres_poste = [\"Stagiaire Data Engineer (PFE)\", \"Data Analyst Junior\", \"Développeur Backend\", \"Ingénieur Machine Learning\"]\nsocietes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "api_secret",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "api_secret = cfg.password\n# Liste d'exemples pour chaque section de l'offre\ntitres_poste = [\"Stagiaire Data Engineer (PFE)\", \"Data Analyst Junior\", \"Développeur Backend\", \"Ingénieur Machine Learning\"]\nsocietes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "titres_poste",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "titres_poste = [\"Stagiaire Data Engineer (PFE)\", \"Data Analyst Junior\", \"Développeur Backend\", \"Ingénieur Machine Learning\"]\nsocietes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "societes",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "societes = [\"DataTech Solutions\", \"Tech Innovations\", \"DataWorks\", \"AlgoConsulting\"]\nlieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",\n    \"Familiarité avec les plateformes cloud (AWS, Google Cloud)\"",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "lieux",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "lieux = [\"Lyon, France\", \"Paris, France\", \"Télétravail\", \"Marseille, France\"]\nsalaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",\n    \"Familiarité avec les plateformes cloud (AWS, Google Cloud)\"\n]",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "salaires",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "salaires = [1200, 1500, 1800, 2000]  # en EUR\nlangues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",\n    \"Familiarité avec les plateformes cloud (AWS, Google Cloud)\"\n]\ndef envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "langues_requises",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "langues_requises = [\"Anglais, Français\", \"Français\", \"Anglais\"]\nformations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",\n    \"Familiarité avec les plateformes cloud (AWS, Google Cloud)\"\n]\ndef envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):\n    try:",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "formations = [\"Étudiant en informatique ou domaine connexe\", \"Diplômé en ingénierie logicielle\", \"Master en Data Science\"]\ncompetences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",\n    \"Familiarité avec les plateformes cloud (AWS, Google Cloud)\"\n]\ndef envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):\n    try:\n        # Configuration du producteur Kafka avec SASL_SSL",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "competences",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "competences = [\n    \"Excellente maîtrise de Python et SQL\",\n    \"Expérience avec des outils ETL comme Apache Airflow ou Talend\",\n    \"Connaissance des bases de données NoSQL (MongoDB, Cassandra)\",\n    \"Familiarité avec les plateformes cloud (AWS, Google Cloud)\"\n]\ndef envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):\n    try:\n        # Configuration du producteur Kafka avec SASL_SSL\n        producer = KafkaProducer(",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "process_message_groq",
        "kind": 2,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "def process_message_groq(message: str):\n    \"\"\"Traite un message et le formate dans le format JSON voulu.\"\"\"\n    try:\n        # Envoi du texte à l'API Groq pour extraction des données\n        headers = {\n            'Authorization': f'Bearer {GROQ_API_KEY}',\n            'Content-Type': 'application/json',\n        }\n        payload = {\n            'text': message  # Le texte de l'offre que vous envoyez à l'API",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "GROQ_API_KEY",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\ndef process_message_groq(message: str):\n    \"\"\"Traite un message et le formate dans le format JSON voulu.\"\"\"\n    try:\n        # Envoi du texte à l'API Groq pour extraction des données\n        headers = {\n            'Authorization': f'Bearer {GROQ_API_KEY}',\n            'Content-Type': 'application/json',\n        }\n        payload = {",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Créer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n        .config(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\")\n        .master(\"local[*]\")",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    \"\"\"Créer une table Delta vide si elle n'existe pas.\"\"\"\n    try:\n        DeltaTable.forPath(spark, path)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    \"\"\"Sauvegarder les données dans une Delta Table.\"\"\"\n    df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n    print(\"Data written to Delta Table.\")\ndef process_message(spark: SparkSession, message: str):\n    \"\"\"Traiter un message Kafka et l'enregistrer dans une table Delta.\"\"\"\n    try:\n        job_posting= process_message_groq(spark, message)\n        df = spark.createDataFrame([job_posting], schema=OFFRE_SCHEMA)\n        save_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "process_message",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def process_message(spark: SparkSession, message: str):\n    \"\"\"Traiter un message Kafka et l'enregistrer dans une table Delta.\"\"\"\n    try:\n        job_posting= process_message_groq(spark, message)\n        df = spark.createDataFrame([job_posting], schema=OFFRE_SCHEMA)\n        save_to_delta(df, OUTPUT_PATH)\n    except Exception as e:\n        print(f\"Error processing message: {e}\")\n# Configurer le consommateur Kafka pour Confluent\nconsumer = KafkaConsumer(",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = os.getenv(\"ADLS_STORAGE_ACCOUNT_NAME\")\nADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_GROUP_ID",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_API_KEY",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_API_SECRET",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_TOPIC",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Créer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "consumer = KafkaConsumer(\n    KAFKA_TOPIC,\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_plain_username=KAFKA_API_KEY,\n    sasl_plain_password=KAFKA_API_SECRET,",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "kind": 2,
        "importPath": "Recruiter_Apps.recruitboot.converteur",
        "description": "Recruiter_Apps.recruitboot.converteur",
        "peekOfCode": "def get_pdf_text(pdf_docs):\n    text = \"\"\n    for pdf in pdf_docs:\n        # Vérifiez si le chemin du fichier PDF est valide\n        if not os.path.exists(pdf):\n            print(f\"Le fichier {pdf} n'existe pas.\")\n            continue  # Passez au fichier suivant si celui-ci n'existe pas\n        try:\n            pdf_reader = PdfReader(pdf)\n            # Parcourir toutes les pages du PDF et extraire le texte",
        "detail": "Recruiter_Apps.recruitboot.converteur",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "kind": 2,
        "importPath": "Recruiter_Apps.recruitboot.ingest",
        "description": "Recruiter_Apps.recruitboot.ingest",
        "peekOfCode": "def get_text_chunks(text):\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    chunks = text_splitter.split_text(text)\n    return chunks\ndef get_vectorstore(text_chunks):",
        "detail": "Recruiter_Apps.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "kind": 2,
        "importPath": "Recruiter_Apps.recruitboot.ingest",
        "description": "Recruiter_Apps.recruitboot.ingest",
        "peekOfCode": "def get_vectorstore(text_chunks):\n    # Charger les embeddings Hugging Face\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    # Créer le vecteur FAISS à partir des textes et des embeddings\n    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n    return vectorstore",
        "detail": "Recruiter_Apps.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "kind": 2,
        "importPath": "Recruiter_Apps.recruitboot.retrieval_generation",
        "description": "Recruiter_Apps.recruitboot.retrieval_generation",
        "peekOfCode": "def create_conversation_chain(vectorstore):\n    if not isinstance(vectorstore, FAISS):\n        raise TypeError(\"Le vectorstore doit être un objet de type FAISS.\")\n    # Initialisation du LLM avec HuggingFaceHub\n    llm = HuggingFaceHub(\n        repo_id=\"google/flan-t5-large\",\n        huggingfacehub_api_token=\"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\",\n        model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n    )\n    # Configuration de la mémoire pour conserver l'historique des conversations",
        "detail": "Recruiter_Apps.recruitboot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "Recruiter_Apps.utils.prepare_data",
        "description": "Recruiter_Apps.utils.prepare_data",
        "peekOfCode": "def ats_extractor(resume_data):\n    api_key_grouq = \"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\"\n    # Prompt for extracting specific information from the resume\n    prompt = '''\n    You are an AI bot designed to act as a professional for parsing resumes. You are given a resume, and your job is to extract the following information:\n    1. skills \n    2. languages\n    3. education (field of study such as Computer Science, Management, Medicine, Marketing, Electrical Engineering, etc.)\n    4. total number of work experiences\n    Provide the extracted information in JSON format only with english translation. Additionally, generate a unique ID for the candidate .",
        "detail": "Recruiter_Apps.utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": "Recruiter_Apps.utils.prepare_data",
        "description": "Recruiter_Apps.utils.prepare_data",
        "peekOfCode": "def extract_text_from_pdf(upload_folder):\n    \"\"\"\n    Extrait le texte de tous les fichiers PDF dans un dossier.\n    Args:\n        upload_folder (str): Chemin du dossier contenant les fichiers PDF.\n    Returns:\n        dict: Dictionnaire avec les noms de fichiers comme clés et les textes extraits comme valeurs.\n    \"\"\"\n    #extracted_text = {}\n    try:",
        "detail": "Recruiter_Apps.utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "generee_offres",
        "kind": 2,
        "importPath": "Recruiter_Apps.utils.prepare_data",
        "description": "Recruiter_Apps.utils.prepare_data",
        "peekOfCode": "def generee_offres(descriptions):\n    api_key_grouq = \"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\"\n    description_prompt = '''\n        You are an AI bot designed to generate detailed job descriptions. Based on the following candidate information, create a professional job description in English. The format should include:\n        - Industry: Mention the industry (e.g., \"Computer and Technology\").\n        - Job Title: Suggest a title based on skills and education (e.g., \"Data Scientist\").\n        - Date: Use the current date in the format YYYY-MM-DD.\n        - Description: Provide a paragraph summarizing the skills, languages, education, and years of experience. Make the description engaging and professional.\n        Example:\n        Industry: Computer And Technology",
        "detail": "Recruiter_Apps.utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "json_to_csv",
        "kind": 2,
        "importPath": "Recruiter_Apps.utils.prepare_data",
        "description": "Recruiter_Apps.utils.prepare_data",
        "peekOfCode": "def json_to_csv(json_data, csv_file_path):\n    # Ensure json_data is a dictionary\n    if not isinstance(json_data, dict):\n        return {\"error\": \"Invalid JSON data.\"}\n    # Define the CSV header\n    header = [\"candidate_id\", \"competences\", \"langues\", \"formation\", \"nbr_years_exp\"]\n    # Convert data into a list of rows\n    rows = [\n        [\n            json_data.get(\"candidate_id\", \"None\"),",
        "detail": "Recruiter_Apps.utils.prepare_data",
        "documentation": {}
    },
    {
        "label": "search_similar_vectors_candidat",
        "kind": 2,
        "importPath": "Recruiter_Apps.utils.similar_vectors",
        "description": "Recruiter_Apps.utils.similar_vectors",
        "peekOfCode": "def search_similar_vectors_candidat(query_embedding, top_k=5):\n    api_key_pin = \"pcsk_vUaKS_3PK35kGth5rcmSKZkihFFuaS7B44xzMycHCnot1s9Czf1WE8iXSPZg4nDph81Ak\"\n    pc = pinecone.Pinecone(api_key=api_key_pin)\n    # Connexion à l'index Pinecone\n    index_name = 'candidates'  # Le nom de votre index Pinecone\n    index_candidats = pc.Index(index_name)\n    print(f\"Query embedding: {query_embedding}\")\n    result = index_candidats.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n    print(f\"Result: {result}\")\n    # Vérification de la présence de la clé 'matches'",
        "detail": "Recruiter_Apps.utils.similar_vectors",
        "documentation": {}
    },
    {
        "label": "search_similar_vectors_job",
        "kind": 2,
        "importPath": "Recruiter_Apps.utils.similar_vectors",
        "description": "Recruiter_Apps.utils.similar_vectors",
        "peekOfCode": "def search_similar_vectors_job(query_embedding, top_k=5):\n    api_key_pin = \"pcsk_vUaKS_3PK35kGth5rcmSKZkihFFuaS7B44xzMycHCnot1s9Czf1WE8iXSPZg4nDph81Ak\"\n    pc = pinecone.Pinecone(api_key=api_key_pin)\n    # Connexion à l'index Pinecone\n    index_name = 'job'  # Le nom de votre index Pinecone\n    index_job= pc.Index(index_name)\n    print(f\"Query embedding: {query_embedding}\")\n    result = index_job.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n    print(f\"Result: {result}\")\n    # Vérification de la présence de la clé 'matches'",
        "detail": "Recruiter_Apps.utils.similar_vectors",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "def index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 2,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "def chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)\n        # Retourner uniquement la réponse textuelle sans l'objet complet\n        answer = result[\"answer\"] if \"answer\" in result else \"Désolé, je n'ai pas pu trouver une réponse.\"\n        return jsonify({\"response\": answer})  # Retourner la réponse du chatbot",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "app = Flask(__name__)\nload_dotenv()\n# Fonction de génération de la chaîne conversationnelle (pour intégrer LangChain)\n# Traitement du PDF\npdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "pdf_docs",
        "kind": 5,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "pdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "text = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "chunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "vectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "conversation_chain",
        "kind": 5,
        "importPath": "Recruiter_Apps.app",
        "description": "Recruiter_Apps.app",
        "peekOfCode": "conversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse",
        "detail": "Recruiter_Apps.app",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "def index():\n    global similar_vectors_candidat\n    global similar_vectors_job\n    global nom_cv\n    if request.method == 'POST':\n        if 'resume_folder' in request.files:\n            # Traiter le téléchargement des fichiers PDF\n            resume_folder = request.files.getlist('resume_folder')\n            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n            for file in resume_folder:",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "ranking",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "def ranking():\n    global similar_vectors_candidat\n    #global similar_vectors_job\n    # Nombre de vecteurs similaires\n    nbr_simil = len(similar_vectors_candidat)\n    # Générer la liste `resumes` avec les champs id et score arrondi (en pourcentage)\n    resumes_candidat = [\n        {\n            \"ID\": vector['id'],  # ID du vecteur\n            \"Score\": round(vector['score'] * 100, 2)  # Score en pourcentage",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "jobs",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "def jobs():\n    global similar_vectors_candidat\n    global similar_vectors_job\n    global resumes_job\n    # Nombre de vecteurs similaires\n    nbr_simil = len(similar_vectors_job)\n    # Générer la liste `resumes` avec les champs id et score arrondi (en pourcentage)\n    resumes_job = [\n        {\n            \"ID\": vector['id'],  # ID du vecteur",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "show_jobs",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "def show_jobs():\n    global resumes_job\n    global descriptions\n    print(resumes_job)\n    # Récupération des IDs (avec protection pour éviter l'erreur d'indexation)\n    id_job1 = resumes_job[0]['ID'] if len(resumes_job) > 0 else None\n    id_job2 = resumes_job[1]['ID'] if len(resumes_job) > 1 else None\n    id_job3 = resumes_job[2]['ID'] if len(resumes_job) > 2 else None\n    id_job4 = resumes_job[3]['ID'] if len(resumes_job) > 3 else None\n    id_job5 = resumes_job[4]['ID'] if len(resumes_job) > 4 else None",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "post_cvs",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "def post_cvs(key):\n    global descriptions\n    global nom_cv\n    # Exemple de nom de CV (ceci peut être dynamique si récupéré de l'utilisateur ou d'une base de données)\n      # Exemple de nom, à remplacer par une logique dynamique si nécessaire.\n    # Ajouter les informations dans Redis sous la forme (key: offer_id) -> CV name\n    redis_client.set(key, nom_cv)  # Stocke le nom du CV pour l'ID de l'offre\n    # Ajouter un message de notification avec flash\n    flash(f\"Submit your CV: {nom_cv} for Offer ID: {key}\", \"info\")\n    # Rediriger vers la page des offres après l'application",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "account",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "account = \"phztxrc-go36107\"\nuser = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "user",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "user = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "password",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "password = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "role",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "role = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "warehouse",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "warehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "database",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "database = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "schema",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "schema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "conn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema\n)\napp = Flask(__name__)",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "app = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = 'uploads/'\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global similar_vectors_candidat",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "app.config['UPLOAD_FOLDER']",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "app.config['UPLOAD_FOLDER'] = 'uploads/'\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global similar_vectors_candidat\n    global similar_vectors_job",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "app.config['REDIS_URL']",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "app.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global similar_vectors_candidat\n    global similar_vectors_job\n    global nom_cv",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "app.secret_key",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "app.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global similar_vectors_candidat\n    global similar_vectors_job\n    global nom_cv\n    if request.method == 'POST':\n        if 'resume_folder' in request.files:",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "redis_client",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_cvs",
        "description": "Recruiter_Apps.app_cvs",
        "peekOfCode": "redis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    global similar_vectors_candidat\n    global similar_vectors_job\n    global nom_cv\n    if request.method == 'POST':\n        if 'resume_folder' in request.files:\n            # Traiter le téléchargement des fichiers PDF\n            resume_folder = request.files.getlist('resume_folder')",
        "detail": "Recruiter_Apps.app_cvs",
        "documentation": {}
    },
    {
        "label": "job_description",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "def job_description():\n    global similar_vectors_candidat\n    if request.method == 'POST':\n        job_description = request.form['job_description']\n        # Pass the extracted text to the ats_extractor function\n        extracted_info = ats_extractor(job_description)\n        resumes_csv = json_to_csv(extracted_info, \"resumes.csv\")\n        data = pd.read_csv(\"resumes.csv\")\n        model = SentenceTransformer('all-MiniLM-L6-v2')\n        data['competences'] = data['competences'].apply(",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "ranking2",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "def ranking2():\n    global similar_vectors_candidat\n    # Nombre de vecteurs similaires\n    nbr_simil = len(similar_vectors_candidat)\n    # Générer la liste `resumes` avec les champs id et score arrondi (en pourcentage)\n    resumes_candidat = [\n        {\n            \"ID\": vector['id'],  # ID du vecteur\n            \"Score\": round(vector['score'] * 100, 2)  # Score en pourcentage\n        }",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "def index():\n    return render_template('chat.html')\n@app.route('/chatbot_key/<key>', methods=['GET', 'POST'])\ndef chatbot_key(key):\n    return f\"9alib 3liha\"+key\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "chatbot_key",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "def chatbot_key(key):\n    return f\"9alib 3liha\"+key\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 2,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "def chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)\n        # Retourner uniquement la réponse textuelle sans l'objet complet\n        answer = result[\"answer\"] if \"answer\" in result else \"Désolé, je n'ai pas pu trouver une réponse.\"\n        return jsonify({\"response\": answer})  # Retourner la réponse du chatbot",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "pdf_docs",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "pdf_docs = [r\"data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\napp = Flask(__name__)\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "text = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\napp = Flask(__name__)\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "chunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\napp = Flask(__name__)\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "vectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\napp = Flask(__name__)\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "conversation_chain",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "conversation_chain = create_conversation_chain(vectorstore)\napp = Flask(__name__)\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef job_description():\n    global similar_vectors_candidat",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "app = Flask(__name__)\napp.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef job_description():\n    global similar_vectors_candidat\n    if request.method == 'POST':",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "app.config['REDIS_URL']",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "app.config['REDIS_URL'] = \"redis://localhost:6379/0\"  # Utilisez l'URL de votre instance Redis\n# Définir une clé secrète pour la session\napp.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef job_description():\n    global similar_vectors_candidat\n    if request.method == 'POST':\n        job_description = request.form['job_description']",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "app.secret_key",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "app.secret_key = 'said'\n# Connexion à Redis\nredis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef job_description():\n    global similar_vectors_candidat\n    if request.method == 'POST':\n        job_description = request.form['job_description']\n        # Pass the extracted text to the ats_extractor function\n        extracted_info = ats_extractor(job_description)",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "redis_client",
        "kind": 5,
        "importPath": "Recruiter_Apps.app_offres",
        "description": "Recruiter_Apps.app_offres",
        "peekOfCode": "redis_client = redis.StrictRedis.from_url(app.config['REDIS_URL'])\n@app.route('/', methods=['GET', 'POST'])\ndef job_description():\n    global similar_vectors_candidat\n    if request.method == 'POST':\n        job_description = request.form['job_description']\n        # Pass the extracted text to the ats_extractor function\n        extracted_info = ats_extractor(job_description)\n        resumes_csv = json_to_csv(extracted_info, \"resumes.csv\")\n        data = pd.read_csv(\"resumes.csv\")",
        "detail": "Recruiter_Apps.app_offres",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "kind": 6,
        "importPath": "SegmentationSpark.reader_data",
        "description": "SegmentationSpark.reader_data",
        "peekOfCode": "class CandidatDataLoader:\n    def __init__(self, jdbc_url=\"jdbc:postgresql://localhost:5432/cond_db\"):\n        \"\"\"\n        Initialise la classe pour charger les données depuis PostgreSQL.\n        :param jdbc_url: URL de connexion JDBC à la base de données PostgreSQL.\n        \"\"\"\n        self.jdbc_url = jdbc_url\n        self.query = \"\"\"\n        SELECT \n            c.full_name AS candidate_name,",
        "detail": "SegmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatSegmentation",
        "kind": 6,
        "importPath": "SegmentationSpark.segmentation",
        "description": "SegmentationSpark.segmentation",
        "peekOfCode": "class CandidatSegmentation:\n    def __init__(self, df, k=3, seed=42):\n        \"\"\"\n        Initialise la classe pour la segmentation des candidats.\n        :param df: DataFrame Spark contenant les données transformées.\n        :param k: Nombre de clusters pour KMeans.\n        :param seed: Seed pour la reproductibilité.\n        \"\"\"\n        self.df = df\n        self.k = k",
        "detail": "SegmentationSpark.segmentation",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "kind": 6,
        "importPath": "SegmentationSpark.transformer",
        "description": "SegmentationSpark.transformer",
        "peekOfCode": "class CandidatTransformer:\n    def __init__(self, df):\n        \"\"\"\n        Initialise la classe avec un DataFrame Spark.\n        :param df: DataFrame Spark contenant les données.\n        \"\"\"\n        self.df = df\n    def build_pipeline(self):\n        \"\"\"\n        Construit un pipeline pour transformer les données.",
        "detail": "SegmentationSpark.transformer",
        "documentation": {}
    },
    {
        "label": "batch_vectors",
        "kind": 2,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "def batch_vectors(vectors, batch_size):\n    \"\"\"\n    Divise une liste de vecteurs en lots plus petits.\n    \"\"\"\n    for i in range(0, len(vectors), batch_size):\n        yield vectors[i:i + batch_size]\n# Taille maximale des lots (ajustez si nécessaire)\nbatch_size = 100  # Taille de lot initiale, adaptée à vos besoins et à vos tests.\n# --- Insérer les offres ---\noffre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "account",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "account = \"phztxrc-go36107\"\nuser = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "user",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "user = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "password",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "password = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "role",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "role = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "warehouse",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "warehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "database",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "database = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "schema",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "schema = \"my_project_schema\"\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "conn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema\n)\n# Requête pour les offres",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "query_offres",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "query_offres = \"\"\"\n SELECT o.offre_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        o.experience_dur\n FROM my_project_database.my_project_schema.offre_fait o\n JOIN my_project_database.my_project_schema.offre_comp oc ON o.offre_id = oc.offre_id\n JOIN my_project_database.my_project_schema.competence cmp ON oc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_offr lo ON o.offre_id = lo.offre_id",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "query_candidats",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "query_candidats = \"\"\"\n SELECT c.candidat_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        c.nbr_years_exp\n FROM my_project_database.my_project_schema.candidat_fait c\n LEFT JOIN my_project_database.my_project_schema.candidat_comp cc ON c.candidat_id = cc.candidat_id\n LEFT JOIN my_project_database.my_project_schema.competence cmp ON cc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_cand lc ON c.candidat_id = lc.candidat_id",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n# Trier les compétences et les langues pour chaque offre\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les compétences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n# Trier les compétences et les langues pour chaque offre\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les compétences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les compétences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les compétences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['FORMATION']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['FORMATION']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Générer des embeddings pour les offres",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Générer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Générer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Générer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Générer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['description']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Générer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "embeddings_offres",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "embeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Générer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Générer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "#df_offres['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Générer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['description']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Étape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "embeddings_candidats",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "embeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Étape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Étape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone\n# Créez une instance de Pinecone avec votre clé API\napi_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "#df_candidats['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Étape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone\n# Créez une instance de Pinecone avec votre clé API\napi_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"\npc = pinecone.Pinecone(api_key=api_key)\n# Vérifiez si l'index existe déjà\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "api_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"\npc = pinecone.Pinecone(api_key=api_key)\n# Vérifiez si l'index existe déjà\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:\n        # Créez l'index si nécessaire\n        pc.create_index(\n            name='jobcandidates',\n            dimension=384,  # Adaptez la dimension selon le modèle utilisé\n            metric='cosine',  # Vous pouvez choisir la métrique appropriée",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "pc",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "pc = pinecone.Pinecone(api_key=api_key)\n# Vérifiez si l'index existe déjà\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:\n        # Créez l'index si nécessaire\n        pc.create_index(\n            name='jobcandidates',\n            dimension=384,  # Adaptez la dimension selon le modèle utilisé\n            metric='cosine',  # Vous pouvez choisir la métrique appropriée\n            spec=ServerlessSpec(",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "index = pc.Index(\"jobcandidates\")\ndef batch_vectors(vectors, batch_size):\n    \"\"\"\n    Divise une liste de vecteurs en lots plus petits.\n    \"\"\"\n    for i in range(0, len(vectors), batch_size):\n        yield vectors[i:i + batch_size]\n# Taille maximale des lots (ajustez si nécessaire)\nbatch_size = 100  # Taille de lot initiale, adaptée à vos besoins et à vos tests.\n# --- Insérer les offres ---",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "batch_size = 100  # Taille de lot initiale, adaptée à vos besoins et à vos tests.\n# --- Insérer les offres ---\noffre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in offre_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(offre_vectors, batch_size):",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "offre_vectors",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "offre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in offre_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(offre_vectors, batch_size):\n    try:\n        index.upsert(vectors=batch)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "candidat_vectors",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "candidat_vectors = [(str(row.CANDIDAT_ID), row.embedding) for _, row in df_candidats.iterrows()]\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in candidat_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(candidat_vectors, batch_size):\n    try:\n        index.upsert(vectors=batch)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "generate_requirements",
        "kind": 2,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "def generate_requirements(modules, output_file=\"requirements.txt\"):\n    \"\"\"\n    Generate a requirements.txt file with the installed versions of the specified modules.\n    Args:\n        modules (list): List of module names to include in the requirements file.\n        output_file (str): The output file name for requirements.txt.\n    \"\"\"\n    requirements = []\n    for module in modules:\n        try:",
        "detail": "test",
        "documentation": {}
    },
    {
        "label": "modules",
        "kind": 5,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "modules = [\n    \"datetime\", \"flask\", \"os\", \"pandas\", \"gensim\", \"numpy\", \"nltk\",\n    \"sklearn\", \"fitz\", \"groq\", \"re\", \"csv\", \"pinecone\", \"sentence-transformers\",\n    \"snowflake-connector-python\", \"uuid\", \"redis\"\n]\n# Generate the requirements.txt file\ngenerate_requirements(modules)",
        "detail": "test",
        "documentation": {}
    }
]