[
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "functions",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "pyspark.sql.functions",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "udf",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "year",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "month",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "dayofmonth",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "hour",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "year",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "month",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "dayofmonth",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "hour",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "config",
        "description": "config",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "ray",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ray",
        "description": "ray",
        "detail": "ray",
        "documentation": {}
    },
    {
        "label": "DataLakeServiceClient",
        "importPath": "azure.storage.filedatalake",
        "description": "azure.storage.filedatalake",
        "isExtraImport": true,
        "detail": "azure.storage.filedatalake",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "PdfReader",
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "isExtraImport": true,
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceInstructEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "importPath": "ecommbot.retrieval_generation",
        "description": "ecommbot.retrieval_generation",
        "isExtraImport": true,
        "detail": "ecommbot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "importPath": "ecommbot.converteur",
        "description": "ecommbot.converteur",
        "isExtraImport": true,
        "detail": "ecommbot.converteur",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "pyspark.ml.clustering",
        "description": "pyspark.ml.clustering",
        "isExtraImport": true,
        "detail": "pyspark.ml.clustering",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "importPath": "reader_data",
        "description": "reader_data",
        "isExtraImport": true,
        "detail": "reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "importPath": "reader_data",
        "description": "reader_data",
        "isExtraImport": true,
        "detail": "reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "importPath": "transformer",
        "description": "transformer",
        "isExtraImport": true,
        "detail": "transformer",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "StringIndexer",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "VectorAssembler",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "pyspark.ml",
        "description": "pyspark.ml",
        "isExtraImport": true,
        "detail": "pyspark.ml",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "pyspark.ml.classification",
        "description": "pyspark.ml.classification",
        "isExtraImport": true,
        "detail": "pyspark.ml.classification",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.config",
        "description": "CV_Processus.consumerSpark.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.consumerSpark.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']\n# Fonction d'extraction du texte directement depuis les bytes\ndef extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "extract_text_from_bytes",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:\n            text = \"\"\n            for page_num in range(doc.page_count):\n                page = doc.load_page(page_num)\n                text += page.get_text(\"text\")\n            return text\n    except Exception as e:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def ats_extractor(resume_data, api_key):\n    prompt = '''\n    You are an AI bot designed to act as a professional for parsing resumes. You are given a resume, and your job is to extract the following information:\n    1. first name\n    2. last name\n    1. full name\n    4. title\n    5. address\n    6. objective\n    7. date_of_birth",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "save_results_to_json",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def save_results_to_json(offset, text, analysis):\n    try:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{RESULTS_DIR}/resume_analysis_{offset}_{timestamp}.json\"\n        results = {\n            \"offset\": offset,\n            \"timestamp\": timestamp,\n            \"extracted_text\": text,\n            \"analysis\": json.loads(analysis) if isinstance(analysis, str) else analysis\n        }",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "process_pdf_udf",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def process_pdf_udf(value):\n    try:\n        # Les données arrivent déjà en bytes depuis Kafka\n        pdf_bytes = value\n        # Extraction du texte directement depuis les bytes\n        extracted_text = extract_text_from_bytes(pdf_bytes)\n        # Analyse avec Groq\n        analysis_result = ats_extractor(extracted_text, api_key)\n        return (extracted_text, json.dumps(analysis_result))\n    except Exception as e:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "process_batch",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def process_batch(df, epoch_id):\n    # Conversion du DataFrame en liste de dictionnaires pour traitement\n    rows = df.collect()\n    for row in rows:\n        save_results_to_json(\n            row['offset'],\n            row['extracted_text'],\n            row['analysis_result']\n        )\n# Lecture du stream Kafka",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "spark = SparkSession.builder.appName(\"PDF_Processor\").getOrCreate()\n# Création du dossier pour les résultats JSON s'il n'existe pas\nRESULTS_DIR = \"extracted_results\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n# Chargement de la configuration\ndef load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "RESULTS_DIR",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "RESULTS_DIR = \"extracted_results\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n# Chargement de la configuration\ndef load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']\n# Fonction d'extraction du texte directement depuis les bytes\ndef extract_text_from_bytes(pdf_bytes):",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "api_key = load_config()\n# Définition du schéma de sortie\noutput_schema = StructType([\n    StructField(\"pdf_text\", StringType(), True),\n    StructField(\"analysis_result\", StringType(), True)\n])\n# UDF pour le traitement complet\n@udf(returnType=output_schema)\ndef process_pdf_udf(value):\n    try:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "output_schema",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "output_schema = StructType([\n    StructField(\"pdf_text\", StringType(), True),\n    StructField(\"analysis_result\", StringType(), True)\n])\n# UDF pour le traitement complet\n@udf(returnType=output_schema)\ndef process_pdf_udf(value):\n    try:\n        # Les données arrivent déjà en bytes depuis Kafka\n        pdf_bytes = value",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"192.168.1.21:29093\") \\\n    .option(\"subscribe\", \"TopicCV\") \\\n    .load()\n# Application du traitement\nprocessed_df = df.select(\n    col(\"offset\"),\n    process_pdf_udf(col(\"value\")).alias(\"processed_data\")\n)\n# Extraction des colonnes du struct retourné par l'UDF",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "processed_df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "processed_df = df.select(\n    col(\"offset\"),\n    process_pdf_udf(col(\"value\")).alias(\"processed_data\")\n)\n# Extraction des colonnes du struct retourné par l'UDF\nfinal_df = processed_df.select(\n    col(\"offset\"),\n    col(\"processed_data.pdf_text\").alias(\"extracted_text\"),\n    col(\"processed_data.analysis_result\").alias(\"analysis_result\")\n)",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "final_df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "final_df = processed_df.select(\n    col(\"offset\"),\n    col(\"processed_data.pdf_text\").alias(\"extracted_text\"),\n    col(\"processed_data.analysis_result\").alias(\"analysis_result\")\n)\n# Configuration des streams de sortie\n# 1. Console output\nconsole_query = final_df.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "console_query",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "console_query = final_df.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\\n    .option(\"truncate\", False) \\\n    .start()\n# 2. Sauvegarde JSON via foreachBatch\njson_query = final_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attente de la terminaison des deux streams",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "json_query",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "json_query = final_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attente de la terminaison des deux streams\nspark.streams.awaitAnyTermination()",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.producer.config",
        "description": "CV_Processus.producer.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.producer.config",
        "documentation": {}
    },
    {
        "label": "producer",
        "kind": 5,
        "importPath": "CV_Processus.producer.prod",
        "description": "CV_Processus.producer.prod",
        "peekOfCode": "producer = KafkaProducer(bootstrap_servers='localhost:29092')\n# 2. Lire le fichier PDF en mode binaire\nwith open('document.pdf', 'rb') as fichier_pdf:\n    contenu_pdf = fichier_pdf.read()\n# 3. Envoyer les données en bytes dans Kafka\nproducer.send(config.topic, contenu_pdf)\nproducer.flush()\nproducer.close()",
        "detail": "CV_Processus.producer.prod",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.contat3",
        "description": "notoyage_data.contat3",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_type_trav.xlsx')\n# Charger le fichier contenant les types de contrat avec leurs identifiants\ncontrat_df = pd.read_excel('type_contrat_avec_id.xlsx')\n# Effectuer la jointure entre les deux DataFrames sur la colonne 'contrat'\ndata_with_id = data.merge(contrat_df, how='left', left_on='contrat', right_on='nom_type_contrat')\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata_with_id.to_excel('fichier_reduit_avec_id_type_contrat.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data_with_id.head())",
        "detail": "notoyage_data.contat3",
        "documentation": {}
    },
    {
        "label": "contrat_df",
        "kind": 5,
        "importPath": "notoyage_data.contat3",
        "description": "notoyage_data.contat3",
        "peekOfCode": "contrat_df = pd.read_excel('type_contrat_avec_id.xlsx')\n# Effectuer la jointure entre les deux DataFrames sur la colonne 'contrat'\ndata_with_id = data.merge(contrat_df, how='left', left_on='contrat', right_on='nom_type_contrat')\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata_with_id.to_excel('fichier_reduit_avec_id_type_contrat.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data_with_id.head())",
        "detail": "notoyage_data.contat3",
        "documentation": {}
    },
    {
        "label": "data_with_id",
        "kind": 5,
        "importPath": "notoyage_data.contat3",
        "description": "notoyage_data.contat3",
        "peekOfCode": "data_with_id = data.merge(contrat_df, how='left', left_on='contrat', right_on='nom_type_contrat')\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata_with_id.to_excel('fichier_reduit_avec_id_type_contrat.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data_with_id.head())",
        "detail": "notoyage_data.contat3",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.contrat2",
        "description": "notoyage_data.contrat2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_type_trav.xlsx')\n# Extraire les types de contrat uniques\nunique_contrats = data['contrat'].dropna().unique()\n# Créer un DataFrame avec l'id et le nom des types de contrat\ncontrat_df = pd.DataFrame({\n    'id_type_contrat': range(1, len(unique_contrats) + 1),\n    'nom_type_contrat': unique_contrats\n})\n# Sauvegarder le DataFrame dans un fichier Excel\ncontrat_df.to_excel('type_contrat_avec_id.xlsx', index=False)",
        "detail": "notoyage_data.contrat2",
        "documentation": {}
    },
    {
        "label": "unique_contrats",
        "kind": 5,
        "importPath": "notoyage_data.contrat2",
        "description": "notoyage_data.contrat2",
        "peekOfCode": "unique_contrats = data['contrat'].dropna().unique()\n# Créer un DataFrame avec l'id et le nom des types de contrat\ncontrat_df = pd.DataFrame({\n    'id_type_contrat': range(1, len(unique_contrats) + 1),\n    'nom_type_contrat': unique_contrats\n})\n# Sauvegarder le DataFrame dans un fichier Excel\ncontrat_df.to_excel('type_contrat_avec_id.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(contrat_df.head())",
        "detail": "notoyage_data.contrat2",
        "documentation": {}
    },
    {
        "label": "contrat_df",
        "kind": 5,
        "importPath": "notoyage_data.contrat2",
        "description": "notoyage_data.contrat2",
        "peekOfCode": "contrat_df = pd.DataFrame({\n    'id_type_contrat': range(1, len(unique_contrats) + 1),\n    'nom_type_contrat': unique_contrats\n})\n# Sauvegarder le DataFrame dans un fichier Excel\ncontrat_df.to_excel('type_contrat_avec_id.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(contrat_df.head())",
        "detail": "notoyage_data.contrat2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.entreprise2",
        "description": "notoyage_data.entreprise2",
        "peekOfCode": "data = pd.read_csv('salaries_max_convertis.csv')  # Le fichier principal\ncompanies = pd.read_csv('entreprises_avec_id.csv')  # Le fichier avec id_company\n# Fusionner les deux fichiers pour ajouter la colonne id_company\n# Fusionner sur la colonne \"Company\" (nom_company dans le fichier entreprises)\ndata_with_company_id = pd.merge(data, companies[['id_company', 'nom_company']],\n                                left_on='Company', right_on='nom_company',\n                                how='left')\n# Supprimer la colonne \"nom_company\" ajoutée par la fusion pour éviter la redondance\ndata_with_company_id.drop(columns=['nom_company'], inplace=True)\n# Sauvegarder le résultat dans un nouveau fichier Excel",
        "detail": "notoyage_data.entreprise2",
        "documentation": {}
    },
    {
        "label": "companies",
        "kind": 5,
        "importPath": "notoyage_data.entreprise2",
        "description": "notoyage_data.entreprise2",
        "peekOfCode": "companies = pd.read_csv('entreprises_avec_id.csv')  # Le fichier avec id_company\n# Fusionner les deux fichiers pour ajouter la colonne id_company\n# Fusionner sur la colonne \"Company\" (nom_company dans le fichier entreprises)\ndata_with_company_id = pd.merge(data, companies[['id_company', 'nom_company']],\n                                left_on='Company', right_on='nom_company',\n                                how='left')\n# Supprimer la colonne \"nom_company\" ajoutée par la fusion pour éviter la redondance\ndata_with_company_id.drop(columns=['nom_company'], inplace=True)\n# Sauvegarder le résultat dans un nouveau fichier Excel\ndata_with_company_id.to_csv('fichier_valide.csv', index=False)",
        "detail": "notoyage_data.entreprise2",
        "documentation": {}
    },
    {
        "label": "data_with_company_id",
        "kind": 5,
        "importPath": "notoyage_data.entreprise2",
        "description": "notoyage_data.entreprise2",
        "peekOfCode": "data_with_company_id = pd.merge(data, companies[['id_company', 'nom_company']],\n                                left_on='Company', right_on='nom_company',\n                                how='left')\n# Supprimer la colonne \"nom_company\" ajoutée par la fusion pour éviter la redondance\ndata_with_company_id.drop(columns=['nom_company'], inplace=True)\n# Sauvegarder le résultat dans un nouveau fichier Excel\ndata_with_company_id.to_csv('fichier_valide.csv', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data_with_company_id.head())",
        "detail": "notoyage_data.entreprise2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_contrat.xlsx')\n# Charger le fichier contenant la correspondance des pays avec les ID\nunique_countries = pd.read_excel('unique_countries.xlsx')\n# Fusionner les deux fichiers pour ajouter la colonne id_etat à data\ndata_with_ids = data.merge(unique_countries, how='left', left_on='Country', right_on='nom_etat')\n# Supprimer la colonne \"nom_etat\" si elle est inutile\ndata_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoutée\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperçu des résultats",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "unique_countries",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "unique_countries = pd.read_excel('unique_countries.xlsx')\n# Fusionner les deux fichiers pour ajouter la colonne id_etat à data\ndata_with_ids = data.merge(unique_countries, how='left', left_on='Country', right_on='nom_etat')\n# Supprimer la colonne \"nom_etat\" si elle est inutile\ndata_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoutée\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperçu des résultats\nprint(data_with_ids.head())",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "data_with_ids",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "data_with_ids = data.merge(unique_countries, how='left', left_on='Country', right_on='nom_etat')\n# Supprimer la colonne \"nom_etat\" si elle est inutile\ndata_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoutée\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperçu des résultats\nprint(data_with_ids.head())",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "data_with_ids",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "data_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoutée\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperçu des résultats\nprint(data_with_ids.head())",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "get_training_for_sector",
        "kind": 2,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "def get_training_for_sector(sector_name):\n    prompt = f\"Associer le secteur '{sector_name}' avec une formation parmi les suivantes : {', '.join(formations.values())}.\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"Vous êtes un assistant pour associer des secteurs à des formations.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Générer la réponse\n    response = groq_client.chat.completions.create(\n        model=\"llama3-8b-8192\",\n        messages=messages,",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "data_corrected",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "data_corrected = {\n    \"ID\": range(1, 46),  # Adjusting the range to match the length of the training list (1 to 45 inclusive)\n    \"Training\": [\n        \"Agronomy\",\n        \"Electrical Engineering\",\n        \"Architecture\",\n        \"Medicine\",\n        \"Computer Science\",\n        \"Finance\",\n        \"Management\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_corrected",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_corrected = pd.DataFrame(data_corrected)\n# Save to an Excel file\nfile_path_corrected = \"Training_List_Corrected.xlsx\"\ndf_corrected.to_excel(file_path_corrected, index=False)\nprint(f\"Excel file '{file_path_corrected}' has been created successfully.\")\nimport pandas as pd\n# Liste des secteurs\nsecteurs = {\n    \"ID_Secteur\": list(range(1, 205)),  # IDs des secteurs de 1 à 204\n    \"Nom_Secteur\": [",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "file_path_corrected",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "file_path_corrected = \"Training_List_Corrected.xlsx\"\ndf_corrected.to_excel(file_path_corrected, index=False)\nprint(f\"Excel file '{file_path_corrected}' has been created successfully.\")\nimport pandas as pd\n# Liste des secteurs\nsecteurs = {\n    \"ID_Secteur\": list(range(1, 205)),  # IDs des secteurs de 1 à 204\n    \"Nom_Secteur\": [\n        \"Diversified\", \"Financial Services\", \"Insurance\", \"Energy\", \"Infrastructure\", \"Logistics\",\n        \"Transportation\", \"Media & Entertainment\", \"Food and Beverage\", \"Telecommunications\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "secteurs",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "secteurs = {\n    \"ID_Secteur\": list(range(1, 205)),  # IDs des secteurs de 1 à 204\n    \"Nom_Secteur\": [\n        \"Diversified\", \"Financial Services\", \"Insurance\", \"Energy\", \"Infrastructure\", \"Logistics\",\n        \"Transportation\", \"Media & Entertainment\", \"Food and Beverage\", \"Telecommunications\",\n        \"Manufacturing\", \"Pharmaceuticals\", \"Industrial\", \"Conglomerate\", \"Financials\",\n        \"Travel and Leisure\", \"Elevators & Escalators\", \"Energy/Oil and Gas\",\n        \"Information Technology\", \"Utilities\", \"Engineering\", \"Airlines\", \"Healthcare\",\n        \"Real Estate\", \"Hospitality/Hotels\", \"Technology\", \"IT Services\", \"Consumer Goods\",\n        \"Technology/Music Streaming\", \"Automotive\", \"Media\", \"Beauty/Cosmetics\", \"Chemicals\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "formations = {\n    \"ID_Formation\": range(1, 46),  # IDs des formations de 1 à 45\n    \"Nom_Formation\": [\n        \"Agronomy\", \"Electrical Engineering\", \"Architecture\", \"Medicine\", \"Computer Science\",\n        \"Finance\", \"Management\", \"Law\", \"Marketing\", \"Civil Engineering\", \"Chemistry\",\n        \"Aeronautics\", \"Logistics\", \"Urban Planning\", \"Mechanical Engineering\", \"Pharmacy\",\n        \"Biotechnologies\", \"Renewable Energies\", \"Audiovisual\", \"Tourism\", \"Hospitality\",\n        \"Design\", \"Multimedia\", \"Data Science\", \"Artificial Intelligence\", \"Actuarial Science\",\n        \"Petroleum Engineering\", \"Production and Automation\", \"Electronics\", \"Communication\",\n        \"Environmental Sciences\", \"Cosmetics\", \"Agri-food\", \"Networks and Systems\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "secteurs_formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "secteurs_formations = {\n    \"ID_Secteur\": secteurs[\"ID_Secteur\"],\n    \"Nom_Secteur\": secteurs[\"Nom_Secteur\"],\n    \"ID_Formation\": [i % 45 + 1 for i in secteurs[\"ID_Secteur\"]],  # Associer un ID_Formation de façon cyclique\n    \"Nom_Formation\": [formations[\"Nom_Formation\"][i % 45] for i in secteurs[\"ID_Secteur\"]]\n}\n# Convertir en DataFrame\ndf_secteurs_formations = pd.DataFrame(secteurs_formations)\n# Sauvegarder dans un fichier Excel\ndf_secteurs_formations.to_excel(\"Secteurs_Formation.xlsx\", index=False)",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_secteurs_formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_secteurs_formations = pd.DataFrame(secteurs_formations)\n# Sauvegarder dans un fichier Excel\ndf_secteurs_formations.to_excel(\"Secteurs_Formation.xlsx\", index=False)\nprint(\"Le fichier 'Secteurs_Formation.xlsx' a été créé avec succès.\")\nimport pandas as pd\n# Liste des formations avec une association logique\nformations = [\n    (1, \"Agronomy\"),\n    (2, \"Finance\"),\n    (3, \"Insurance\"),",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "formations = [\n    (1, \"Agronomy\"),\n    (2, \"Finance\"),\n    (3, \"Insurance\"),\n    (4, \"Energy Engineering\"),\n    (5, \"Civil Engineering\"),\n    (6, \"Logistics\"),\n    (7, \"Transportation\"),\n    (8, \"Audiovisual\"),\n    (9, \"Agri-food\"),",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "secteurs",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "secteurs = [\n    (1, \"Diversified\"),\n    (2, \"Financial Services\"),\n    (3, \"Insurance\"),\n    (4, \"Energy\"),\n    (5, \"Infrastructure\"),\n    (6, \"Logistics\"),\n    (7, \"Transportation\"),\n    (8, \"Media & Entertainment\"),\n    (9, \"Food and Beverage\"),",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "correspondance",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "correspondance = [\n    {\n        \"ID Secteur\": secteur[0],\n        \"Nom Secteur\": secteur[1],\n        \"Formation Associée\": next(f[1] for f in formations if f[0] == secteur[0])\n    }\n    for secteur in secteurs\n]\n# Création du DataFrame avec les données\ndf = pd.DataFrame(correspondance)",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df = pd.DataFrame(correspondance)\n# Sauvegarde dans un fichier Excel\ndf.to_excel(\"secteurs_formations_association.xlsx\", index=False)\nprint(\"Le fichier Excel a été créé avec succès.\")\npip install groq\nimport pandas as pd\nimport yaml\nfrom groq import Groq # Remplacez cela avec la bibliothèque appropriée pour Llama\n# Charger les données des secteurs à partir du CSV\ndf_sectors = pd.read_csv('/content/secteurs_avec_id.csv')  # Remplacez par le chemin de votre CSV",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_sectors",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_sectors = pd.read_csv('/content/secteurs_avec_id.csv')  # Remplacez par le chemin de votre CSV\n# Liste des formations (ID, nom)\nformations = {\n    1: 'Agronomy',\n    2: 'Electrical Engineering',\n    3: 'Architecture',\n    4: 'Medicine',\n    5: 'Computer Science',\n    6: 'Finance',\n    7: 'Management',",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "formations = {\n    1: 'Agronomy',\n    2: 'Electrical Engineering',\n    3: 'Architecture',\n    4: 'Medicine',\n    5: 'Computer Science',\n    6: 'Finance',\n    7: 'Management',\n    8: 'Law',\n    9: 'Marketing',",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "CONFIG_PATH",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "CONFIG_PATH = r\"/content/config.yaml\"\napi_key = None\n# Charger la clé API depuis le fichier YAML\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialiser le client Groq pour Llama\ngroq_client = Groq(api_key=api_key)\n# Fonction pour interroger Llama et obtenir une formation\ndef get_training_for_sector(sector_name):",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "api_key = None\n# Charger la clé API depuis le fichier YAML\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialiser le client Groq pour Llama\ngroq_client = Groq(api_key=api_key)\n# Fonction pour interroger Llama et obtenir une formation\ndef get_training_for_sector(sector_name):\n    prompt = f\"Associer le secteur '{sector_name}' avec une formation parmi les suivantes : {', '.join(formations.values())}.\"",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "groq_client",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "groq_client = Groq(api_key=api_key)\n# Fonction pour interroger Llama et obtenir une formation\ndef get_training_for_sector(sector_name):\n    prompt = f\"Associer le secteur '{sector_name}' avec une formation parmi les suivantes : {', '.join(formations.values())}.\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"Vous êtes un assistant pour associer des secteurs à des formations.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Générer la réponse\n    response = groq_client.chat.completions.create(",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_sectors['id_formation']",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_sectors['id_formation'] = None\ndf_sectors['nom_formation'] = None\n# Associer chaque secteur à une formation\nfor idx, row in df_sectors.iterrows():\n    sector_name = row['nom_secteur']\n    id_formation, formation_name = get_training_for_sector(sector_name)\n    # Mettre à jour les colonnes du dataframe\n    df_sectors.at[idx, 'id_formation'] = id_formation\n    df_sectors.at[idx, 'nom_formation'] = formation_name\n# Sauvegarder le résultat dans un fichier Excel",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_sectors['nom_formation']",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_sectors['nom_formation'] = None\n# Associer chaque secteur à une formation\nfor idx, row in df_sectors.iterrows():\n    sector_name = row['nom_secteur']\n    id_formation, formation_name = get_training_for_sector(sector_name)\n    # Mettre à jour les colonnes du dataframe\n    df_sectors.at[idx, 'id_formation'] = id_formation\n    df_sectors.at[idx, 'nom_formation'] = formation_name\n# Sauvegarder le résultat dans un fichier Excel\ndf_sectors.to_excel('secteurs_formations_associées.xlsx', index=False)",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.secteurData",
        "description": "notoyage_data.secteurData",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_ville.xlsx')\nsecteurs_df = pd.read_excel('secteurs_avec_id.xlsx')\n# Associer chaque secteur à son id_secteur\n# Nous effectuons un merge (jointure) entre les deux DataFrames sur le nom du secteur\ndata = data.merge(secteurs_df, left_on='Secteur', right_on='nom_secteur', how='left')\n# Supprimer la colonne nom_secteur puisqu'on a déjà la colonne Secteur\ndata.drop(columns=['nom_secteur'], inplace=True)\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_id_secteur.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier",
        "detail": "notoyage_data.secteurData",
        "documentation": {}
    },
    {
        "label": "secteurs_df",
        "kind": 5,
        "importPath": "notoyage_data.secteurData",
        "description": "notoyage_data.secteurData",
        "peekOfCode": "secteurs_df = pd.read_excel('secteurs_avec_id.xlsx')\n# Associer chaque secteur à son id_secteur\n# Nous effectuons un merge (jointure) entre les deux DataFrames sur le nom du secteur\ndata = data.merge(secteurs_df, left_on='Secteur', right_on='nom_secteur', how='left')\n# Supprimer la colonne nom_secteur puisqu'on a déjà la colonne Secteur\ndata.drop(columns=['nom_secteur'], inplace=True)\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_id_secteur.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data.head())",
        "detail": "notoyage_data.secteurData",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.secteurData",
        "description": "notoyage_data.secteurData",
        "peekOfCode": "data = data.merge(secteurs_df, left_on='Secteur', right_on='nom_secteur', how='left')\n# Supprimer la colonne nom_secteur puisqu'on a déjà la colonne Secteur\ndata.drop(columns=['nom_secteur'], inplace=True)\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_id_secteur.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data.head())",
        "detail": "notoyage_data.secteurData",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "df = pd.read_excel('skills_output.xlsx')\n# Extraire les compétences uniques (distinctes) de la colonne 'Skill'\nunique_skills = df['Skill'].drop_duplicates().reset_index(drop=True)\n# Ajouter un identifiant unique commençant à 1\nunique_skills_df = pd.DataFrame({\n    'Skill ID': range(1, len(unique_skills) + 1),\n    'Skill': unique_skills\n})\n# Enregistrer les compétences distinctes dans un nouveau fichier Excel\ndistinct_skills_excel_file = 'distinct_skills.xlsx'",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "unique_skills",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "unique_skills = df['Skill'].drop_duplicates().reset_index(drop=True)\n# Ajouter un identifiant unique commençant à 1\nunique_skills_df = pd.DataFrame({\n    'Skill ID': range(1, len(unique_skills) + 1),\n    'Skill': unique_skills\n})\n# Enregistrer les compétences distinctes dans un nouveau fichier Excel\ndistinct_skills_excel_file = 'distinct_skills.xlsx'\nunique_skills_df.to_excel(distinct_skills_excel_file, index=False)\nprint(f\"Les compétences distinctes ont été enregistrées dans le fichier Excel : {distinct_skills_excel_file}\")",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "unique_skills_df",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "unique_skills_df = pd.DataFrame({\n    'Skill ID': range(1, len(unique_skills) + 1),\n    'Skill': unique_skills\n})\n# Enregistrer les compétences distinctes dans un nouveau fichier Excel\ndistinct_skills_excel_file = 'distinct_skills.xlsx'\nunique_skills_df.to_excel(distinct_skills_excel_file, index=False)\nprint(f\"Les compétences distinctes ont été enregistrées dans le fichier Excel : {distinct_skills_excel_file}\")",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "distinct_skills_excel_file",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "distinct_skills_excel_file = 'distinct_skills.xlsx'\nunique_skills_df.to_excel(distinct_skills_excel_file, index=False)\nprint(f\"Les compétences distinctes ont été enregistrées dans le fichier Excel : {distinct_skills_excel_file}\")",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "skills_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "skills_df = pd.read_excel('skills_output.xlsx')  # Contient 'Job Id' et 'Skill'\ndistinct_skills_df = pd.read_excel('distinct_skills.xlsx')  # Contient 'Skill ID' et 'Skill'\n# Fusionner les deux DataFrames pour obtenir 'Skill ID' à partir du 'Skill'\nmerged_df = pd.merge(skills_df, distinct_skills_df, on='Skill', how='inner')\n# Garder uniquement les colonnes 'Job Id' et 'Skill ID'\njob_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le résultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les données Job Id et Skill ID ont été enregistrées dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "distinct_skills_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "distinct_skills_df = pd.read_excel('distinct_skills.xlsx')  # Contient 'Skill ID' et 'Skill'\n# Fusionner les deux DataFrames pour obtenir 'Skill ID' à partir du 'Skill'\nmerged_df = pd.merge(skills_df, distinct_skills_df, on='Skill', how='inner')\n# Garder uniquement les colonnes 'Job Id' et 'Skill ID'\njob_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le résultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les données Job Id et Skill ID ont été enregistrées dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "merged_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "merged_df = pd.merge(skills_df, distinct_skills_df, on='Skill', how='inner')\n# Garder uniquement les colonnes 'Job Id' et 'Skill ID'\njob_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le résultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les données Job Id et Skill ID ont été enregistrées dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "job_skill_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "job_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le résultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les données Job Id et Skill ID ont été enregistrées dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "output_job_skill_excel",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "output_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les données Job Id et Skill ID ont été enregistrées dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "input_json_file",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "input_json_file = 'extracted_skills.json'  # Remplacez par le nom de votre fichier JSON\nwith open(input_json_file, 'r') as file:\n    json_data = json.load(file)\n# Créer une liste pour stocker les données structurées\ndata = []\n# Parcourir chaque entrée dans le fichier JSON\nfor entry in json_data:\n    job_id = entry[\"Job Id\"]\n    skills = entry[\"Skills\"]\n    # Omettre les deux premiers skills et le dernier",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "data = []\n# Parcourir chaque entrée dans le fichier JSON\nfor entry in json_data:\n    job_id = entry[\"Job Id\"]\n    skills = entry[\"Skills\"]\n    # Omettre les deux premiers skills et le dernier\n    filtered_skills = skills[2:-1]  # Exclut les deux premiers et le dernier\n    # Ajouter chaque skill avec l'ID correspondant à la liste de données\n    for skill in filtered_skills:\n        data.append({\"Job Id\": job_id, \"Skill\": skill})",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "df = pd.DataFrame(data)\n# Enregistrer les données dans un fichier Excel\noutput_excel_file = 'skills_output.xlsx'\ndf.to_excel(output_excel_file, index=False)\nprint(f\"Les données ont été transférées dans le fichier Excel : {output_excel_file}\")",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "output_excel_file",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "output_excel_file = 'skills_output.xlsx'\ndf.to_excel(output_excel_file, index=False)\nprint(f\"Les données ont été transférées dans le fichier Excel : {output_excel_file}\")",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "def ats_extractor(text):\n    # Prompt for extracting skills information\n    prompt = \"\"\"\n    You are an AI assistant. Your task is to extract a list of skills mentioned in the text provided by the user. \n    Please list the skills you can identify, separating them with commas.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": text}\n    ]",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "CONFIG_PATH",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "CONFIG_PATH = r\"config.yaml\"\napi_key = None\n# Load the API key from YAML configuration file\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialize the Groq API client\ngroq_client = Groq(api_key=api_key)\ndef ats_extractor(text):\n    # Prompt for extracting skills information",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "api_key = None\n# Load the API key from YAML configuration file\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialize the Groq API client\ngroq_client = Groq(api_key=api_key)\ndef ats_extractor(text):\n    # Prompt for extracting skills information\n    prompt = \"\"\"",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "groq_client",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "groq_client = Groq(api_key=api_key)\ndef ats_extractor(text):\n    # Prompt for extracting skills information\n    prompt = \"\"\"\n    You are an AI assistant. Your task is to extract a list of skills mentioned in the text provided by the user. \n    Please list the skills you can identify, separating them with commas.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": text}",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "excel_file_path",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "excel_file_path = 'fichier_reduit_avec_contrat.xlsx'  # Remplacez par le chemin de votre fichier Excel\ndata = pd.read_excel(excel_file_path)\n# Create a list to store JSON data\njson_data = []\n# Iterate over the rows in the Excel file\nfor index, row in data.iterrows():\n    job_id = row['Job Id']\n    skills_text = row['skills']\n    # Extract skills using the ats_extractor function\n    skills_list = ats_extractor(skills_text)",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "data = pd.read_excel(excel_file_path)\n# Create a list to store JSON data\njson_data = []\n# Iterate over the rows in the Excel file\nfor index, row in data.iterrows():\n    job_id = row['Job Id']\n    skills_text = row['skills']\n    # Extract skills using the ats_extractor function\n    skills_list = ats_extractor(skills_text)\n    # Append data to JSON structure",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "json_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "json_data = []\n# Iterate over the rows in the Excel file\nfor index, row in data.iterrows():\n    job_id = row['Job Id']\n    skills_text = row['skills']\n    # Extract skills using the ats_extractor function\n    skills_list = ats_extractor(skills_text)\n    # Append data to JSON structure\n    json_data.append({\n        \"Job Id\": job_id,",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "output_json_file",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "output_json_file = 'extracted_skills.json'\nwith open(output_json_file, 'w') as json_file:\n    json.dump(json_data, json_file, indent=4)\nprint(f\"Les compétences extraites ont été sauvegardées dans {output_json_file}\")",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "assign_language",
        "kind": 2,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "def assign_language(country):\n    if country == \"Morocco\":\n        # 90 % Français, 10 % Anglais\n        if random.random() < 0.9:\n            return 2  # ID de Français\n        else:\n            return 4  # ID d'Anglais\n    else:\n        # Si le pays n'est pas spécifié, retourner une langue par défaut\n        return 4  # Par exemple, ID de Arabe",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_contrat.xlsx')\n# Charger les langues depuis un autre fichier Excel ou définir manuellement\nlanguages = [\n    {\"id_lang\": 1, \"name\": \"Arabe\"},\n    {\"id_lang\": 2, \"name\": \"Français\"},\n    {\"id_lang\": 3, \"name\": \"Amazigh\"},\n    {\"id_lang\": 4, \"name\": \"Anglais\"},\n    {\"id_lang\": 13, \"name\": \"Espagnol\"},\n    # Ajoutez d'autres langues si nécessaire\n]",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "languages",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "languages = [\n    {\"id_lang\": 1, \"name\": \"Arabe\"},\n    {\"id_lang\": 2, \"name\": \"Français\"},\n    {\"id_lang\": 3, \"name\": \"Amazigh\"},\n    {\"id_lang\": 4, \"name\": \"Anglais\"},\n    {\"id_lang\": 13, \"name\": \"Espagnol\"},\n    # Ajoutez d'autres langues si nécessaire\n]\n# Convertir les langues en DataFrame\nlanguages_df = pd.DataFrame(languages)",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "languages_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "languages_df = pd.DataFrame(languages)\n# Fonction pour assigner une langue en fonction du pays\ndef assign_language(country):\n    if country == \"Morocco\":\n        # 90 % Français, 10 % Anglais\n        if random.random() < 0.9:\n            return 2  # ID de Français\n        else:\n            return 4  # ID d'Anglais\n    else:",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "data['id_lang']",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "data['id_lang'] = data['Country'].apply(assign_language)\n# Créer un DataFrame pour \"Job Id\" et \"id_lang\"\noutput_df = data[['Job Id', 'id_lang']]\n# Sauvegarder les résultats dans un fichier Excel\noutput_df.to_excel('job_id_languages.xlsx', index=False)\n# Afficher un aperçu des données sauvegardées\nprint(output_df.head())",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "output_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "output_df = data[['Job Id', 'id_lang']]\n# Sauvegarder les résultats dans un fichier Excel\noutput_df.to_excel('job_id_languages.xlsx', index=False)\n# Afficher un aperçu des données sauvegardées\nprint(output_df.head())",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert12_etat",
        "description": "notoyage_data.transfert12_etat",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_contrat.xlsx')\n# Extraire la colonne \"Country\" et supprimer les doublons\nunique_countries = data['Country'].drop_duplicates().reset_index(drop=True)\n# Générer un DataFrame avec id_etat et nom de l'état\nunique_countries_df = pd.DataFrame({\n    'id_etat': range(1, len(unique_countries) + 1),  # Générer des ID uniques\n    'nom_etat': unique_countries\n})\n# Sauvegarder le DataFrame dans un fichier Excel\nunique_countries_df.to_excel('unique_countries.xlsx', index=False)",
        "detail": "notoyage_data.transfert12_etat",
        "documentation": {}
    },
    {
        "label": "unique_countries",
        "kind": 5,
        "importPath": "notoyage_data.transfert12_etat",
        "description": "notoyage_data.transfert12_etat",
        "peekOfCode": "unique_countries = data['Country'].drop_duplicates().reset_index(drop=True)\n# Générer un DataFrame avec id_etat et nom de l'état\nunique_countries_df = pd.DataFrame({\n    'id_etat': range(1, len(unique_countries) + 1),  # Générer des ID uniques\n    'nom_etat': unique_countries\n})\n# Sauvegarder le DataFrame dans un fichier Excel\nunique_countries_df.to_excel('unique_countries.xlsx', index=False)\n# Afficher un aperçu des résultats\nprint(unique_countries_df.head())",
        "detail": "notoyage_data.transfert12_etat",
        "documentation": {}
    },
    {
        "label": "unique_countries_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert12_etat",
        "description": "notoyage_data.transfert12_etat",
        "peekOfCode": "unique_countries_df = pd.DataFrame({\n    'id_etat': range(1, len(unique_countries) + 1),  # Générer des ID uniques\n    'nom_etat': unique_countries\n})\n# Sauvegarder le DataFrame dans un fichier Excel\nunique_countries_df.to_excel('unique_countries.xlsx', index=False)\n# Afficher un aperçu des résultats\nprint(unique_countries_df.head())",
        "detail": "notoyage_data.transfert12_etat",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_etat.xlsx')\n# Extraire les colonnes 'Location' et 'id_etat'\ncities = data[['location', 'id_etat']].drop_duplicates()\n# Supprimer les lignes où 'Location' est vide ou NaN\ncities = cities.dropna(subset=['location'])\n# Générer un id_ville unique pour chaque ville\ncities['id_ville'] = range(1, len(cities) + 1)\n# Réorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clarté",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities = data[['location', 'id_etat']].drop_duplicates()\n# Supprimer les lignes où 'Location' est vide ou NaN\ncities = cities.dropna(subset=['location'])\n# Générer un id_ville unique pour chaque ville\ncities['id_ville'] = range(1, len(cities) + 1)\n# Réorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clarté\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities = cities.dropna(subset=['location'])\n# Générer un id_ville unique pour chaque ville\ncities['id_ville'] = range(1, len(cities) + 1)\n# Réorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clarté\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes\ncities.to_excel('villes_avec_id.xlsx', index=False)\n# Afficher un aperçu du résultat",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities['id_ville']",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities['id_ville'] = range(1, len(cities) + 1)\n# Réorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clarté\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes\ncities.to_excel('villes_avec_id.xlsx', index=False)\n# Afficher un aperçu du résultat\nprint(cities.head())",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clarté\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes\ncities.to_excel('villes_avec_id.xlsx', index=False)\n# Afficher un aperçu du résultat\nprint(cities.head())",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert14_secteur",
        "description": "notoyage_data.transfert14_secteur",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_ville.xlsx')\n# Extraire les secteurs uniques\nsecteurs_uniques = data['Secteur'].dropna().unique()\n# Créer un DataFrame avec un id pour chaque secteur unique\nsecteurs_df = pd.DataFrame({\n    'id_secteur': range(1, len(secteurs_uniques) + 1),\n    'nom_secteur': secteurs_uniques\n})\n# Sauvegarder le fichier avec les secteurs et leurs identifiants\nsecteurs_df.to_excel('secteurs_avec_id.xlsx', index=False)",
        "detail": "notoyage_data.transfert14_secteur",
        "documentation": {}
    },
    {
        "label": "secteurs_uniques",
        "kind": 5,
        "importPath": "notoyage_data.transfert14_secteur",
        "description": "notoyage_data.transfert14_secteur",
        "peekOfCode": "secteurs_uniques = data['Secteur'].dropna().unique()\n# Créer un DataFrame avec un id pour chaque secteur unique\nsecteurs_df = pd.DataFrame({\n    'id_secteur': range(1, len(secteurs_uniques) + 1),\n    'nom_secteur': secteurs_uniques\n})\n# Sauvegarder le fichier avec les secteurs et leurs identifiants\nsecteurs_df.to_excel('secteurs_avec_id.xlsx', index=False)\n# Afficher un aperçu des secteurs générés pour vérification\nprint(secteurs_df.head())",
        "detail": "notoyage_data.transfert14_secteur",
        "documentation": {}
    },
    {
        "label": "secteurs_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert14_secteur",
        "description": "notoyage_data.transfert14_secteur",
        "peekOfCode": "secteurs_df = pd.DataFrame({\n    'id_secteur': range(1, len(secteurs_uniques) + 1),\n    'nom_secteur': secteurs_uniques\n})\n# Sauvegarder le fichier avec les secteurs et leurs identifiants\nsecteurs_df.to_excel('secteurs_avec_id.xlsx', index=False)\n# Afficher un aperçu des secteurs générés pour vérification\nprint(secteurs_df.head())",
        "detail": "notoyage_data.transfert14_secteur",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_secteur.xlsx')\n# Extraire les types de travail uniques\nunique_work_types = data['Work Type'].dropna().unique()\n# Créer un DataFrame avec les types de travail et leur identifiant\nwork_type_df = pd.DataFrame(unique_work_types, columns=['nom_type_trav'])\n# Ajouter un identifiant pour chaque type de travail\nwork_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "unique_work_types",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "unique_work_types = data['Work Type'].dropna().unique()\n# Créer un DataFrame avec les types de travail et leur identifiant\nwork_type_df = pd.DataFrame(unique_work_types, columns=['nom_type_trav'])\n# Ajouter un identifiant pour chaque type de travail\nwork_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(work_type_df.head())",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "work_type_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "work_type_df = pd.DataFrame(unique_work_types, columns=['nom_type_trav'])\n# Ajouter un identifiant pour chaque type de travail\nwork_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(work_type_df.head())",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "work_type_df['id_type_trav']",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "work_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(work_type_df.head())",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "assign_contrat",
        "kind": 2,
        "importPath": "notoyage_data.transfert16_contrat",
        "description": "notoyage_data.transfert16_contrat",
        "peekOfCode": "def assign_contrat(row, cdi_ratio=0.4):\n    work_type = row['Work Type']\n    # Si le type de travail est \"Contract\", il y a une chance de 40% d'être CDI\n    if work_type == 'Contract':\n        if random.random() < cdi_ratio:  # 40% chance de CDI\n            return 'CDI'\n        else:\n            return 'CDD'\n    # Affecter les autres types directement\n    elif work_type == 'Full-Time':",
        "detail": "notoyage_data.transfert16_contrat",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert16_contrat",
        "description": "notoyage_data.transfert16_contrat",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_telephone_mise_a_jour.xlsx')\n# Fonction pour assigner les valeurs du contrat selon le type de travail\ndef assign_contrat(row, cdi_ratio=0.4):\n    work_type = row['Work Type']\n    # Si le type de travail est \"Contract\", il y a une chance de 40% d'être CDI\n    if work_type == 'Contract':\n        if random.random() < cdi_ratio:  # 40% chance de CDI\n            return 'CDI'\n        else:\n            return 'CDD'",
        "detail": "notoyage_data.transfert16_contrat",
        "documentation": {}
    },
    {
        "label": "data['contrat']",
        "kind": 5,
        "importPath": "notoyage_data.transfert16_contrat",
        "description": "notoyage_data.transfert16_contrat",
        "peekOfCode": "data['contrat'] = data.apply(assign_contrat, axis=1)\n# Sauvegarder le DataFrame mis à jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_contrat.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(data.head())",
        "detail": "notoyage_data.transfert16_contrat",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "data = pd.read_csv('salaries_max_convertis.csv')\n# Supprimer les doublons par entreprise et conserver les premières occurrences de chaque entreprise\nunique_companies = data.drop_duplicates(subset=['Company'])\n# Créer un DataFrame avec les informations demandées\ncompany_data = unique_companies[['Company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']].copy()\n# Renommer les colonnes pour correspondre au format demandé\ncompany_data.rename(columns={\n    'Company': 'nom_company',\n    'email': 'email',\n    'URL': 'URL',",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "unique_companies",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "unique_companies = data.drop_duplicates(subset=['Company'])\n# Créer un DataFrame avec les informations demandées\ncompany_data = unique_companies[['Company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']].copy()\n# Renommer les colonnes pour correspondre au format demandé\ncompany_data.rename(columns={\n    'Company': 'nom_company',\n    'email': 'email',\n    'URL': 'URL',\n    'telephone': 'telephone'\n}, inplace=True)",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "company_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "company_data = unique_companies[['Company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']].copy()\n# Renommer les colonnes pour correspondre au format demandé\ncompany_data.rename(columns={\n    'Company': 'nom_company',\n    'email': 'email',\n    'URL': 'URL',\n    'telephone': 'telephone'\n}, inplace=True)\n# Ajouter un identifiant unique pour chaque entreprise\ncompany_data['id_company'] = range(1, len(company_data) + 1)",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "company_data['id_company']",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "company_data['id_company'] = range(1, len(company_data) + 1)\n# Réorganiser les colonnes pour l'ordre souhaité\ncompany_data = company_data[['id_company', 'nom_company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']]\n# Sauvegarder les informations dans un nouveau fichier Excel\ncompany_data.to_csv('entreprises_avec_id.csv', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(company_data.head())",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "company_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "company_data = company_data[['id_company', 'nom_company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']]\n# Sauvegarder les informations dans un nouveau fichier Excel\ncompany_data.to_csv('entreprises_avec_id.csv', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(company_data.head())",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "data = pd.read_csv('fichier_valide.csv')\n# Sélectionner les colonnes souhaitées et renommer si nécessaire\nfields_to_keep = {\n    'Job Id': 'Job Id',              # Garder la colonne Job ID\n    'id_type_contrat': 'id_type_contrat',  # Garder la colonne id_type_contrat\n    'id_company': 'id_company',          # Garder la colonne id_company\n    'id_type_trav': 'id_type_trav',      # Garder la colonne id_type_trav\n    'salaire': 'salaire',                 # Renommer Salary en salaire\n    'Experience': 'Experience',          # Garder la colonne Experience\n    'email': 'email_offre'               # Renommer Email en email_offre",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "fields_to_keep",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "fields_to_keep = {\n    'Job Id': 'Job Id',              # Garder la colonne Job ID\n    'id_type_contrat': 'id_type_contrat',  # Garder la colonne id_type_contrat\n    'id_company': 'id_company',          # Garder la colonne id_company\n    'id_type_trav': 'id_type_trav',      # Garder la colonne id_type_trav\n    'salaire': 'salaire',                 # Renommer Salary en salaire\n    'Experience': 'Experience',          # Garder la colonne Experience\n    'email': 'email_offre'               # Renommer Email en email_offre\n}\nnew_data = data[list(fields_to_keep.keys())].rename(columns=fields_to_keep)",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "new_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "new_data = data[list(fields_to_keep.keys())].rename(columns=fields_to_keep)\n# Convertir la colonne 'salaire' en un nombre décimal\nnew_data['salaire'] = (\n    new_data['salaire']  # Travailler sur la colonne salaire\n    .replace({',': ''}, regex=True)  # Supprimer les virgules\n    .astype(float)  # Convertir en nombre décimal\n)\n# Sauvegarder le nouveau fichier Excel\nnew_data.to_csv('offres_emploi_filtrees2.csv', index=False)\n# Afficher un aperçu des premières lignes",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "new_data['salaire']",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "new_data['salaire'] = (\n    new_data['salaire']  # Travailler sur la colonne salaire\n    .replace({',': ''}, regex=True)  # Supprimer les virgules\n    .astype(float)  # Convertir en nombre décimal\n)\n# Sauvegarder le nouveau fichier Excel\nnew_data.to_csv('offres_emploi_filtrees2.csv', index=False)\n# Afficher un aperçu des premières lignes\nprint(new_data.head())",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "extract_max_salary",
        "kind": 2,
        "importPath": "notoyage_data.transfert1_salaire",
        "description": "notoyage_data.transfert1_salaire",
        "peekOfCode": "def extract_max_salary(salary_range):\n    # Diviser les valeurs en deux parties\n    _, max_salary = salary_range.split('-')\n    # Enlever le symbole $ et le K, puis multiplier par 100 pour obtenir le format décimal\n    max_decimal = float(max_salary.replace('$', '').replace('K', '')) * 100\n    # Retourner la valeur maximale en format décimal avec deux chiffres après la virgule\n    return f\"{max_decimal:,.2f}\"\n# Appliquer la conversion pour extraire la valeur maximale et créer la colonne 'salaire'\ndata['salaire'] = data['Salary Range'].apply(extract_max_salary)\n# Sauvegarder le résultat dans un nouveau fichier Excel",
        "detail": "notoyage_data.transfert1_salaire",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert1_salaire",
        "description": "notoyage_data.transfert1_salaire",
        "peekOfCode": "data = pd.read_excel('fichier_reduit.xlsx')\n# Fonction pour convertir la valeur maximale de la plage en décimal\ndef extract_max_salary(salary_range):\n    # Diviser les valeurs en deux parties\n    _, max_salary = salary_range.split('-')\n    # Enlever le symbole $ et le K, puis multiplier par 100 pour obtenir le format décimal\n    max_decimal = float(max_salary.replace('$', '').replace('K', '')) * 100\n    # Retourner la valeur maximale en format décimal avec deux chiffres après la virgule\n    return f\"{max_decimal:,.2f}\"\n# Appliquer la conversion pour extraire la valeur maximale et créer la colonne 'salaire'",
        "detail": "notoyage_data.transfert1_salaire",
        "documentation": {}
    },
    {
        "label": "data['salaire']",
        "kind": 5,
        "importPath": "notoyage_data.transfert1_salaire",
        "description": "notoyage_data.transfert1_salaire",
        "peekOfCode": "data['salaire'] = data['Salary Range'].apply(extract_max_salary)\n# Sauvegarder le résultat dans un nouveau fichier Excel\ndata.to_excel('salaries_max_convertis.xlsx', index=False)\nprint(\"La conversion est terminée et les données sont sauvegardées dans 'salaries_max_convertis.xlsx'.\")",
        "detail": "notoyage_data.transfert1_salaire",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert2_maroc_villes",
        "description": "notoyage_data.transfert2_maroc_villes",
        "peekOfCode": "data = pd.read_excel('salaries_max_convertis.xlsx')\n# Liste des villes marocaines\nmoroccan_cities = [\n    'Casablanca', 'Rabat', 'Salé', 'Agadir', 'Tanger', 'Fès',\n    'Marrakech', 'Meknès', 'Oujda', 'Kenitra', 'Tétouan', 'Safi',\n    'Mohammedia', 'El Jadida', 'Khouribga', 'Beni Mellal',\n    'Nador', 'Khemisset', 'Settat', 'Larache', 'Guelmim',\n    'Taza', 'Tan-Tan', 'Errachidia', 'Azrou', 'Ouarzazate',\n    'Laâyoune', 'Al Hoceima', 'Ifrane', 'Asilah'\n]",
        "detail": "notoyage_data.transfert2_maroc_villes",
        "documentation": {}
    },
    {
        "label": "moroccan_cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert2_maroc_villes",
        "description": "notoyage_data.transfert2_maroc_villes",
        "peekOfCode": "moroccan_cities = [\n    'Casablanca', 'Rabat', 'Salé', 'Agadir', 'Tanger', 'Fès',\n    'Marrakech', 'Meknès', 'Oujda', 'Kenitra', 'Tétouan', 'Safi',\n    'Mohammedia', 'El Jadida', 'Khouribga', 'Beni Mellal',\n    'Nador', 'Khemisset', 'Settat', 'Larache', 'Guelmim',\n    'Taza', 'Tan-Tan', 'Errachidia', 'Azrou', 'Ouarzazate',\n    'Laâyoune', 'Al Hoceima', 'Ifrane', 'Asilah'\n]\n# Sélectionner aléatoirement 700 indices de lignes dans le dataset\nindices_to_change = random.sample(range(len(data)), 700)",
        "detail": "notoyage_data.transfert2_maroc_villes",
        "documentation": {}
    },
    {
        "label": "indices_to_change",
        "kind": 5,
        "importPath": "notoyage_data.transfert2_maroc_villes",
        "description": "notoyage_data.transfert2_maroc_villes",
        "peekOfCode": "indices_to_change = random.sample(range(len(data)), 700)\n# Modifier les lignes sélectionnées avec le pays 'Morocco' et des villes marocaines aléatoires\nfor idx in indices_to_change:\n    data.at[idx, 'Country'] = 'Morocco'\n    data.at[idx, 'location'] = random.choice(moroccan_cities)\n# Sauvegarder le dataset modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_maroc.xlsx', index=False)\nprint(\"Modification terminée et les données sont sauvegardées dans 'fichier_reduit_avec_maroc.xlsx'.\")",
        "detail": "notoyage_data.transfert2_maroc_villes",
        "documentation": {}
    },
    {
        "label": "extract_sector",
        "kind": 2,
        "importPath": "notoyage_data.transfert3_cree_sector",
        "description": "notoyage_data.transfert3_cree_sector",
        "peekOfCode": "def extract_sector(company_profile):\n    try:\n        # Convertir la chaîne de caractères en dictionnaire\n        profile_dict = ast.literal_eval(company_profile)\n        # Retourner la valeur du secteur\n        return profile_dict.get('Sector', None)  # Renvoie None si 'Sector' n'existe pas\n    except:\n        return None\n# Appliquer la fonction à la colonne 'Company Profile' et créer la nouvelle colonne 'Secteur'\ndata['Secteur'] = data['Company Profile'].apply(extract_sector)",
        "detail": "notoyage_data.transfert3_cree_sector",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert3_cree_sector",
        "description": "notoyage_data.transfert3_cree_sector",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_maroc.xlsx')\n# Fonction pour extraire la valeur du secteur depuis la colonne \"Company Profile\"\ndef extract_sector(company_profile):\n    try:\n        # Convertir la chaîne de caractères en dictionnaire\n        profile_dict = ast.literal_eval(company_profile)\n        # Retourner la valeur du secteur\n        return profile_dict.get('Sector', None)  # Renvoie None si 'Sector' n'existe pas\n    except:\n        return None",
        "detail": "notoyage_data.transfert3_cree_sector",
        "documentation": {}
    },
    {
        "label": "data['Secteur']",
        "kind": 5,
        "importPath": "notoyage_data.transfert3_cree_sector",
        "description": "notoyage_data.transfert3_cree_sector",
        "peekOfCode": "data['Secteur'] = data['Company Profile'].apply(extract_sector)\n# Sauvegarder le dataset modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_secteur.xlsx', index=False)\nprint(\"Modification terminée et les données sont sauvegardées dans 'fichier_reduit_avec_secteur.xlsx'.\")",
        "detail": "notoyage_data.transfert3_cree_sector",
        "documentation": {}
    },
    {
        "label": "update_company",
        "kind": 2,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "def update_company(row):\n    country = row['Country']\n    secteur = row['Secteur']  # Récupérer le secteur\n    if country == 'Morocco' and secteur in companies_by_sector:\n        # Sélectionner une entreprise au hasard dans le secteur correspondant\n        company = random.choice(companies_by_sector[secteur])\n        row['Company'] = company  # Mettre à jour la colonne 'Company'\n    return row\n# Appliquer la fonction à chaque ligne du dataframe\ndata = data.apply(update_company, axis=1)",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "companies_by_sector",
        "kind": 5,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "companies_by_sector = {\n    'Energy': ['Nareva Holding', 'OCP Group', 'LafargeHolcim Maroc', 'Managem'],\n    'Consumer Goods': ['Cosumar', 'Sidi Ali', 'Danone Maroc', 'Unilever Maroc'],\n    'Healthcare Services': ['Clinique Internationale de Marrakech', 'Clinique Al Azhar', 'Réseau de santé Al Amal'],\n    'Insurance': ['Attijariwafa Assurance', 'RMA Watanya', 'Saham Assurance'],\n    'Lab Equipment': ['Biopharma', 'Medtech Maroc', 'Pharmatex'],\n    'Financial Services': ['Bank of Africa', 'Attijariwafa Bank', 'BMCE Bank of Africa', 'CIH Bank'],\n    'Healthcare Technology': ['Medisys', 'HemoTech', 'InnovHealth'],\n    'Industrial': ['LafargeHolcim Maroc', 'Maroc Chimie', 'Managem'],\n    'Logistics': ['CTM', 'Groupe BDP International', 'Transports Océan'],",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_secteur.xlsx')\n# Fonction pour mettre à jour la colonne 'Company' en fonction du secteur et du pays\ndef update_company(row):\n    country = row['Country']\n    secteur = row['Secteur']  # Récupérer le secteur\n    if country == 'Morocco' and secteur in companies_by_sector:\n        # Sélectionner une entreprise au hasard dans le secteur correspondant\n        company = random.choice(companies_by_sector[secteur])\n        row['Company'] = company  # Mettre à jour la colonne 'Company'\n    return row",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "data = data.apply(update_company, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_entreprises_mises_a_jour.xlsx', index=False)\nprint(\"Les entreprises ont été mises à jour dans le fichier 'fichier_reduit_avec_entreprises_mises_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "extract_url",
        "kind": 2,
        "importPath": "notoyage_data.transfert5_url",
        "description": "notoyage_data.transfert5_url",
        "peekOfCode": "def extract_url(company_profile):\n    try:\n        # Convertir le texte JSON en dictionnaire Python\n        profile = json.loads(company_profile)\n        # Extraire l'URL si elle existe\n        return profile.get(\"Website\", \"\")  # Retourne la valeur de \"Website\" ou une chaîne vide si non trouvé\n    except (json.JSONDecodeError, TypeError):\n        # Retourne une chaîne vide en cas d'erreur de format\n        return \"\"\n# Appliquer la fonction à la colonne 'Company Profile' pour créer la nouvelle colonne 'URL'",
        "detail": "notoyage_data.transfert5_url",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert5_url",
        "description": "notoyage_data.transfert5_url",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_entreprises_mises_a_jour.xlsx')\n# Fonction pour extraire l'URL du champ 'Company Profile'\ndef extract_url(company_profile):\n    try:\n        # Convertir le texte JSON en dictionnaire Python\n        profile = json.loads(company_profile)\n        # Extraire l'URL si elle existe\n        return profile.get(\"Website\", \"\")  # Retourne la valeur de \"Website\" ou une chaîne vide si non trouvé\n    except (json.JSONDecodeError, TypeError):\n        # Retourne une chaîne vide en cas d'erreur de format",
        "detail": "notoyage_data.transfert5_url",
        "documentation": {}
    },
    {
        "label": "data['URL']",
        "kind": 5,
        "importPath": "notoyage_data.transfert5_url",
        "description": "notoyage_data.transfert5_url",
        "peekOfCode": "data['URL'] = data['Company Profile'].apply(extract_url)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_URL.xlsx', index=False)\nprint(\"La colonne URL a été ajoutée et sauvegardée dans le fichier 'fichier_reduit_avec_URL.xlsx'.\")",
        "detail": "notoyage_data.transfert5_url",
        "documentation": {}
    },
    {
        "label": "update_url",
        "kind": 2,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "def update_url(row):\n    company_name = row['Company']\n    # Si l'entreprise existe dans le dictionnaire, on met à jour l'URL, sinon on garde l'ancienne valeur\n    return company_urls.get(company_name, row['URL'])  # Renvoyer l'ancienne URL si l'entreprise n'est pas trouvée\n# Appliquer la fonction à la colonne 'Company' pour mettre à jour la colonne 'URL'\ndata['URL'] = data.apply(update_url, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_URL_mis_a_jour.xlsx', index=False)\nprint(\"La colonne URL a été mise à jour et sauvegardée dans le fichier 'fichier_reduit_avec_URL_mis_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_URL.xlsx')\n# Dictionnaire des entreprises et de leurs URLs\ncompany_urls = {\n    \"Nareva Holding\": \"http://www.nareva.ma\",\n    \"Unilever Maroc\": \"https://www.unilever.com\",\n    \"Clinique Internationale de Marrakech\": \"http://www.cim.ma\",\n    \"Attijariwafa Assurance\": \"https://www.attijariwafassurance.ma\",\n    \"Pharmatex\": \"https://www.pharmatex.ma\",\n    \"BMCE Bank of Africa\": \"https://www.bmcebank.ma\",\n    \"HemoTech\": \"http://www.hemotech.com\",",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "company_urls",
        "kind": 5,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "company_urls = {\n    \"Nareva Holding\": \"http://www.nareva.ma\",\n    \"Unilever Maroc\": \"https://www.unilever.com\",\n    \"Clinique Internationale de Marrakech\": \"http://www.cim.ma\",\n    \"Attijariwafa Assurance\": \"https://www.attijariwafassurance.ma\",\n    \"Pharmatex\": \"https://www.pharmatex.ma\",\n    \"BMCE Bank of Africa\": \"https://www.bmcebank.ma\",\n    \"HemoTech\": \"http://www.hemotech.com\",\n    \"Maroc Chimie\": \"http://www.marocchimie.ma\",\n    \"Groupe BDP International\": \"http://www.bdp-group.com\",",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "data['URL']",
        "kind": 5,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "data['URL'] = data.apply(update_url, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_URL_mis_a_jour.xlsx', index=False)\nprint(\"La colonne URL a été mise à jour et sauvegardée dans le fichier 'fichier_reduit_avec_URL_mis_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "generate_email",
        "kind": 2,
        "importPath": "notoyage_data.transfert7_mail",
        "description": "notoyage_data.transfert7_mail",
        "peekOfCode": "def generate_email(row):\n    # Extraire le prénom et le nom de la colonne 'Contact Person'\n    contact_person = row['Contact Person']\n    if contact_person:  # Si la personne de contact n'est pas vide\n        name_parts = contact_person.split()  # Séparer le prénom et le nom\n        first_name = name_parts[0].lower()  # Prénom en minuscules\n        last_name = name_parts[-1].lower()  # Nom en minuscules\n        # Extraire le nom de l'entreprise\n        company_name = row['Company'].replace(\" \", \"_\").lower()  # Remplacer les espaces par des underscores et mettre en minuscules\n        # Créer l'email",
        "detail": "notoyage_data.transfert7_mail",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert7_mail",
        "description": "notoyage_data.transfert7_mail",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_URL_mis_a_jour.xlsx')\n# Fonction pour générer l'email\ndef generate_email(row):\n    # Extraire le prénom et le nom de la colonne 'Contact Person'\n    contact_person = row['Contact Person']\n    if contact_person:  # Si la personne de contact n'est pas vide\n        name_parts = contact_person.split()  # Séparer le prénom et le nom\n        first_name = name_parts[0].lower()  # Prénom en minuscules\n        last_name = name_parts[-1].lower()  # Nom en minuscules\n        # Extraire le nom de l'entreprise",
        "detail": "notoyage_data.transfert7_mail",
        "documentation": {}
    },
    {
        "label": "data['email']",
        "kind": 5,
        "importPath": "notoyage_data.transfert7_mail",
        "description": "notoyage_data.transfert7_mail",
        "peekOfCode": "data['email'] = data.apply(generate_email, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_email.xlsx', index=False)\nprint(\"La colonne email a été ajoutée et sauvegardée dans le fichier 'fichier_reduit_avec_email.xlsx'.\")",
        "detail": "notoyage_data.transfert7_mail",
        "documentation": {}
    },
    {
        "label": "generate_telephone",
        "kind": 2,
        "importPath": "notoyage_data.transfert8_telephone",
        "description": "notoyage_data.transfert8_telephone",
        "peekOfCode": "def generate_telephone(row):\n    contact_value = row['Contact']  # Récupérer la valeur de la colonne Contact\n    if contact_value:\n        # Limiter à 20 caractères\n        telephone_value = contact_value[:20]  # Prendre uniquement les 20 premiers caractères\n        return telephone_value\n    return \"\"  # Retourner une chaîne vide si le champ Contact est vide\n# Appliquer la fonction à chaque ligne du DataFrame pour créer la colonne 'telephone'\ndata['telephone'] = data.apply(generate_telephone, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel",
        "detail": "notoyage_data.transfert8_telephone",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert8_telephone",
        "description": "notoyage_data.transfert8_telephone",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_email.xlsx')\n# Fonction pour créer un téléphone à partir du Contact\ndef generate_telephone(row):\n    contact_value = row['Contact']  # Récupérer la valeur de la colonne Contact\n    if contact_value:\n        # Limiter à 20 caractères\n        telephone_value = contact_value[:20]  # Prendre uniquement les 20 premiers caractères\n        return telephone_value\n    return \"\"  # Retourner une chaîne vide si le champ Contact est vide\n# Appliquer la fonction à chaque ligne du DataFrame pour créer la colonne 'telephone'",
        "detail": "notoyage_data.transfert8_telephone",
        "documentation": {}
    },
    {
        "label": "data['telephone']",
        "kind": 5,
        "importPath": "notoyage_data.transfert8_telephone",
        "description": "notoyage_data.transfert8_telephone",
        "peekOfCode": "data['telephone'] = data.apply(generate_telephone, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_telephone.xlsx', index=False)\nprint(\"La colonne telephone a été ajoutée et sauvegardée dans le fichier 'fichier_reduit_avec_telephone.xlsx'.\")",
        "detail": "notoyage_data.transfert8_telephone",
        "documentation": {}
    },
    {
        "label": "generate_moroccan_phone_number",
        "kind": 2,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "def generate_moroccan_phone_number():\n    # Générer un numéro marocain au format +212 6XX XXX XXX\n    first_part = \"+212 6\"\n    second_part = str(random.randint(100, 999))  # Trois chiffres pour la seconde partie\n    third_part = str(random.randint(100, 999))  # Trois chiffres pour la troisième partie\n    fourth_part = str(random.randint(100, 999))  # Trois chiffres pour la quatrième partie\n    return f\"{first_part}{second_part} {third_part} {fourth_part}\"\n# Fonction pour mettre à jour la colonne telephone\ndef update_telephone(row):\n    contact_value = row['Contact']  # Récupérer la valeur de la colonne Contact",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "update_telephone",
        "kind": 2,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "def update_telephone(row):\n    contact_value = row['Contact']  # Récupérer la valeur de la colonne Contact\n    country_value = row['Country']  # Récupérer la valeur de la colonne Country\n    if country_value == \"Morocco\":\n        # Si le pays est le Maroc, remplacer le téléphone par un numéro marocain\n        return generate_moroccan_phone_number()\n    else:\n        # Si le pays n'est pas le Maroc, limiter la valeur à 20 caractères\n        if contact_value:\n            return contact_value[:20]  # Prendre uniquement les 20 premiers caractères",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_telephone.xlsx')\n# Fonction pour générer un numéro marocain fictif\ndef generate_moroccan_phone_number():\n    # Générer un numéro marocain au format +212 6XX XXX XXX\n    first_part = \"+212 6\"\n    second_part = str(random.randint(100, 999))  # Trois chiffres pour la seconde partie\n    third_part = str(random.randint(100, 999))  # Trois chiffres pour la troisième partie\n    fourth_part = str(random.randint(100, 999))  # Trois chiffres pour la quatrième partie\n    return f\"{first_part}{second_part} {third_part} {fourth_part}\"\n# Fonction pour mettre à jour la colonne telephone",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "data['telephone']",
        "kind": 5,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "data['telephone'] = data.apply(update_telephone, axis=1)\n# Sauvegarder le fichier modifié dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_telephone_mise_a_jour.xlsx', index=False)\nprint(\n    \"La colonne 'telephone' a été mise à jour et sauvegardée dans le fichier 'fichier_reduit_avec_telephone_mise_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.type_trav2",
        "description": "notoyage_data.type_trav2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_secteur.xlsx')\nwork_type_df = pd.read_excel('type_trav_avec_id.xlsx')\n# Fusionner les deux DataFrames sur la colonne 'Work Type' pour ajouter 'id_type_trav'\nmerged_data = pd.merge(data, work_type_df, how='left', left_on='Work Type', right_on='nom_type_trav')\n# Sauvegarder le fichier mis à jour dans un nouveau fichier Excel\nmerged_data.to_excel('fichier_reduit_avec_id_type_trav.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(merged_data.head())",
        "detail": "notoyage_data.type_trav2",
        "documentation": {}
    },
    {
        "label": "work_type_df",
        "kind": 5,
        "importPath": "notoyage_data.type_trav2",
        "description": "notoyage_data.type_trav2",
        "peekOfCode": "work_type_df = pd.read_excel('type_trav_avec_id.xlsx')\n# Fusionner les deux DataFrames sur la colonne 'Work Type' pour ajouter 'id_type_trav'\nmerged_data = pd.merge(data, work_type_df, how='left', left_on='Work Type', right_on='nom_type_trav')\n# Sauvegarder le fichier mis à jour dans un nouveau fichier Excel\nmerged_data.to_excel('fichier_reduit_avec_id_type_trav.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(merged_data.head())",
        "detail": "notoyage_data.type_trav2",
        "documentation": {}
    },
    {
        "label": "merged_data",
        "kind": 5,
        "importPath": "notoyage_data.type_trav2",
        "description": "notoyage_data.type_trav2",
        "peekOfCode": "merged_data = pd.merge(data, work_type_df, how='left', left_on='Work Type', right_on='nom_type_trav')\n# Sauvegarder le fichier mis à jour dans un nouveau fichier Excel\nmerged_data.to_excel('fichier_reduit_avec_id_type_trav.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérifier\nprint(merged_data.head())",
        "detail": "notoyage_data.type_trav2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.ville2",
        "description": "notoyage_data.ville2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_etat.xlsx')\n# Charger le fichier des villes avec id_ville et id_etat\ncities = pd.read_excel('villes_avec_id.xlsx')\n# Fusionner les données pour ajouter la colonne id_ville\n# en utilisant 'Location' et 'id_etat' comme clé de correspondance\nmerged_data = pd.merge(data, cities, how='left', left_on=['location', 'id_etat'], right_on=['nom_ville', 'id_etat'])\n# Supprimer la colonne 'nom_ville' utilisée uniquement pour la correspondance\nmerged_data.drop(columns=['nom_ville'], inplace=True)\n# Sauvegarder le fichier final avec la nouvelle colonne 'id_ville'\nmerged_data.to_excel('fichier_reduit_avec_id_ville.xlsx', index=False)",
        "detail": "notoyage_data.ville2",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.ville2",
        "description": "notoyage_data.ville2",
        "peekOfCode": "cities = pd.read_excel('villes_avec_id.xlsx')\n# Fusionner les données pour ajouter la colonne id_ville\n# en utilisant 'Location' et 'id_etat' comme clé de correspondance\nmerged_data = pd.merge(data, cities, how='left', left_on=['location', 'id_etat'], right_on=['nom_ville', 'id_etat'])\n# Supprimer la colonne 'nom_ville' utilisée uniquement pour la correspondance\nmerged_data.drop(columns=['nom_ville'], inplace=True)\n# Sauvegarder le fichier final avec la nouvelle colonne 'id_ville'\nmerged_data.to_excel('fichier_reduit_avec_id_ville.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérification\nprint(merged_data.head())",
        "detail": "notoyage_data.ville2",
        "documentation": {}
    },
    {
        "label": "merged_data",
        "kind": 5,
        "importPath": "notoyage_data.ville2",
        "description": "notoyage_data.ville2",
        "peekOfCode": "merged_data = pd.merge(data, cities, how='left', left_on=['location', 'id_etat'], right_on=['nom_ville', 'id_etat'])\n# Supprimer la colonne 'nom_ville' utilisée uniquement pour la correspondance\nmerged_data.drop(columns=['nom_ville'], inplace=True)\n# Sauvegarder le fichier final avec la nouvelle colonne 'id_ville'\nmerged_data.to_excel('fichier_reduit_avec_id_ville.xlsx', index=False)\n# Afficher un aperçu des 5 premières lignes pour vérification\nprint(merged_data.head())",
        "detail": "notoyage_data.ville2",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "offre_Process.data.config",
        "description": "offre_Process.data.config",
        "peekOfCode": "topic = \"offres_travail\"",
        "detail": "offre_Process.data.config",
        "documentation": {}
    },
    {
        "label": "send_text",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def send_text(producer, topic, text):\n    # Envoi du texte au topic Kafka\n    producer.send(topic, text.encode('utf-8'))\n# Fonction principale pour envoyer une série de messages texte à Kafka\ndef publish_texts(producer, topic, texts):\n    print('Publishing texts...')\n    for text in texts:\n        # Appel de la fonction pour envoyer le texte au topic Kafka\n        send_text(producer, topic, text)\n        # Délai entre chaque envoi pour simuler un envoi régulier",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "publish_texts",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def publish_texts(producer, topic, texts):\n    print('Publishing texts...')\n    for text in texts:\n        # Appel de la fonction pour envoyer le texte au topic Kafka\n        send_text(producer, topic, text)\n        # Délai entre chaque envoi pour simuler un envoi régulier\n        time.sleep(1)\n    print('Publish complete')\n# Point d'entrée du script (si le fichier est exécuté directement)\nif __name__ == \"__main__\":",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "extract_job_info",
        "kind": 2,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "def extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq\n        client = Groq(api_key=groq_api_key)\n        # Préparer le prompt pour Groq\n        request_content = f\"\"\"\nVeuillez extraire les informations suivantes de l'offre d'emploi et retournez-les strictement au format JSON sans texte supplémentaire. Si un champ n'existe pas, faites \"None\".\nExemple attendu :\n{{\n    \"titre_du_poste\": \"Développeur Python\",",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "groq_api_key",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "groq_api_key = os.getenv(\"GROQ_API_KEY\")\nkafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\n# Initialiser Spark\nspark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "kafka_bootstrap_servers",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "kafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\n# Initialiser Spark\nspark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):\n    try:",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "kafka_group_id",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "kafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\n# Initialiser Spark\nspark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "spark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq\n        client = Groq(api_key=groq_api_key)\n        # Préparer le prompt pour Groq",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "consumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=kafka_bootstrap_servers,\n    group_id=kafka_group_id,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\n# Liste des offres d'emploi extraites\njob_postings = []\nlogging.info(\"Démarrage de la consommation des messages Kafka...\")",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "job_postings",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "job_postings = []\nlogging.info(\"Démarrage de la consommation des messages Kafka...\")\n# Consommer les messages Kafka en temps réel\nfor message in consumer:\n    job_posting_text = message.value.decode(\"utf-8\")\n    logging.info(f\"Message reçu : {job_posting_text[:100]}...\")  # Afficher un extrait du message\n    job_postings.append(job_posting_text)\n    # Si vous voulez traiter les messages en temps réel, vous pouvez ajouter une logique pour un arrêt conditionnel.\n    if len(job_postings) >= 100:  # Par exemple, arrêter après avoir consommé 100 messages\n        break",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "rdd",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "rdd = spark.sparkContext.parallelize(job_postings)\nextracted_results = rdd.map(extract_job_info).collect()\nlogging.info(f\"Résultats extraits : {len(extracted_results)} offres traitées\")\n# Dossier local où stocker les fichiers JSON\nlocal_output_dir = \"./output_results\"\nos.makedirs(local_output_dir, exist_ok=True)  # Crée le dossier si nécessaire\nlogging.info(f\"Répertoire de sortie local : {local_output_dir}\")\n# Envoi des résultats localement\nfor result in extracted_results:\n    if isinstance(result, dict):  # S'assurer que le résultat est un dictionnaire avant de le traiter",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "extracted_results",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "extracted_results = rdd.map(extract_job_info).collect()\nlogging.info(f\"Résultats extraits : {len(extracted_results)} offres traitées\")\n# Dossier local où stocker les fichiers JSON\nlocal_output_dir = \"./output_results\"\nos.makedirs(local_output_dir, exist_ok=True)  # Crée le dossier si nécessaire\nlogging.info(f\"Répertoire de sortie local : {local_output_dir}\")\n# Envoi des résultats localement\nfor result in extracted_results:\n    if isinstance(result, dict):  # S'assurer que le résultat est un dictionnaire avant de le traiter\n        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "local_output_dir",
        "kind": 5,
        "importPath": "offre_Process.Ray.consS",
        "description": "offre_Process.Ray.consS",
        "peekOfCode": "local_output_dir = \"./output_results\"\nos.makedirs(local_output_dir, exist_ok=True)  # Crée le dossier si nécessaire\nlogging.info(f\"Répertoire de sortie local : {local_output_dir}\")\n# Envoi des résultats localement\nfor result in extracted_results:\n    if isinstance(result, dict):  # S'assurer que le résultat est un dictionnaire avant de le traiter\n        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n        output_file = f\"resultat_offre_{timestamp}.json\"\n        output_path = os.path.join(local_output_dir, output_file)\n        # Conversion des résultats en JSON",
        "detail": "offre_Process.Ray.consS",
        "documentation": {}
    },
    {
        "label": "initialize_adls_client",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "def initialize_adls_client():\n    try:\n        # Vérification de la clé d'accès\n        if not azure_storage_key:\n            raise ValueError(\"La clé d'accès (AZURE_STORAGE_KEY) est manquante.\")\n        # Initialiser le client avec la clé d'accès\n        service_client = DataLakeServiceClient(account_url=account_url, credential=azure_storage_key)\n        print(\"Connexion au Data Lake réussie avec la clé d'accès.\")\n        return service_client\n    except Exception as e:",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "upload_to_adls",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "def upload_to_adls(file_name, content, file_system_name, directory_name):\n    try:\n        # Vérification et conversion du contenu en chaîne UTF-8\n        if isinstance(content, str):\n            content_utf8 = content.encode('utf-8')  # Convertir en bytes UTF-8 si c'est déjà une chaîne\n        else:\n            content_utf8 = content  # Si le contenu est déjà en bytes, pas besoin de conversion\n        service_client = initialize_adls_client()\n        file_system_client = service_client.get_file_system_client(file_system_name)\n        directory_client = file_system_client.get_directory_client(directory_name)",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "extract_job_info",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "def extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq\n        client = Groq(api_key=groq_api_key)\n        # Préparer le prompt pour Groq\n        request_content = f\"\"\"\nVeuillez extraire les informations suivantes de l'offre d'emploi et retournez-les strictement au format JSON sans texte supplémentaire. Si un champ n'existe pas, faites \"None\".\nExemple attendu :\n{{\n    \"titre_du_poste\": \"Développeur Python\",",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "account_url",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "account_url = os.getenv(\"AZURE_ACCOUNT_URL\")\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\nkafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "groq_api_key",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "groq_api_key = os.getenv(\"GROQ_API_KEY\")\nkafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "kafka_bootstrap_servers",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "kafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès\ndef initialize_adls_client():",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "kafka_group_id",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "kafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès\ndef initialize_adls_client():\n    try:",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "azure_storage_key",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "azure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès\ndef initialize_adls_client():\n    try:\n        # Vérification de la clé d'accès",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "consumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=kafka_bootstrap_servers,\n    group_id=kafka_group_id,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\n# Lire les messages Kafka et traiter chaque offre d'emploi\nfile_system_name = os.getenv(\"file_system_name\")\ndirectory_name = os.getenv(\"directory_name\")",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "file_system_name",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "file_system_name = os.getenv(\"file_system_name\")\ndirectory_name = os.getenv(\"directory_name\")\nfor message in consumer:\n    job_posting_text = message.value.decode(\"utf-8\")\n    # Envoi du texte de l'offre d'emploi à Ray\n    handle = extract_job_info.remote(job_posting_text)\n    result = ray.get(handle)\n    # Création du nom de fichier avec horodatage\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"resultat_offre_{timestamp}.json\"",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "directory_name",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "directory_name = os.getenv(\"directory_name\")\nfor message in consumer:\n    job_posting_text = message.value.decode(\"utf-8\")\n    # Envoi du texte de l'offre d'emploi à Ray\n    handle = extract_job_info.remote(job_posting_text)\n    result = ray.get(handle)\n    # Création du nom de fichier avec horodatage\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"resultat_offre_{timestamp}.json\"\n    # Conversion des résultats en JSON",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    # Sauvegarder les données traitées dans la table Delta avec la fusion de schémas\n    df.write.format(\"delta\") \\\n        .mode(\"append\") \\\n        .option(\"mergeSchema\", \"true\") \\\n        .save(output_path)\n    print(\"Data written to Delta Table\")\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n# Définir le schéma pour les offres d'emploi\nOFFER_SCHEMA = StructType([",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "extract_job_info",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "def extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq\n        client = Groq(api_key=groq_api_key)\n        # Préparer le prompt pour Groq\n        request_content = f\"\"\"\nVeuillez extraire les informations suivantes de l'offre d'emploi et retournez-les strictement au format JSON sans texte supplémentaire. Si un champ n'existe pas, faites \"None\".\nExemple attendu :\n{{\n    \"titre_du_poste\": \"Développeur Python\",",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "OFFER_SCHEMA",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "OFFER_SCHEMA = StructType([\n    StructField(\"titre_du_poste\", StringType(), True),\n    StructField(\"societe\", StringType(), True),\n    StructField(\"competences\", ArrayType(StringType()), True),\n    StructField(\"lieu\", StringType(), True),\n    StructField(\"type_offre\", StringType(), True),\n    StructField(\"durée\", StringType(), True),\n    StructField(\"type_de_contrat\", StringType(), True),\n    StructField(\"email\", StringType(), True),\n    StructField(\"telephone\", StringType(), True),",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "groq_api_key",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "groq_api_key = os.getenv(\"GROQ_API_KEY\")\nkafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\n# Initialiser Spark\nspark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "kafka_bootstrap_servers",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "kafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\n# Initialiser Spark\nspark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):\n    try:",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "kafka_group_id",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "kafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\n# Initialiser Spark\nspark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "spark = SparkSession.builder.appName(\"JobPostingExtractor\").getOrCreate()\n# Vérification des variables d'environnement critiques\nif not all([groq_api_key, kafka_bootstrap_servers, kafka_group_id]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour extraire les informations d'une offre d'emploi\ndef extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq\n        client = Groq(api_key=groq_api_key)\n        # Préparer le prompt pour Groq",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "consumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=kafka_bootstrap_servers,\n    group_id=kafka_group_id,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\n# Liste des offres d'emploi extraites\njob_postings = []\nlogging.info(\"Démarrage de la consommation des messages Kafka...\")",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "job_postings",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "job_postings = []\nlogging.info(\"Démarrage de la consommation des messages Kafka...\")\n# Consommer les messages Kafka en temps réel\nfor message in consumer:\n    job_posting_text = message.value.decode(\"utf-8\")\n    logging.info(f\"Message reçu : {job_posting_text[:100]}...\")  # Afficher un extrait du message\n    job_postings.append(job_posting_text)\n    # Limite pour test\n    if len(job_postings) >= 5:\n        break",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "rdd",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "rdd = spark.sparkContext.parallelize(job_postings)\nextracted_results = rdd.map(extract_job_info).collect()\n# Créer un DataFrame en utilisant le schéma défini\ndf = spark.read.json(rdd, schema=OFFER_SCHEMA )\nlogging.info(f\"Résultats extraits : {len(extracted_results)} offres traitées\")\n# Sauvegarder les données traitées dans la table Delta\nADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "extracted_results",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "extracted_results = rdd.map(extract_job_info).collect()\n# Créer un DataFrame en utilisant le schéma défini\ndf = spark.read.json(rdd, schema=OFFER_SCHEMA )\nlogging.info(f\"Résultats extraits : {len(extracted_results)} offres traitées\")\n# Sauvegarder les données traitées dans la table Delta\nADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "df = spark.read.json(rdd, schema=OFFER_SCHEMA )\nlogging.info(f\"Résultats extraits : {len(extracted_results)} offres traitées\")\n# Sauvegarder les données traitées dans la table Delta\nADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\nsave_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "ADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\nsave_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "ADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\nsave_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "ADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\nsave_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumerSpark",
        "description": "offre_Process.Ray.consumerSpark",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\nsave_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.Ray.consumerSpark",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.converteur",
        "description": "Recruitment_chatBot.recruitboot.converteur",
        "peekOfCode": "def get_pdf_text(pdf_docs):\n    text = \"\"\n    for pdf in pdf_docs:\n        # Vérifiez si le chemin du fichier PDF est valide\n        if not os.path.exists(pdf):\n            print(f\"Le fichier {pdf} n'existe pas.\")\n            continue  # Passez au fichier suivant si celui-ci n'existe pas\n        try:\n            pdf_reader = PdfReader(pdf)\n            # Parcourir toutes les pages du PDF et extraire le texte",
        "detail": "Recruitment_chatBot.recruitboot.converteur",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.ingest",
        "description": "Recruitment_chatBot.recruitboot.ingest",
        "peekOfCode": "def get_text_chunks(text):\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    chunks = text_splitter.split_text(text)\n    return chunks\ndef get_vectorstore(text_chunks):",
        "detail": "Recruitment_chatBot.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.ingest",
        "description": "Recruitment_chatBot.recruitboot.ingest",
        "peekOfCode": "def get_vectorstore(text_chunks):\n    # Charger les embeddings Hugging Face\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    # Créer le vecteur FAISS à partir des textes et des embeddings\n    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n    return vectorstore",
        "detail": "Recruitment_chatBot.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "description": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "peekOfCode": "def create_conversation_chain(vectorstore):\n    if not isinstance(vectorstore, FAISS):\n        raise TypeError(\"Le vectorstore doit être un objet de type FAISS.\")\n    # Initialisation du LLM avec HuggingFaceHub\n    llm = HuggingFaceHub(\n        repo_id=\"google/flan-t5-large\",\n        huggingfacehub_api_token=\"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\",\n        model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n    )\n    # Configuration de la mémoire pour conserver l'historique des conversations",
        "detail": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "def index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 2,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "def chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)\n        # Retourner uniquement la réponse textuelle sans l'objet complet\n        answer = result[\"answer\"] if \"answer\" in result else \"Désolé, je n'ai pas pu trouver une réponse.\"\n        return jsonify({\"response\": answer})  # Retourner la réponse du chatbot",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "app = Flask(__name__)\nload_dotenv()\n# Fonction de génération de la chaîne conversationnelle (pour intégrer LangChain)\n# Traitement du PDF\npdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "pdf_docs",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "pdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "text = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "chunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "vectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "conversation_chain",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "conversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "kind": 6,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "class CandidatDataLoader:\n    def __init__(self, jdbc_url=\"jdbc:postgresql://localhost:5433/cond_db\"):\n        \"\"\"\n        Initialise la classe pour charger les données depuis PostgreSQL.\n        :param jdbc_url: URL de connexion JDBC à la base de données PostgreSQL.\n        \"\"\"\n        self.jdbc_url = jdbc_url\n        self.query = \"\"\"\n        SELECT \n            c.full_name AS candidate_name,",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatSegmentation",
        "kind": 6,
        "importPath": "segmentationSpark.segmentation",
        "description": "segmentationSpark.segmentation",
        "peekOfCode": "class CandidatSegmentation:\n    def __init__(self, df, k=3, seed=42):\n        \"\"\"\n        Initialise la classe pour la segmentation des candidats.\n        :param df: DataFrame Spark contenant les données transformées.\n        :param k: Nombre de clusters pour KMeans.\n        :param seed: Seed pour la reproductibilité.\n        \"\"\"\n        self.df = df\n        self.k = k",
        "detail": "segmentationSpark.segmentation",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "kind": 6,
        "importPath": "segmentationSpark.transformer",
        "description": "segmentationSpark.transformer",
        "peekOfCode": "class CandidatTransformer:\n    def __init__(self, df):\n        \"\"\"\n        Initialise la classe avec un DataFrame Spark.\n        :param df: DataFrame Spark contenant les données.\n        \"\"\"\n        self.df = df\n    def build_pipeline(self):\n        \"\"\"\n        Construit un pipeline pour transformer les données.",
        "detail": "segmentationSpark.transformer",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "def create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    # Vérifier si la table Delta existe\n    try:\n        delta_table = DeltaTable.forPath(spark, path)",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    # Sauvegarder les données traitées dans la table Delta avec la fusion de schémas\n    df.write.format(\"delta\") \\\n        .mode(\"append\") \\\n        .option(\"mergeSchema\", \"true\") \\\n        .save(output_path)\n    print(\"Data written to Delta Table\")\n# ===================================================================================\n#                           MAIN ENTRYPOINT\n# ===================================================================================",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "json_data",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "json_data = {\n    \"first_name\": \"None\",\n    \"last_name\": \"None\",\n    \"full_name\": \"None\",\n    \"title\": \"Information Technology Specialist\",\n    \"address\": {\n        \"formatted_location\": \"None\",\n        \"city\": \"None\",\n        \"region\": \"None\",\n        \"country\": \"None\",",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "ADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "ADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "ADLS_FOLDER_PATH = \"CV\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",\n]\ndef create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "spark = create_or_get_spark(\n    app_name=\"json_to_delta\", packages=PACKAGES, cluster_manager=\"local[*]\"\n)\n# Configurer la connexion à Azure\nspark.conf.set(\n    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n    ADLS_ACCOUNT_KEY,\n)\nprint(\"Spark Session Created\")\n# Lire les données JSON depuis la variable",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "last",
        "description": "last",
        "peekOfCode": "df = spark.read.json(spark.sparkContext.parallelize([json_data]), schema=PERSON_SCHEMA)\nprint(\"JSON data loaded\")\n# Traiter les données\nprint(\"Data processed\")\n# Créer une table Delta vide (si elle n'existe pas encore)\ncreate_empty_delta_table(\n    spark=spark,\n    schema=PERSON_SCHEMA,\n    path=OUTPUT_PATH,\n    partition_cols=[\"first_name\"],",
        "detail": "last",
        "documentation": {}
    },
    {
        "label": "clean_column",
        "kind": 2,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "def clean_column(col):\n    if isinstance(col.dataType, StructType):\n        return F.when(F.col(col.name).isNull(), None).otherwise(F.col(col.name))\n    elif isinstance(col.dataType, ArrayType):\n        return F.when(F.col(col.name).isNull(), None).otherwise(F.col(col.name))\n    else:\n        return F.when(F.col(col).equalTo(\"None\"), None).otherwise(F.col(col))\n# Appliquer la fonction de nettoyage sur toutes les colonnes\ndf_clean = df.select(\n    [clean_column(c).alias(c) for c in df.columns]",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "spark = SparkSession.builder.appName(\"JsonProcessing\").getOrCreate()\nfrom pyspark.sql.types import StructType, StructField, StringType, ArrayType\n# Définir le schéma pour le JSON\nPERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "json_file_path",
        "kind": 5,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "json_file_path = \"/kaggle/input/datajob/data.json\"\n# Charger les données JSON en utilisant le schéma défini\ndf = spark.read.json(json_file_path, schema=PERSON_SCHEMA)\n# Fonction pour remplacer \"None\" par null dans les colonnes STRUCT ou ARRAY\ndef clean_column(col):\n    if isinstance(col.dataType, StructType):\n        return F.when(F.col(col.name).isNull(), None).otherwise(F.col(col.name))\n    elif isinstance(col.dataType, ArrayType):\n        return F.when(F.col(col.name).isNull(), None).otherwise(F.col(col.name))\n    else:",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "df = spark.read.json(json_file_path, schema=PERSON_SCHEMA)\n# Fonction pour remplacer \"None\" par null dans les colonnes STRUCT ou ARRAY\ndef clean_column(col):\n    if isinstance(col.dataType, StructType):\n        return F.when(F.col(col.name).isNull(), None).otherwise(F.col(col.name))\n    elif isinstance(col.dataType, ArrayType):\n        return F.when(F.col(col.name).isNull(), None).otherwise(F.col(col.name))\n    else:\n        return F.when(F.col(col).equalTo(\"None\"), None).otherwise(F.col(col))\n# Appliquer la fonction de nettoyage sur toutes les colonnes",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "df_clean",
        "kind": 5,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "df_clean = df.select(\n    [clean_column(c).alias(c) for c in df.columns]\n)\n# Afficher le DataFrame pour vérifier les données\ndf_clean.show(truncate=False)\n# Sauvegarder les données traitées dans Delta\ndelta_output_path = \"abfss://offres@dataoffre.dfs.core.windows.net/CV\"\ndf_clean.write.format(\"delta\").mode(\"overwrite\").save(delta_output_path)\nprint(\"Données traitées et sauvegardées dans Delta.\")",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "delta_output_path",
        "kind": 5,
        "importPath": "schema",
        "description": "schema",
        "peekOfCode": "delta_output_path = \"abfss://offres@dataoffre.dfs.core.windows.net/CV\"\ndf_clean.write.format(\"delta\").mode(\"overwrite\").save(delta_output_path)\nprint(\"Données traitées et sauvegardées dans Delta.\")",
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "def create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    # Vérifier si la table Delta existe\n    try:\n        delta_table = DeltaTable.forPath(spark, path)",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    # Sauvegarder les données traitées dans la table Delta avec la fusion de schémas\n    df.write.format(\"delta\") \\\n        .mode(\"append\") \\\n        .option(\"mergeSchema\", \"true\") \\\n        .save(output_path)\n    print(\"Data written to Delta Table\")\n# ===================================================================================\n#                           MAIN ENTRYPOINT\n# ===================================================================================",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"first_name\", StringType(), True),\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "json_data",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "json_data = {\n    \"first_name\": \"None\",\n    \"last_name\": \"None\",\n    \"full_name\": \"None\",\n    \"title\": \"Information Technology Specialist\",\n    \"address\": {\n        \"formatted_location\": \"None\",\n        \"city\": \"None\",\n        \"region\": \"None\",\n        \"country\": \"None\",",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"\"\nADLS_FOLDER_PATH = \"\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "ADLS_ACCOUNT_KEY = \"\"  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"\"\nADLS_FOLDER_PATH = \"\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "ADLS_CONTAINER_NAME = \"\"\nADLS_FOLDER_PATH = \"\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "ADLS_FOLDER_PATH = \"\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",\n]\ndef create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "spark = create_or_get_spark(\n    app_name=\"json_to_delta\", packages=PACKAGES, cluster_manager=\"local[*]\"\n)\n# Configurer la connexion à Azure\nspark.conf.set(\n    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n    ADLS_ACCOUNT_KEY,\n)\nprint(\"Spark Session Created\")\n# Lire les données JSON depuis la variable",
        "detail": "spark_TO_delta",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "spark_TO_delta",
        "description": "spark_TO_delta",
        "peekOfCode": "df = spark.read.json(spark.sparkContext.parallelize([json_data]), schema=PERSON_SCHEMA)\nprint(\"JSON data loaded\")\n# Traiter les données\nprint(\"Data processed\")\n# Créer une table Delta vide (si elle n'existe pas encore)\ncreate_empty_delta_table(\n    spark=spark,\n    schema=PERSON_SCHEMA,\n    path=OUTPUT_PATH,\n    partition_cols=[\"first_name\"],",
        "detail": "spark_TO_delta",
        "documentation": {}
    }
]