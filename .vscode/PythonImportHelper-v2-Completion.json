[
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "udf",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "config",
        "description": "config",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "ray",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ray",
        "description": "ray",
        "detail": "ray",
        "documentation": {}
    },
    {
        "label": "DataLakeServiceClient",
        "importPath": "azure.storage.filedatalake",
        "description": "azure.storage.filedatalake",
        "isExtraImport": true,
        "detail": "azure.storage.filedatalake",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "PdfReader",
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "isExtraImport": true,
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceInstructEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "importPath": "ecommbot.retrieval_generation",
        "description": "ecommbot.retrieval_generation",
        "isExtraImport": true,
        "detail": "ecommbot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "importPath": "ecommbot.converteur",
        "description": "ecommbot.converteur",
        "isExtraImport": true,
        "detail": "ecommbot.converteur",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "pyspark.ml.clustering",
        "description": "pyspark.ml.clustering",
        "isExtraImport": true,
        "detail": "pyspark.ml.clustering",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "pyspark.ml",
        "description": "pyspark.ml",
        "isExtraImport": true,
        "detail": "pyspark.ml",
        "documentation": {}
    },
    {
        "label": "StringIndexer",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "VectorAssembler",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "pyspark.ml.classification",
        "description": "pyspark.ml.classification",
        "isExtraImport": true,
        "detail": "pyspark.ml.classification",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.config",
        "description": "CV_Processus.consumerSpark.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.consumerSpark.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']\n# Fonction d'extraction du texte directement depuis les bytes\ndef extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "extract_text_from_bytes",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:\n            text = \"\"\n            for page_num in range(doc.page_count):\n                page = doc.load_page(page_num)\n                text += page.get_text(\"text\")\n            return text\n    except Exception as e:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def ats_extractor(resume_data, api_key):\n    prompt = '''\n    You are an AI bot designed to act as a professional for parsing resumes. You are given a resume, and your job is to extract the following information:\n    1. first name\n    2. last name\n    1. full name\n    4. title\n    5. address\n    6. objective\n    7. date_of_birth",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "save_results_to_json",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def save_results_to_json(offset, text, analysis):\n    try:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{RESULTS_DIR}/resume_analysis_{offset}_{timestamp}.json\"\n        results = {\n            \"offset\": offset,\n            \"timestamp\": timestamp,\n            \"extracted_text\": text,\n            \"analysis\": json.loads(analysis) if isinstance(analysis, str) else analysis\n        }",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "process_pdf_udf",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def process_pdf_udf(value):\n    try:\n        # Les données arrivent déjà en bytes depuis Kafka\n        pdf_bytes = value\n        # Extraction du texte directement depuis les bytes\n        extracted_text = extract_text_from_bytes(pdf_bytes)\n        # Analyse avec Groq\n        analysis_result = ats_extractor(extracted_text, api_key)\n        return (extracted_text, json.dumps(analysis_result))\n    except Exception as e:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "process_batch",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def process_batch(df, epoch_id):\n    # Conversion du DataFrame en liste de dictionnaires pour traitement\n    rows = df.collect()\n    for row in rows:\n        save_results_to_json(\n            row['offset'],\n            row['extracted_text'],\n            row['analysis_result']\n        )\n# Lecture du stream Kafka",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "spark = SparkSession.builder.appName(\"PDF_Processor\").getOrCreate()\n# Création du dossier pour les résultats JSON s'il n'existe pas\nRESULTS_DIR = \"extracted_results\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n# Chargement de la configuration\ndef load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "RESULTS_DIR",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "RESULTS_DIR = \"extracted_results\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n# Chargement de la configuration\ndef load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']\n# Fonction d'extraction du texte directement depuis les bytes\ndef extract_text_from_bytes(pdf_bytes):",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "api_key = load_config()\n# Définition du schéma de sortie\noutput_schema = StructType([\n    StructField(\"pdf_text\", StringType(), True),\n    StructField(\"analysis_result\", StringType(), True)\n])\n# UDF pour le traitement complet\n@udf(returnType=output_schema)\ndef process_pdf_udf(value):\n    try:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "output_schema",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "output_schema = StructType([\n    StructField(\"pdf_text\", StringType(), True),\n    StructField(\"analysis_result\", StringType(), True)\n])\n# UDF pour le traitement complet\n@udf(returnType=output_schema)\ndef process_pdf_udf(value):\n    try:\n        # Les données arrivent déjà en bytes depuis Kafka\n        pdf_bytes = value",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"192.168.1.21:29093\") \\\n    .option(\"subscribe\", \"TopicCV\") \\\n    .load()\n# Application du traitement\nprocessed_df = df.select(\n    col(\"offset\"),\n    process_pdf_udf(col(\"value\")).alias(\"processed_data\")\n)\n# Extraction des colonnes du struct retourné par l'UDF",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "processed_df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "processed_df = df.select(\n    col(\"offset\"),\n    process_pdf_udf(col(\"value\")).alias(\"processed_data\")\n)\n# Extraction des colonnes du struct retourné par l'UDF\nfinal_df = processed_df.select(\n    col(\"offset\"),\n    col(\"processed_data.pdf_text\").alias(\"extracted_text\"),\n    col(\"processed_data.analysis_result\").alias(\"analysis_result\")\n)",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "final_df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "final_df = processed_df.select(\n    col(\"offset\"),\n    col(\"processed_data.pdf_text\").alias(\"extracted_text\"),\n    col(\"processed_data.analysis_result\").alias(\"analysis_result\")\n)\n# Configuration des streams de sortie\n# 1. Console output\nconsole_query = final_df.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "console_query",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "console_query = final_df.writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"console\") \\\n    .option(\"truncate\", False) \\\n    .start()\n# 2. Sauvegarde JSON via foreachBatch\njson_query = final_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attente de la terminaison des deux streams",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "json_query",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "json_query = final_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attente de la terminaison des deux streams\nspark.streams.awaitAnyTermination()",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.producer.config",
        "description": "CV_Processus.producer.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.producer.config",
        "documentation": {}
    },
    {
        "label": "producer",
        "kind": 5,
        "importPath": "CV_Processus.producer.prod",
        "description": "CV_Processus.producer.prod",
        "peekOfCode": "producer = KafkaProducer(bootstrap_servers='localhost:29092')\n# 2. Lire le fichier PDF en mode binaire\nwith open('document.pdf', 'rb') as fichier_pdf:\n    contenu_pdf = fichier_pdf.read()\n# 3. Envoyer les données en bytes dans Kafka\nproducer.send(config.topic, contenu_pdf)\nproducer.flush()\nproducer.close()",
        "detail": "CV_Processus.producer.prod",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "offre_Process.data.config",
        "description": "offre_Process.data.config",
        "peekOfCode": "topic = \"offres_travail\"",
        "detail": "offre_Process.data.config",
        "documentation": {}
    },
    {
        "label": "send_text",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def send_text(producer, topic, text):\n    # Envoi du texte au topic Kafka\n    producer.send(topic, text.encode('utf-8'))\n# Fonction principale pour envoyer une série de messages texte à Kafka\ndef publish_texts(producer, topic, texts):\n    print('Publishing texts...')\n    for text in texts:\n        # Appel de la fonction pour envoyer le texte au topic Kafka\n        send_text(producer, topic, text)\n        # Délai entre chaque envoi pour simuler un envoi régulier",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "publish_texts",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def publish_texts(producer, topic, texts):\n    print('Publishing texts...')\n    for text in texts:\n        # Appel de la fonction pour envoyer le texte au topic Kafka\n        send_text(producer, topic, text)\n        # Délai entre chaque envoi pour simuler un envoi régulier\n        time.sleep(1)\n    print('Publish complete')\n# Point d'entrée du script (si le fichier est exécuté directement)\nif __name__ == \"__main__\":",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "initialize_adls_client",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "def initialize_adls_client():\n    try:\n        # Vérification de la clé d'accès\n        if not azure_storage_key:\n            raise ValueError(\"La clé d'accès (AZURE_STORAGE_KEY) est manquante.\")\n        # Initialiser le client avec la clé d'accès\n        service_client = DataLakeServiceClient(account_url=account_url, credential=azure_storage_key)\n        print(\"Connexion au Data Lake réussie avec la clé d'accès.\")\n        return service_client\n    except Exception as e:",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "upload_to_adls",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "def upload_to_adls(file_name, content, file_system_name, directory_name):\n    try:\n        # Vérification et conversion du contenu en chaîne UTF-8\n        if isinstance(content, str):\n            content_utf8 = content.encode('utf-8')  # Convertir en bytes UTF-8 si c'est déjà une chaîne\n        else:\n            content_utf8 = content  # Si le contenu est déjà en bytes, pas besoin de conversion\n        service_client = initialize_adls_client()\n        file_system_client = service_client.get_file_system_client(file_system_name)\n        directory_client = file_system_client.get_directory_client(directory_name)",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "extract_job_info",
        "kind": 2,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "def extract_job_info(job_posting_text):\n    try:\n        # Initialiser le client Groq\n        client = Groq(api_key=groq_api_key)\n        # Préparer le prompt pour Groq\n        request_content = f\"\"\"\nVeuillez extraire les informations suivantes de l'offre d'emploi et retournez-les strictement au format JSON sans texte supplémentaire. Si un champ n'existe pas, faites \"None\".\nExemple attendu :\n{{\n    \"titre_du_poste\": \"Développeur Python\",",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "account_url",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "account_url = os.getenv(\"AZURE_ACCOUNT_URL\")\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\nkafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "groq_api_key",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "groq_api_key = os.getenv(\"GROQ_API_KEY\")\nkafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "kafka_bootstrap_servers",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "kafka_bootstrap_servers = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nkafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès\ndef initialize_adls_client():",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "kafka_group_id",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "kafka_group_id = os.getenv(\"KAFKA_GROUP_ID\")\nazure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès\ndef initialize_adls_client():\n    try:",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "azure_storage_key",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "azure_storage_key = os.getenv(\"AZURE_STORAGE_KEY\")\n# Initialiser Ray\nray.init()\n# Vérification des variables d'environnement critiques\nif not all([account_url, groq_api_key, kafka_bootstrap_servers, kafka_group_id, azure_storage_key]):\n    raise ValueError(\"Une ou plusieurs variables d'environnement sont manquantes.\")\n# Fonction pour initialiser le client ADLS avec une clé d'accès\ndef initialize_adls_client():\n    try:\n        # Vérification de la clé d'accès",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "consumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=kafka_bootstrap_servers,\n    group_id=kafka_group_id,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\n# Lire les messages Kafka et traiter chaque offre d'emploi\nfile_system_name = os.getenv(\"file_system_name\")\ndirectory_name = os.getenv(\"directory_name\")",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "file_system_name",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "file_system_name = os.getenv(\"file_system_name\")\ndirectory_name = os.getenv(\"directory_name\")\nfor message in consumer:\n    job_posting_text = message.value.decode(\"utf-8\")\n    # Envoi du texte de l'offre d'emploi à Ray\n    handle = extract_job_info.remote(job_posting_text)\n    result = ray.get(handle)\n    # Création du nom de fichier avec horodatage\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"resultat_offre_{timestamp}.json\"",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "directory_name",
        "kind": 5,
        "importPath": "offre_Process.Ray.consumer",
        "description": "offre_Process.Ray.consumer",
        "peekOfCode": "directory_name = os.getenv(\"directory_name\")\nfor message in consumer:\n    job_posting_text = message.value.decode(\"utf-8\")\n    # Envoi du texte de l'offre d'emploi à Ray\n    handle = extract_job_info.remote(job_posting_text)\n    result = ray.get(handle)\n    # Création du nom de fichier avec horodatage\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    output_file = f\"resultat_offre_{timestamp}.json\"\n    # Conversion des résultats en JSON",
        "detail": "offre_Process.Ray.consumer",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.converteur",
        "description": "Recruitment_chatBot.recruitboot.converteur",
        "peekOfCode": "def get_pdf_text(pdf_docs):\n    text = \"\"\n    for pdf in pdf_docs:\n        # Vérifiez si le chemin du fichier PDF est valide\n        if not os.path.exists(pdf):\n            print(f\"Le fichier {pdf} n'existe pas.\")\n            continue  # Passez au fichier suivant si celui-ci n'existe pas\n        try:\n            pdf_reader = PdfReader(pdf)\n            # Parcourir toutes les pages du PDF et extraire le texte",
        "detail": "Recruitment_chatBot.recruitboot.converteur",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.ingest",
        "description": "Recruitment_chatBot.recruitboot.ingest",
        "peekOfCode": "def get_text_chunks(text):\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    chunks = text_splitter.split_text(text)\n    return chunks\ndef get_vectorstore(text_chunks):",
        "detail": "Recruitment_chatBot.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.ingest",
        "description": "Recruitment_chatBot.recruitboot.ingest",
        "peekOfCode": "def get_vectorstore(text_chunks):\n    # Charger les embeddings Hugging Face\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    # Créer le vecteur FAISS à partir des textes et des embeddings\n    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n    return vectorstore",
        "detail": "Recruitment_chatBot.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "description": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "peekOfCode": "def create_conversation_chain(vectorstore):\n    if not isinstance(vectorstore, FAISS):\n        raise TypeError(\"Le vectorstore doit être un objet de type FAISS.\")\n    # Initialisation du LLM avec HuggingFaceHub\n    llm = HuggingFaceHub(\n        repo_id=\"google/flan-t5-large\",\n        huggingfacehub_api_token=\"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\",\n        model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n    )\n    # Configuration de la mémoire pour conserver l'historique des conversations",
        "detail": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "def index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 2,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "def chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)\n        # Retourner uniquement la réponse textuelle sans l'objet complet\n        answer = result[\"answer\"] if \"answer\" in result else \"Désolé, je n'ai pas pu trouver une réponse.\"\n        return jsonify({\"response\": answer})  # Retourner la réponse du chatbot",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "app = Flask(__name__)\nload_dotenv()\n# Fonction de génération de la chaîne conversationnelle (pour intégrer LangChain)\n# Traitement du PDF\npdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "pdf_docs",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "pdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "text = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "chunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "vectorstore = get_vectorstore(chunks)\n# Création de la chaîne de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "conversation_chain",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "conversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel à invoke() pour obtenir la réponse",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "CandidatSegmentation",
        "kind": 6,
        "importPath": "segmentationSpark.Candidat_segementation",
        "description": "segmentationSpark.Candidat_segementation",
        "peekOfCode": "class CandidatSegmentation:\n    def __init__(self, df, k=3, seed=42):\n        \"\"\"\n        Initialise la classe pour la segmentation des candidats.\n        :param df: DataFrame Spark contenant les données transformées.\n        :param k: Nombre de clusters pour KMeans.\n        :param seed: Seed pour la reproductibilité.\n        \"\"\"\n        self.df = df\n        self.k = k",
        "detail": "segmentationSpark.Candidat_segementation",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "spark = SparkSession.builder \\\n    .appName(\"SegmentationCandidats\") \\\n    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n    .getOrCreate()\n# Lire les données depuis PostgreSQL\njdbc_url = \"jdbc:postgresql://localhost:5433/candidat\"\ntable_name = \"personnel\"\nproperties = {\n    \"user\": \"postgres\",\n    \"password\": \"papapapa\",",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "jdbc_url",
        "kind": 5,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "jdbc_url = \"jdbc:postgresql://localhost:5433/candidat\"\ntable_name = \"personnel\"\nproperties = {\n    \"user\": \"postgres\",\n    \"password\": \"papapapa\",\n    \"driver\": \"org.postgresql.Driver\"\n}\ndf_candidats = spark.read.jdbc(url=jdbc_url, table=\"personnel\", properties=properties)\ndf_candidats.show()",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "table_name",
        "kind": 5,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "table_name = \"personnel\"\nproperties = {\n    \"user\": \"postgres\",\n    \"password\": \"papapapa\",\n    \"driver\": \"org.postgresql.Driver\"\n}\ndf_candidats = spark.read.jdbc(url=jdbc_url, table=\"personnel\", properties=properties)\ndf_candidats.show()",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "properties",
        "kind": 5,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "properties = {\n    \"user\": \"postgres\",\n    \"password\": \"papapapa\",\n    \"driver\": \"org.postgresql.Driver\"\n}\ndf_candidats = spark.read.jdbc(url=jdbc_url, table=\"personnel\", properties=properties)\ndf_candidats.show()",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "df_candidats",
        "kind": 5,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "df_candidats = spark.read.jdbc(url=jdbc_url, table=\"personnel\", properties=properties)\ndf_candidats.show()",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "kind": 6,
        "importPath": "segmentationSpark.transformer",
        "description": "segmentationSpark.transformer",
        "peekOfCode": "class CandidatTransformer:\n    def __init__(self, df):\n        \"\"\"\n        Initialise la classe avec un DataFrame Spark.\n        :param df: DataFrame Spark contenant les données.\n        \"\"\"\n        self.df = df\n    def build_pipeline(self):\n        \"\"\"\n        Construit un pipeline pour transformer les données.",
        "detail": "segmentationSpark.transformer",
        "documentation": {}
    }
]