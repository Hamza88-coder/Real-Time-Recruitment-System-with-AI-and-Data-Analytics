[
    {
        "label": "dotenv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dotenv",
        "description": "dotenv",
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "pyspark.sql.functions",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "udf",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "udf",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "year",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "month",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "dayofmonth",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "hour",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "IntegerType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "DoubleType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "TimestampType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StructField",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "StringType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "ArrayType",
        "importPath": "pyspark.sql.types",
        "description": "pyspark.sql.types",
        "isExtraImport": true,
        "detail": "pyspark.sql.types",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "DeltaTable",
        "importPath": "delta.tables",
        "description": "delta.tables",
        "isExtraImport": true,
        "detail": "delta.tables",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHPERSON_SCHEMA",
        "importPath": "CV_Processus.consumerSpark.schema",
        "description": "CV_Processus.consumerSpark.schema",
        "isExtraImport": true,
        "detail": "CV_Processus.consumerSpark.schema",
        "documentation": {}
    },
    {
        "label": "api_key",
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "isExtraImport": true,
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaProducer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "KafkaConsumer",
        "importPath": "kafka",
        "description": "kafka",
        "isExtraImport": true,
        "detail": "kafka",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "config",
        "description": "config",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "FlaskForm",
        "importPath": "flask_wtf",
        "description": "flask_wtf",
        "isExtraImport": true,
        "detail": "flask_wtf",
        "documentation": {}
    },
    {
        "label": "FileField",
        "importPath": "flask_wtf.file",
        "description": "flask_wtf.file",
        "isExtraImport": true,
        "detail": "flask_wtf.file",
        "documentation": {}
    },
    {
        "label": "FileAllowed",
        "importPath": "flask_wtf.file",
        "description": "flask_wtf.file",
        "isExtraImport": true,
        "detail": "flask_wtf.file",
        "documentation": {}
    },
    {
        "label": "StringField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "PasswordField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "SubmitField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "BooleanField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "SelectField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "IntegerField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "TextAreaField",
        "importPath": "wtforms",
        "description": "wtforms",
        "isExtraImport": true,
        "detail": "wtforms",
        "documentation": {}
    },
    {
        "label": "DataRequired",
        "importPath": "wtforms.validators",
        "description": "wtforms.validators",
        "isExtraImport": true,
        "detail": "wtforms.validators",
        "documentation": {}
    },
    {
        "label": "Length",
        "importPath": "wtforms.validators",
        "description": "wtforms.validators",
        "isExtraImport": true,
        "detail": "wtforms.validators",
        "documentation": {}
    },
    {
        "label": "Email",
        "importPath": "wtforms.validators",
        "description": "wtforms.validators",
        "isExtraImport": true,
        "detail": "wtforms.validators",
        "documentation": {}
    },
    {
        "label": "EqualTo",
        "importPath": "wtforms.validators",
        "description": "wtforms.validators",
        "isExtraImport": true,
        "detail": "wtforms.validators",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "wtforms.validators",
        "description": "wtforms.validators",
        "isExtraImport": true,
        "detail": "wtforms.validators",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "User",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "Jobs",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "Review",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "Application",
        "importPath": "app.models",
        "description": "app.models",
        "isExtraImport": true,
        "detail": "app.models",
        "documentation": {}
    },
    {
        "label": "db",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "login_manager",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "db",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "bcrypt",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "app",
        "description": "app",
        "isExtraImport": true,
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "UserMixin",
        "importPath": "flask_login",
        "description": "flask_login",
        "isExtraImport": true,
        "detail": "flask_login",
        "documentation": {}
    },
    {
        "label": "login_user",
        "importPath": "flask_login",
        "description": "flask_login",
        "isExtraImport": true,
        "detail": "flask_login",
        "documentation": {}
    },
    {
        "label": "current_user",
        "importPath": "flask_login",
        "description": "flask_login",
        "isExtraImport": true,
        "detail": "flask_login",
        "documentation": {}
    },
    {
        "label": "logout_user",
        "importPath": "flask_login",
        "description": "flask_login",
        "isExtraImport": true,
        "detail": "flask_login",
        "documentation": {}
    },
    {
        "label": "login_required",
        "importPath": "flask_login",
        "description": "flask_login",
        "isExtraImport": true,
        "detail": "flask_login",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "url_for",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "flash",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "redirect",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "send_file",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "redirect",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "url_for",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "redirect",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "url_for",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "secrets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "secrets",
        "description": "secrets",
        "detail": "secrets",
        "documentation": {}
    },
    {
        "label": "RegistrationForm",
        "importPath": "app.forms",
        "description": "app.forms",
        "isExtraImport": true,
        "detail": "app.forms",
        "documentation": {}
    },
    {
        "label": "LoginForm",
        "importPath": "app.forms",
        "description": "app.forms",
        "isExtraImport": true,
        "detail": "app.forms",
        "documentation": {}
    },
    {
        "label": "ReviewForm",
        "importPath": "app.forms",
        "description": "app.forms",
        "isExtraImport": true,
        "detail": "app.forms",
        "documentation": {}
    },
    {
        "label": "JobForm",
        "importPath": "app.forms",
        "description": "app.forms",
        "isExtraImport": true,
        "detail": "app.forms",
        "documentation": {}
    },
    {
        "label": "ApplicationForm",
        "importPath": "app.forms",
        "description": "app.forms",
        "isExtraImport": true,
        "detail": "app.forms",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "extract_job_info",
        "importPath": "extractor",
        "description": "extractor",
        "isExtraImport": true,
        "detail": "extractor",
        "documentation": {}
    },
    {
        "label": "OFFRE_SCHEMA",
        "importPath": "schema",
        "description": "schema",
        "isExtraImport": true,
        "detail": "schema",
        "documentation": {}
    },
    {
        "label": "gensim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gensim",
        "description": "gensim",
        "detail": "gensim",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "Word2Vec",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "PdfReader",
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "isExtraImport": true,
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "FAISS",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "CharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain.llms",
        "description": "langchain.llms",
        "isExtraImport": true,
        "detail": "langchain.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceInstructEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain.embeddings",
        "description": "langchain.embeddings",
        "isExtraImport": true,
        "detail": "langchain.embeddings",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "ConversationalRetrievalChain",
        "importPath": "langchain.chains",
        "description": "langchain.chains",
        "isExtraImport": true,
        "detail": "langchain.chains",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "HuggingFaceHub",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "importPath": "ecommbot.ingest",
        "description": "ecommbot.ingest",
        "isExtraImport": true,
        "detail": "ecommbot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "importPath": "ecommbot.retrieval_generation",
        "description": "ecommbot.retrieval_generation",
        "isExtraImport": true,
        "detail": "ecommbot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "importPath": "ecommbot.converteur",
        "description": "ecommbot.converteur",
        "isExtraImport": true,
        "detail": "ecommbot.converteur",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "pyspark.ml.clustering",
        "description": "pyspark.ml.clustering",
        "isExtraImport": true,
        "detail": "pyspark.ml.clustering",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "importPath": "reader_data",
        "description": "reader_data",
        "isExtraImport": true,
        "detail": "reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "importPath": "reader_data",
        "description": "reader_data",
        "isExtraImport": true,
        "detail": "reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "importPath": "transformer",
        "description": "transformer",
        "isExtraImport": true,
        "detail": "transformer",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "StringIndexer",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "VectorAssembler",
        "importPath": "pyspark.ml.feature",
        "description": "pyspark.ml.feature",
        "isExtraImport": true,
        "detail": "pyspark.ml.feature",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "pyspark.ml",
        "description": "pyspark.ml",
        "isExtraImport": true,
        "detail": "pyspark.ml",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "pyspark.ml.classification",
        "description": "pyspark.ml.classification",
        "isExtraImport": true,
        "detail": "pyspark.ml.classification",
        "documentation": {}
    },
    {
        "label": "snowflake.connector",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "snowflake.connector",
        "description": "snowflake.connector",
        "detail": "snowflake.connector",
        "documentation": {}
    },
    {
        "label": "pinecone",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pinecone",
        "description": "pinecone",
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "Pinecone",
        "importPath": "pinecone",
        "description": "pinecone",
        "isExtraImport": true,
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "ServerlessSpec",
        "importPath": "pinecone",
        "description": "pinecone",
        "isExtraImport": true,
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "PineconeApiException",
        "importPath": "pinecone",
        "description": "pinecone",
        "isExtraImport": true,
        "detail": "pinecone",
        "documentation": {}
    },
    {
        "label": "SparkContext",
        "importPath": "pyspark",
        "description": "pyspark",
        "isExtraImport": true,
        "detail": "pyspark",
        "documentation": {}
    },
    {
        "label": "SparkConf",
        "importPath": "pyspark",
        "description": "pyspark",
        "isExtraImport": true,
        "detail": "pyspark",
        "documentation": {}
    },
    {
        "label": "SparkConf",
        "importPath": "pyspark",
        "description": "pyspark",
        "isExtraImport": true,
        "detail": "pyspark",
        "documentation": {}
    },
    {
        "label": "SparkContext",
        "importPath": "pyspark",
        "description": "pyspark",
        "isExtraImport": true,
        "detail": "pyspark",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.config",
        "description": "CV_Processus.consumerSpark.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.consumerSpark.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']\n# Fonction d'extraction du texte depuis les bytes\ndef extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "extract_text_from_bytes",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def extract_text_from_bytes(pdf_bytes):\n    try:\n        memory_buffer = BytesIO(pdf_bytes)\n        with fitz.open(stream=memory_buffer, filetype=\"pdf\") as doc:\n            text = \"\"\n            for page_num in range(doc.page_count):\n                page = doc.load_page(page_num)\n                text += page.get_text(\"text\")\n            return text\n    except Exception as e:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def ats_extractor(resume_data, api_key):\n    prompt = '''\n    You are an AI bot designed to act as a professional for parsing resumes. You are given a resume, and your job is to extract the following information:\n    1. first name\n    2. last name\n    1. full name\n    4. title\n    5. address\n    6. objective\n    7. date_of_birth",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "process_pdf_udf",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def process_pdf_udf(value):\n    try:\n        # Les donnes arrivent dj en bytes depuis Kafka\n        pdf_bytes = value\n        # Extraction du texte directement depuis les bytes\n        extracted_text = extract_text_from_bytes(pdf_bytes)\n        # Analyse avec Groq\n        analysis_result = ats_extractor(extracted_text, api_key)\n        return (extracted_text, json.dumps(analysis_result))\n    except Exception as e:",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "process_batch",
        "kind": 2,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "def process_batch(df, epoch_id):\n    # Conversion du DataFrame en liste de dictionnaires pour traitement\n    rows = df.collect()\n    for row in rows:\n        # Sauvegarde dans Delta\n        delta_df = spark.createDataFrame([(row['offset'], row['extracted_text'], row['analysis_result'])], schema=output_schema)\n        delta_table = DeltaTable.forPath(spark, OUTPUT_PATH)\n        # Merge or Insert new data into Delta table\n        delta_table.alias(\"t\").merge(\n            delta_df.alias(\"s\"),",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = os.getenv(\"ADLS_STORAGE_ACCOUNT_NAME\")\nADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\napi_key=os.getenv(\"api_key\")\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "ADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\napi_key=os.getenv(\"api_key\")\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "ADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\napi_key=os.getenv(\"api_key\")\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "ADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\napi_key=os.getenv(\"api_key\")\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "ADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",\n]\n# Initialisation de Spark\napp_name = \"MonApplicationSpark\"\npackages = [\"org.apache.spark:spark-sql_2.12:3.1.1\", \"io.delta:delta-core_2.12:1.0.0\"]\ncluster_manager = \"local[*]\"",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "app_name",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "app_name = \"MonApplicationSpark\"\npackages = [\"org.apache.spark:spark-sql_2.12:3.1.1\", \"io.delta:delta-core_2.12:1.0.0\"]\ncluster_manager = \"local[*]\"\n# Crer ou rcuprer la session Spark\njars = \",\".join(packages)\nspark = (\n    SparkSession.builder.appName(app_name)\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n    .config(\"spark.jars.packages\", jars)\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "packages",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "packages = [\"org.apache.spark:spark-sql_2.12:3.1.1\", \"io.delta:delta-core_2.12:1.0.0\"]\ncluster_manager = \"local[*]\"\n# Crer ou rcuprer la session Spark\njars = \",\".join(packages)\nspark = (\n    SparkSession.builder.appName(app_name)\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n    .config(\"spark.jars.packages\", jars)\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "cluster_manager",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "cluster_manager = \"local[*]\"\n# Crer ou rcuprer la session Spark\njars = \",\".join(packages)\nspark = (\n    SparkSession.builder.appName(app_name)\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n    .config(\"spark.jars.packages\", jars)\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "jars",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "jars = \",\".join(packages)\nspark = (\n    SparkSession.builder.appName(app_name)\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n    .config(\"spark.jars.packages\", jars)\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "spark = (\n    SparkSession.builder.appName(app_name)\n    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n    .config(\"spark.jars.packages\", jars)\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n    .config(",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "output_schema",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "output_schema = StructType([\n    StructField(\"pdf_text\", StringType(), True),\n    StructField(\"analysis_result\", StringType(), True)\n])\n# Chargement de la configuration\ndef load_config():\n    CONFIG_PATH = \"config.yaml\"\n    with open(CONFIG_PATH) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        return data['GROQ_API_KEY']",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "df = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"192.168.1.21:29093\") \\\n    .option(\"subscribe\", \"TopicCV\") \\\n    .load()\n# Application du traitement\nprocessed_df = df.select(\n    col(\"offset\"),\n    process_pdf_udf(col(\"value\")).alias(\"processed_data\")\n)\n# Extraction des colonnes du struct retourn par l'UDF",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "processed_df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "processed_df = df.select(\n    col(\"offset\"),\n    process_pdf_udf(col(\"value\")).alias(\"processed_data\")\n)\n# Extraction des colonnes du struct retourn par l'UDF\nfinal_df = processed_df.select(\n    col(\"offset\"),\n    col(\"processed_data.pdf_text\").alias(\"extracted_text\"),\n    col(\"processed_data.analysis_result\").alias(\"analysis_result\")\n)",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "final_df",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "final_df = processed_df.select(\n    col(\"offset\"),\n    col(\"processed_data.pdf_text\").alias(\"extracted_text\"),\n    col(\"processed_data.analysis_result\").alias(\"analysis_result\")\n)\n# Configuration des streams de sortie\njson_query = final_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attente de la terminaison du stream",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "json_query",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.consumerspark",
        "description": "CV_Processus.consumerSpark.consumerspark",
        "peekOfCode": "json_query = final_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .start()\n# Attente de la terminaison du stream\nspark.streams.awaitAnyTermination()",
        "detail": "CV_Processus.consumerSpark.consumerspark",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "CV_Processus.consumerSpark.schema",
        "description": "CV_Processus.consumerSpark.schema",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),\n        StructField(\"postal_code\", StringType(), True)",
        "detail": "CV_Processus.consumerSpark.schema",
        "documentation": {}
    },
    {
        "label": "topic",
        "kind": 5,
        "importPath": "CV_Processus.producer.config",
        "description": "CV_Processus.producer.config",
        "peekOfCode": "topic = \"TopicCV\"",
        "detail": "CV_Processus.producer.config",
        "documentation": {}
    },
    {
        "label": "producer",
        "kind": 5,
        "importPath": "CV_Processus.producer.prod",
        "description": "CV_Processus.producer.prod",
        "peekOfCode": "producer = KafkaProducer(bootstrap_servers='localhost:29092')\n# 2. Lire le fichier PDF en mode binaire\nwith open('document.pdf', 'rb') as fichier_pdf:\n    contenu_pdf = fichier_pdf.read()\n# 3. Envoyer les donnes en bytes dans Kafka\nproducer.send(config.topic, contenu_pdf)\nproducer.flush()\nproducer.close()",
        "detail": "CV_Processus.producer.prod",
        "documentation": {}
    },
    {
        "label": "RegistrationForm",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.forms",
        "description": "Flask-Job-Portal.app.forms",
        "peekOfCode": "class RegistrationForm(FlaskForm):\n    usertype = SelectField('Select Usertype',\n                           choices=[('Job Seeker', 'Job Seeker'),\n                                    ('Company', 'Company')],\n                           validators=[DataRequired()])\n    username = StringField('Username',\n                           validators=[DataRequired(), Length(min=2, max=20)])\n    email = StringField('Email',\n                        validators=[DataRequired(), Email()])\n    password = PasswordField('Password', validators=[DataRequired()])",
        "detail": "Flask-Job-Portal.app.forms",
        "documentation": {}
    },
    {
        "label": "LoginForm",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.forms",
        "description": "Flask-Job-Portal.app.forms",
        "peekOfCode": "class LoginForm(FlaskForm):\n    usertype = SelectField('Select Usertype',\n                           choices=[('Job Seeker', 'Job Seeker'),\n                                    ('Company', 'Company')],\n                           validators=[DataRequired()])\n    email = StringField('Email',\n                        validators=[DataRequired(), Email()])\n    password = PasswordField('Password', validators=[DataRequired()])\n    remember = BooleanField('Remember Me')\n    submit = SubmitField('Login')",
        "detail": "Flask-Job-Portal.app.forms",
        "documentation": {}
    },
    {
        "label": "ReviewForm",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.forms",
        "description": "Flask-Job-Portal.app.forms",
        "peekOfCode": "class ReviewForm(FlaskForm):\n    username = StringField('Name',\n                           validators=[DataRequired()])\n    review = TextAreaField('Review',\n                           validators=[DataRequired()])\n    submit = SubmitField('Submit Review')\nclass JobForm(FlaskForm):\n    title = StringField('Job Title',\n                        validators=[DataRequired(), Length(min=2, max=20)])\n    industry = SelectField('Industry', choices=[('Construction', 'Construction'),",
        "detail": "Flask-Job-Portal.app.forms",
        "documentation": {}
    },
    {
        "label": "JobForm",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.forms",
        "description": "Flask-Job-Portal.app.forms",
        "peekOfCode": "class JobForm(FlaskForm):\n    title = StringField('Job Title',\n                        validators=[DataRequired(), Length(min=2, max=20)])\n    industry = SelectField('Industry', choices=[('Construction', 'Construction'),\n                                                ('Education', 'Education'),\n                                                ('Food And Beverage', 'Food and Beverage'),\n                                                ('Pharmaceutical', 'Pharmaceutical'),\n                                                ('Entertainment', 'Entertainment'),\n                                                ('Manufacturing', 'Manufacturing'),\n                                                ('Telecommunication', 'Telecommunication'),",
        "detail": "Flask-Job-Portal.app.forms",
        "documentation": {}
    },
    {
        "label": "ApplicationForm",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.forms",
        "description": "Flask-Job-Portal.app.forms",
        "peekOfCode": "class ApplicationForm(FlaskForm):\n    gender = SelectField('Gender', choices=[('Male', 'Male'),\n                                            ('Female', 'Female'),\n                                            ('Others', 'Other')],\n                         default='male',\n                         validators=[DataRequired()])\n    degree = SelectField('Degree',\n                         default='eSchool',\n                         choices=[('eSchool', 'School'),\n                                  ('dHighSchool', 'HighSchool'),",
        "detail": "Flask-Job-Portal.app.forms",
        "documentation": {}
    },
    {
        "label": "User",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.models",
        "description": "Flask-Job-Portal.app.models",
        "peekOfCode": "class User(db.Model, UserMixin):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(20), unique=True, nullable=False)\n    usertype = db.Column(db.String(20), nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    password = db.Column(db.String(60), nullable=False)\n    jobs = db.relationship('Jobs', backref='job_applier', lazy=True)\n    applications = db.relationship('Application', backref='application_submiter', lazy=True)\n    def __repr__(self):\n        return f\"User('{self.id}', '{self.username}', '{self.usertype}', '{self.email}')\"",
        "detail": "Flask-Job-Portal.app.models",
        "documentation": {}
    },
    {
        "label": "Application",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.models",
        "description": "Flask-Job-Portal.app.models",
        "peekOfCode": "class Application(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    gender = db.Column(db.String(20), nullable=False)\n    date_posted = db.Column(db.DateTime, nullable=False, default=date.today())\n    degree = db.Column(db.String(20), nullable=False)\n    industry = db.Column(db.String(50), nullable=False)\n    experience = db.Column(db.Integer, nullable=False)\n    cv = db.Column(db.String(20), nullable=False)\n    cover_letter = db.Column(db.Text, nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)",
        "detail": "Flask-Job-Portal.app.models",
        "documentation": {}
    },
    {
        "label": "Jobs",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.models",
        "description": "Flask-Job-Portal.app.models",
        "peekOfCode": "class Jobs(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(100), nullable=False)\n    industry = db.Column(db.String(50), nullable=False)\n    description = db.Column(db.Text, nullable=False)\n    date_posted = db.Column(db.DateTime, nullable=False, default=date.today())\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n    applications = db.relationship('Application', backref='application_jober', lazy=True)\n    def __repr__(self):\n        return f\"Jobs('{self.id}','{self.title}', '{self.industry}', '{self.date_posted}')\"",
        "detail": "Flask-Job-Portal.app.models",
        "documentation": {}
    },
    {
        "label": "Review",
        "kind": 6,
        "importPath": "Flask-Job-Portal.app.models",
        "description": "Flask-Job-Portal.app.models",
        "peekOfCode": "class Review(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(20), unique=True, nullable=False)\n    review = db.Column(db.Text, nullable=False)\n    def __repr__(self):\n        return f\"Review('{self.username}', '{self.review}')\"",
        "detail": "Flask-Job-Portal.app.models",
        "documentation": {}
    },
    {
        "label": "load_user",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.models",
        "description": "Flask-Job-Portal.app.models",
        "peekOfCode": "def load_user(user_id):\n    return User.query.get(int(user_id))\nclass User(db.Model, UserMixin):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(20), unique=True, nullable=False)\n    usertype = db.Column(db.String(20), nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    password = db.Column(db.String(60), nullable=False)\n    jobs = db.relationship('Jobs', backref='job_applier', lazy=True)\n    applications = db.relationship('Application', backref='application_submiter', lazy=True)",
        "detail": "Flask-Job-Portal.app.models",
        "documentation": {}
    },
    {
        "label": "register",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def register():\n    if current_user.is_authenticated:\n        if current_user.usertype == 'Job Seeker':\n            return redirect(url_for('show_jobs'))\n        elif current_user.usertype == 'Company':\n            return redirect(url_for('posted_jobs'))\n    form = RegistrationForm()\n    if form.validate_on_submit():\n        hashed_password = bcrypt.generate_password_hash(form.password.data).decode('utf-8')\n        user = User(username=form.username.data, usertype=form.usertype.data, email=form.email.data, password=hashed_password)",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "login",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def login():\n    if current_user.is_authenticated:\n        if current_user.usertype == 'Job Seeker':\n            return redirect(url_for('show_jobs'))\n        elif current_user.usertype == 'Company':\n            return redirect(url_for('posted_jobs'))\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User.query.filter_by(email=form.email.data).first()\n        if user and bcrypt.check_password_hash(user.password, form.password.data):",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "logout",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def logout():\n    logout_user()\n    return redirect(url_for('show_jobs'))\ndef save_picture(form_picture):\n    f_name, f_ext = os.path.splitext(form_picture.filename)\n    picture_fn = f_name + f_ext\n    picture_path = os.path.join(app.root_path, 'static', picture_fn)\n    form_picture.save(picture_path)\n    return picture_fn\n@app.route(\"/post_cvs/<jobid>\", methods=['GET', 'POST'])",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "save_picture",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def save_picture(form_picture):\n    f_name, f_ext = os.path.splitext(form_picture.filename)\n    picture_fn = f_name + f_ext\n    picture_path = os.path.join(app.root_path, 'static', picture_fn)\n    form_picture.save(picture_path)\n    return picture_fn\n@app.route(\"/post_cvs/<jobid>\", methods=['GET', 'POST'])\n@login_required\ndef post_cvs(jobid):\n    form = ApplicationForm()",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "post_cvs",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def post_cvs(jobid):\n    form = ApplicationForm()\n    job = Jobs.query.filter_by(id=jobid).first()\n    if form.validate_on_submit():\n        application = Application(gender=form.gender.data,\n                              degree=form.degree.data,\n                              industry=form.industry.data,\n                              experience=form.experience.data,\n                              cover_letter=form.cover_letter.data,\n                              application_submiter=current_user,",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "post_jobs",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def post_jobs():\n    form = JobForm()\n    if form.validate_on_submit():\n        job = Jobs(title=form.title.data,\n                   industry=form.industry.data,\n                   description=form.description.data,\n                   job_applier=current_user)\n        db.session.add(job)\n        db.session.commit()\n        return redirect(url_for('posted_jobs'))",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "review",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def review():\n    form = ReviewForm()\n    if form.validate_on_submit():\n        review = Review(username=form.username.data,\n                            review=form.review.data)\n        db.session.add(review)\n        db.session.commit()\n        flash('Thank you for providing the review!', 'success')\n        return redirect(url_for('show_jobs'))\n    return  render_template('review.html', form=form, Random_Review=Random_Review)",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "posted_jobs",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def posted_jobs():\n    jobs = Jobs.query.filter_by(job_applier=current_user)\n    return render_template('show_jobs.html', jobs=jobs, Random_Review=Random_Review)\n@app.route(\"/show_applications/<jobid>\", methods=['GET'])\n@login_required\ndef show_applications(jobid):\n    applications = Application.query.filter_by(job_id=jobid).order_by(Application.degree, Application.experience.desc()).all()\n    return render_template('show_applications.html', applications=applications, Random_Review=Random_Review)\n@app.route(\"/meeting/<application_id>\")\n@login_required",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "show_applications",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def show_applications(jobid):\n    applications = Application.query.filter_by(job_id=jobid).order_by(Application.degree, Application.experience.desc()).all()\n    return render_template('show_applications.html', applications=applications, Random_Review=Random_Review)\n@app.route(\"/meeting/<application_id>\")\n@login_required\ndef meeting(application_id):\n    applicant_id = Application.query.get(int(application_id)).user_id\n    applicant = User.query.get(applicant_id)\n    return render_template('meeting.html', applicant=applicant, Random_Review=Random_Review)\n@app.route(\"/\")",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "meeting",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def meeting(application_id):\n    applicant_id = Application.query.get(int(application_id)).user_id\n    applicant = User.query.get(applicant_id)\n    return render_template('meeting.html', applicant=applicant, Random_Review=Random_Review)\n@app.route(\"/\")\n@app.route(\"/show_jobs\")\ndef show_jobs():\n    jobs = Jobs.query.all()\n    return render_template('show_jobs.html', jobs=jobs, Random_Review=Random_Review)\n@app.route(\"/resume/<id>\", methods=['GET'])",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "show_jobs",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def show_jobs():\n    jobs = Jobs.query.all()\n    return render_template('show_jobs.html', jobs=jobs, Random_Review=Random_Review)\n@app.route(\"/resume/<id>\", methods=['GET'])\ndef resume(id):\n    cv = Application.query.get(int(id)).cv\n    return render_template('resume.html', cv=cv, Random_Review=Random_Review, id=id)",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "resume",
        "kind": 2,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "def resume(id):\n    cv = Application.query.get(int(id)).cv\n    return render_template('resume.html', cv=cv, Random_Review=Random_Review, id=id)",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "rev",
        "kind": 5,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "rev = [\n    {\n        'username': 'Jeff Bezos',\n        'review': 'I hired multiple people using this website. Thank you'\n    },\n    {\n        'username': 'Tim David',\n        'review': 'The website helped me to get placed! Seemless experience'\n    },\n    {",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "Review_Obj",
        "kind": 5,
        "importPath": "Flask-Job-Portal.app.routes",
        "description": "Flask-Job-Portal.app.routes",
        "peekOfCode": "Review_Obj = Review.query.all()\nif len(Review_Obj) < 3:\n    Random_Review = rev\nelse:\n    Random_Review = random.sample(Review_Obj, 3)\n@app.route(\"/register\", methods=['GET', 'POST'])\ndef register():\n    if current_user.is_authenticated:\n        if current_user.usertype == 'Job Seeker':\n            return redirect(url_for('show_jobs'))",
        "detail": "Flask-Job-Portal.app.routes",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.contat3",
        "description": "notoyage_data.contat3",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_type_trav.xlsx')\n# Charger le fichier contenant les types de contrat avec leurs identifiants\ncontrat_df = pd.read_excel('type_contrat_avec_id.xlsx')\n# Effectuer la jointure entre les deux DataFrames sur la colonne 'contrat'\ndata_with_id = data.merge(contrat_df, how='left', left_on='contrat', right_on='nom_type_contrat')\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata_with_id.to_excel('fichier_reduit_avec_id_type_contrat.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data_with_id.head())",
        "detail": "notoyage_data.contat3",
        "documentation": {}
    },
    {
        "label": "contrat_df",
        "kind": 5,
        "importPath": "notoyage_data.contat3",
        "description": "notoyage_data.contat3",
        "peekOfCode": "contrat_df = pd.read_excel('type_contrat_avec_id.xlsx')\n# Effectuer la jointure entre les deux DataFrames sur la colonne 'contrat'\ndata_with_id = data.merge(contrat_df, how='left', left_on='contrat', right_on='nom_type_contrat')\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata_with_id.to_excel('fichier_reduit_avec_id_type_contrat.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data_with_id.head())",
        "detail": "notoyage_data.contat3",
        "documentation": {}
    },
    {
        "label": "data_with_id",
        "kind": 5,
        "importPath": "notoyage_data.contat3",
        "description": "notoyage_data.contat3",
        "peekOfCode": "data_with_id = data.merge(contrat_df, how='left', left_on='contrat', right_on='nom_type_contrat')\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata_with_id.to_excel('fichier_reduit_avec_id_type_contrat.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data_with_id.head())",
        "detail": "notoyage_data.contat3",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.contrat2",
        "description": "notoyage_data.contrat2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_type_trav.xlsx')\n# Extraire les types de contrat uniques\nunique_contrats = data['contrat'].dropna().unique()\n# Crer un DataFrame avec l'id et le nom des types de contrat\ncontrat_df = pd.DataFrame({\n    'id_type_contrat': range(1, len(unique_contrats) + 1),\n    'nom_type_contrat': unique_contrats\n})\n# Sauvegarder le DataFrame dans un fichier Excel\ncontrat_df.to_excel('type_contrat_avec_id.xlsx', index=False)",
        "detail": "notoyage_data.contrat2",
        "documentation": {}
    },
    {
        "label": "unique_contrats",
        "kind": 5,
        "importPath": "notoyage_data.contrat2",
        "description": "notoyage_data.contrat2",
        "peekOfCode": "unique_contrats = data['contrat'].dropna().unique()\n# Crer un DataFrame avec l'id et le nom des types de contrat\ncontrat_df = pd.DataFrame({\n    'id_type_contrat': range(1, len(unique_contrats) + 1),\n    'nom_type_contrat': unique_contrats\n})\n# Sauvegarder le DataFrame dans un fichier Excel\ncontrat_df.to_excel('type_contrat_avec_id.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(contrat_df.head())",
        "detail": "notoyage_data.contrat2",
        "documentation": {}
    },
    {
        "label": "contrat_df",
        "kind": 5,
        "importPath": "notoyage_data.contrat2",
        "description": "notoyage_data.contrat2",
        "peekOfCode": "contrat_df = pd.DataFrame({\n    'id_type_contrat': range(1, len(unique_contrats) + 1),\n    'nom_type_contrat': unique_contrats\n})\n# Sauvegarder le DataFrame dans un fichier Excel\ncontrat_df.to_excel('type_contrat_avec_id.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(contrat_df.head())",
        "detail": "notoyage_data.contrat2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.entreprise2",
        "description": "notoyage_data.entreprise2",
        "peekOfCode": "data = pd.read_csv('salaries_max_convertis.csv')  # Le fichier principal\ncompanies = pd.read_csv('entreprises_avec_id.csv')  # Le fichier avec id_company\n# Fusionner les deux fichiers pour ajouter la colonne id_company\n# Fusionner sur la colonne \"Company\" (nom_company dans le fichier entreprises)\ndata_with_company_id = pd.merge(data, companies[['id_company', 'nom_company']],\n                                left_on='Company', right_on='nom_company',\n                                how='left')\n# Supprimer la colonne \"nom_company\" ajoute par la fusion pour viter la redondance\ndata_with_company_id.drop(columns=['nom_company'], inplace=True)\n# Sauvegarder le rsultat dans un nouveau fichier Excel",
        "detail": "notoyage_data.entreprise2",
        "documentation": {}
    },
    {
        "label": "companies",
        "kind": 5,
        "importPath": "notoyage_data.entreprise2",
        "description": "notoyage_data.entreprise2",
        "peekOfCode": "companies = pd.read_csv('entreprises_avec_id.csv')  # Le fichier avec id_company\n# Fusionner les deux fichiers pour ajouter la colonne id_company\n# Fusionner sur la colonne \"Company\" (nom_company dans le fichier entreprises)\ndata_with_company_id = pd.merge(data, companies[['id_company', 'nom_company']],\n                                left_on='Company', right_on='nom_company',\n                                how='left')\n# Supprimer la colonne \"nom_company\" ajoute par la fusion pour viter la redondance\ndata_with_company_id.drop(columns=['nom_company'], inplace=True)\n# Sauvegarder le rsultat dans un nouveau fichier Excel\ndata_with_company_id.to_csv('fichier_valide.csv', index=False)",
        "detail": "notoyage_data.entreprise2",
        "documentation": {}
    },
    {
        "label": "data_with_company_id",
        "kind": 5,
        "importPath": "notoyage_data.entreprise2",
        "description": "notoyage_data.entreprise2",
        "peekOfCode": "data_with_company_id = pd.merge(data, companies[['id_company', 'nom_company']],\n                                left_on='Company', right_on='nom_company',\n                                how='left')\n# Supprimer la colonne \"nom_company\" ajoute par la fusion pour viter la redondance\ndata_with_company_id.drop(columns=['nom_company'], inplace=True)\n# Sauvegarder le rsultat dans un nouveau fichier Excel\ndata_with_company_id.to_csv('fichier_valide.csv', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data_with_company_id.head())",
        "detail": "notoyage_data.entreprise2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_contrat.xlsx')\n# Charger le fichier contenant la correspondance des pays avec les ID\nunique_countries = pd.read_excel('unique_countries.xlsx')\n# Fusionner les deux fichiers pour ajouter la colonne id_etat  data\ndata_with_ids = data.merge(unique_countries, how='left', left_on='Country', right_on='nom_etat')\n# Supprimer la colonne \"nom_etat\" si elle est inutile\ndata_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoute\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperu des rsultats",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "unique_countries",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "unique_countries = pd.read_excel('unique_countries.xlsx')\n# Fusionner les deux fichiers pour ajouter la colonne id_etat  data\ndata_with_ids = data.merge(unique_countries, how='left', left_on='Country', right_on='nom_etat')\n# Supprimer la colonne \"nom_etat\" si elle est inutile\ndata_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoute\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperu des rsultats\nprint(data_with_ids.head())",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "data_with_ids",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "data_with_ids = data.merge(unique_countries, how='left', left_on='Country', right_on='nom_etat')\n# Supprimer la colonne \"nom_etat\" si elle est inutile\ndata_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoute\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperu des rsultats\nprint(data_with_ids.head())",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "data_with_ids",
        "kind": 5,
        "importPath": "notoyage_data.etat2",
        "description": "notoyage_data.etat2",
        "peekOfCode": "data_with_ids = data_with_ids.drop(columns=['nom_etat'])\n# Sauvegarder le nouveau fichier avec la colonne id_etat ajoute\ndata_with_ids.to_excel('fichier_reduit_avec_id_etat.xlsx', index=False)\n# Afficher un aperu des rsultats\nprint(data_with_ids.head())",
        "detail": "notoyage_data.etat2",
        "documentation": {}
    },
    {
        "label": "get_training_for_sector",
        "kind": 2,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "def get_training_for_sector(sector_name):\n    prompt = f\"Associer le secteur '{sector_name}' avec une formation parmi les suivantes : {', '.join(formations.values())}.\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"Vous tes un assistant pour associer des secteurs  des formations.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Gnrer la rponse\n    response = groq_client.chat.completions.create(\n        model=\"llama3-8b-8192\",\n        messages=messages,",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "data_corrected",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "data_corrected = {\n    \"ID\": range(1, 46),  # Adjusting the range to match the length of the training list (1 to 45 inclusive)\n    \"Training\": [\n        \"Agronomy\",\n        \"Electrical Engineering\",\n        \"Architecture\",\n        \"Medicine\",\n        \"Computer Science\",\n        \"Finance\",\n        \"Management\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_corrected",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_corrected = pd.DataFrame(data_corrected)\n# Save to an Excel file\nfile_path_corrected = \"Training_List_Corrected.xlsx\"\ndf_corrected.to_excel(file_path_corrected, index=False)\nprint(f\"Excel file '{file_path_corrected}' has been created successfully.\")\nimport pandas as pd\n# Liste des secteurs\nsecteurs = {\n    \"ID_Secteur\": list(range(1, 205)),  # IDs des secteurs de 1  204\n    \"Nom_Secteur\": [",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "file_path_corrected",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "file_path_corrected = \"Training_List_Corrected.xlsx\"\ndf_corrected.to_excel(file_path_corrected, index=False)\nprint(f\"Excel file '{file_path_corrected}' has been created successfully.\")\nimport pandas as pd\n# Liste des secteurs\nsecteurs = {\n    \"ID_Secteur\": list(range(1, 205)),  # IDs des secteurs de 1  204\n    \"Nom_Secteur\": [\n        \"Diversified\", \"Financial Services\", \"Insurance\", \"Energy\", \"Infrastructure\", \"Logistics\",\n        \"Transportation\", \"Media & Entertainment\", \"Food and Beverage\", \"Telecommunications\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "secteurs",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "secteurs = {\n    \"ID_Secteur\": list(range(1, 205)),  # IDs des secteurs de 1  204\n    \"Nom_Secteur\": [\n        \"Diversified\", \"Financial Services\", \"Insurance\", \"Energy\", \"Infrastructure\", \"Logistics\",\n        \"Transportation\", \"Media & Entertainment\", \"Food and Beverage\", \"Telecommunications\",\n        \"Manufacturing\", \"Pharmaceuticals\", \"Industrial\", \"Conglomerate\", \"Financials\",\n        \"Travel and Leisure\", \"Elevators & Escalators\", \"Energy/Oil and Gas\",\n        \"Information Technology\", \"Utilities\", \"Engineering\", \"Airlines\", \"Healthcare\",\n        \"Real Estate\", \"Hospitality/Hotels\", \"Technology\", \"IT Services\", \"Consumer Goods\",\n        \"Technology/Music Streaming\", \"Automotive\", \"Media\", \"Beauty/Cosmetics\", \"Chemicals\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "formations = {\n    \"ID_Formation\": range(1, 46),  # IDs des formations de 1  45\n    \"Nom_Formation\": [\n        \"Agronomy\", \"Electrical Engineering\", \"Architecture\", \"Medicine\", \"Computer Science\",\n        \"Finance\", \"Management\", \"Law\", \"Marketing\", \"Civil Engineering\", \"Chemistry\",\n        \"Aeronautics\", \"Logistics\", \"Urban Planning\", \"Mechanical Engineering\", \"Pharmacy\",\n        \"Biotechnologies\", \"Renewable Energies\", \"Audiovisual\", \"Tourism\", \"Hospitality\",\n        \"Design\", \"Multimedia\", \"Data Science\", \"Artificial Intelligence\", \"Actuarial Science\",\n        \"Petroleum Engineering\", \"Production and Automation\", \"Electronics\", \"Communication\",\n        \"Environmental Sciences\", \"Cosmetics\", \"Agri-food\", \"Networks and Systems\",",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "secteurs_formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "secteurs_formations = {\n    \"ID_Secteur\": secteurs[\"ID_Secteur\"],\n    \"Nom_Secteur\": secteurs[\"Nom_Secteur\"],\n    \"ID_Formation\": [i % 45 + 1 for i in secteurs[\"ID_Secteur\"]],  # Associer un ID_Formation de faon cyclique\n    \"Nom_Formation\": [formations[\"Nom_Formation\"][i % 45] for i in secteurs[\"ID_Secteur\"]]\n}\n# Convertir en DataFrame\ndf_secteurs_formations = pd.DataFrame(secteurs_formations)\n# Sauvegarder dans un fichier Excel\ndf_secteurs_formations.to_excel(\"Secteurs_Formation.xlsx\", index=False)",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_secteurs_formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_secteurs_formations = pd.DataFrame(secteurs_formations)\n# Sauvegarder dans un fichier Excel\ndf_secteurs_formations.to_excel(\"Secteurs_Formation.xlsx\", index=False)\nprint(\"Le fichier 'Secteurs_Formation.xlsx' a t cr avec succs.\")\nimport pandas as pd\n# Liste des formations avec une association logique\nformations = [\n    (1, \"Agronomy\"),\n    (2, \"Finance\"),\n    (3, \"Insurance\"),",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "formations = [\n    (1, \"Agronomy\"),\n    (2, \"Finance\"),\n    (3, \"Insurance\"),\n    (4, \"Energy Engineering\"),\n    (5, \"Civil Engineering\"),\n    (6, \"Logistics\"),\n    (7, \"Transportation\"),\n    (8, \"Audiovisual\"),\n    (9, \"Agri-food\"),",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "secteurs",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "secteurs = [\n    (1, \"Diversified\"),\n    (2, \"Financial Services\"),\n    (3, \"Insurance\"),\n    (4, \"Energy\"),\n    (5, \"Infrastructure\"),\n    (6, \"Logistics\"),\n    (7, \"Transportation\"),\n    (8, \"Media & Entertainment\"),\n    (9, \"Food and Beverage\"),",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "correspondance",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "correspondance = [\n    {\n        \"ID Secteur\": secteur[0],\n        \"Nom Secteur\": secteur[1],\n        \"Formation Associe\": next(f[1] for f in formations if f[0] == secteur[0])\n    }\n    for secteur in secteurs\n]\n# Cration du DataFrame avec les donnes\ndf = pd.DataFrame(correspondance)",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df = pd.DataFrame(correspondance)\n# Sauvegarde dans un fichier Excel\ndf.to_excel(\"secteurs_formations_association.xlsx\", index=False)\nprint(\"Le fichier Excel a t cr avec succs.\")\npip install groq\nimport pandas as pd\nimport yaml\nfrom groq import Groq # Remplacez cela avec la bibliothque approprie pour Llama\n# Charger les donnes des secteurs  partir du CSV\ndf_sectors = pd.read_csv('/content/secteurs_avec_id.csv')  # Remplacez par le chemin de votre CSV",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_sectors",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_sectors = pd.read_csv('/content/secteurs_avec_id.csv')  # Remplacez par le chemin de votre CSV\n# Liste des formations (ID, nom)\nformations = {\n    1: 'Agronomy',\n    2: 'Electrical Engineering',\n    3: 'Architecture',\n    4: 'Medicine',\n    5: 'Computer Science',\n    6: 'Finance',\n    7: 'Management',",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "formations",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "formations = {\n    1: 'Agronomy',\n    2: 'Electrical Engineering',\n    3: 'Architecture',\n    4: 'Medicine',\n    5: 'Computer Science',\n    6: 'Finance',\n    7: 'Management',\n    8: 'Law',\n    9: 'Marketing',",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "CONFIG_PATH",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "CONFIG_PATH = r\"/content/config.yaml\"\napi_key = None\n# Charger la cl API depuis le fichier YAML\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialiser le client Groq pour Llama\ngroq_client = Groq(api_key=api_key)\n# Fonction pour interroger Llama et obtenir une formation\ndef get_training_for_sector(sector_name):",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "api_key = None\n# Charger la cl API depuis le fichier YAML\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialiser le client Groq pour Llama\ngroq_client = Groq(api_key=api_key)\n# Fonction pour interroger Llama et obtenir une formation\ndef get_training_for_sector(sector_name):\n    prompt = f\"Associer le secteur '{sector_name}' avec une formation parmi les suivantes : {', '.join(formations.values())}.\"",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "groq_client",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "groq_client = Groq(api_key=api_key)\n# Fonction pour interroger Llama et obtenir une formation\ndef get_training_for_sector(sector_name):\n    prompt = f\"Associer le secteur '{sector_name}' avec une formation parmi les suivantes : {', '.join(formations.values())}.\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"Vous tes un assistant pour associer des secteurs  des formations.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Gnrer la rponse\n    response = groq_client.chat.completions.create(",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_sectors['id_formation']",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_sectors['id_formation'] = None\ndf_sectors['nom_formation'] = None\n# Associer chaque secteur  une formation\nfor idx, row in df_sectors.iterrows():\n    sector_name = row['nom_secteur']\n    id_formation, formation_name = get_training_for_sector(sector_name)\n    # Mettre  jour les colonnes du dataframe\n    df_sectors.at[idx, 'id_formation'] = id_formation\n    df_sectors.at[idx, 'nom_formation'] = formation_name\n# Sauvegarder le rsultat dans un fichier Excel",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "df_sectors['nom_formation']",
        "kind": 5,
        "importPath": "notoyage_data.llama_formation_sector",
        "description": "notoyage_data.llama_formation_sector",
        "peekOfCode": "df_sectors['nom_formation'] = None\n# Associer chaque secteur  une formation\nfor idx, row in df_sectors.iterrows():\n    sector_name = row['nom_secteur']\n    id_formation, formation_name = get_training_for_sector(sector_name)\n    # Mettre  jour les colonnes du dataframe\n    df_sectors.at[idx, 'id_formation'] = id_formation\n    df_sectors.at[idx, 'nom_formation'] = formation_name\n# Sauvegarder le rsultat dans un fichier Excel\ndf_sectors.to_excel('secteurs_formations_associes.xlsx', index=False)",
        "detail": "notoyage_data.llama_formation_sector",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.secteurData",
        "description": "notoyage_data.secteurData",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_ville.xlsx')\nsecteurs_df = pd.read_excel('secteurs_avec_id.xlsx')\n# Associer chaque secteur  son id_secteur\n# Nous effectuons un merge (jointure) entre les deux DataFrames sur le nom du secteur\ndata = data.merge(secteurs_df, left_on='Secteur', right_on='nom_secteur', how='left')\n# Supprimer la colonne nom_secteur puisqu'on a dj la colonne Secteur\ndata.drop(columns=['nom_secteur'], inplace=True)\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_id_secteur.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier",
        "detail": "notoyage_data.secteurData",
        "documentation": {}
    },
    {
        "label": "secteurs_df",
        "kind": 5,
        "importPath": "notoyage_data.secteurData",
        "description": "notoyage_data.secteurData",
        "peekOfCode": "secteurs_df = pd.read_excel('secteurs_avec_id.xlsx')\n# Associer chaque secteur  son id_secteur\n# Nous effectuons un merge (jointure) entre les deux DataFrames sur le nom du secteur\ndata = data.merge(secteurs_df, left_on='Secteur', right_on='nom_secteur', how='left')\n# Supprimer la colonne nom_secteur puisqu'on a dj la colonne Secteur\ndata.drop(columns=['nom_secteur'], inplace=True)\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_id_secteur.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data.head())",
        "detail": "notoyage_data.secteurData",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.secteurData",
        "description": "notoyage_data.secteurData",
        "peekOfCode": "data = data.merge(secteurs_df, left_on='Secteur', right_on='nom_secteur', how='left')\n# Supprimer la colonne nom_secteur puisqu'on a dj la colonne Secteur\ndata.drop(columns=['nom_secteur'], inplace=True)\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_id_secteur.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data.head())",
        "detail": "notoyage_data.secteurData",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "df = pd.read_excel('skills_output.xlsx')\n# Extraire les comptences uniques (distinctes) de la colonne 'Skill'\nunique_skills = df['Skill'].drop_duplicates().reset_index(drop=True)\n# Ajouter un identifiant unique commenant  1\nunique_skills_df = pd.DataFrame({\n    'Skill ID': range(1, len(unique_skills) + 1),\n    'Skill': unique_skills\n})\n# Enregistrer les comptences distinctes dans un nouveau fichier Excel\ndistinct_skills_excel_file = 'distinct_skills.xlsx'",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "unique_skills",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "unique_skills = df['Skill'].drop_duplicates().reset_index(drop=True)\n# Ajouter un identifiant unique commenant  1\nunique_skills_df = pd.DataFrame({\n    'Skill ID': range(1, len(unique_skills) + 1),\n    'Skill': unique_skills\n})\n# Enregistrer les comptences distinctes dans un nouveau fichier Excel\ndistinct_skills_excel_file = 'distinct_skills.xlsx'\nunique_skills_df.to_excel(distinct_skills_excel_file, index=False)\nprint(f\"Les comptences distinctes ont t enregistres dans le fichier Excel : {distinct_skills_excel_file}\")",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "unique_skills_df",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "unique_skills_df = pd.DataFrame({\n    'Skill ID': range(1, len(unique_skills) + 1),\n    'Skill': unique_skills\n})\n# Enregistrer les comptences distinctes dans un nouveau fichier Excel\ndistinct_skills_excel_file = 'distinct_skills.xlsx'\nunique_skills_df.to_excel(distinct_skills_excel_file, index=False)\nprint(f\"Les comptences distinctes ont t enregistres dans le fichier Excel : {distinct_skills_excel_file}\")",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "distinct_skills_excel_file",
        "kind": 5,
        "importPath": "notoyage_data.skills3",
        "description": "notoyage_data.skills3",
        "peekOfCode": "distinct_skills_excel_file = 'distinct_skills.xlsx'\nunique_skills_df.to_excel(distinct_skills_excel_file, index=False)\nprint(f\"Les comptences distinctes ont t enregistres dans le fichier Excel : {distinct_skills_excel_file}\")",
        "detail": "notoyage_data.skills3",
        "documentation": {}
    },
    {
        "label": "skills_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "skills_df = pd.read_excel('skills_output.xlsx')  # Contient 'Job Id' et 'Skill'\ndistinct_skills_df = pd.read_excel('distinct_skills.xlsx')  # Contient 'Skill ID' et 'Skill'\n# Fusionner les deux DataFrames pour obtenir 'Skill ID'  partir du 'Skill'\nmerged_df = pd.merge(skills_df, distinct_skills_df, on='Skill', how='inner')\n# Garder uniquement les colonnes 'Job Id' et 'Skill ID'\njob_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le rsultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les donnes Job Id et Skill ID ont t enregistres dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "distinct_skills_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "distinct_skills_df = pd.read_excel('distinct_skills.xlsx')  # Contient 'Skill ID' et 'Skill'\n# Fusionner les deux DataFrames pour obtenir 'Skill ID'  partir du 'Skill'\nmerged_df = pd.merge(skills_df, distinct_skills_df, on='Skill', how='inner')\n# Garder uniquement les colonnes 'Job Id' et 'Skill ID'\njob_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le rsultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les donnes Job Id et Skill ID ont t enregistres dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "merged_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "merged_df = pd.merge(skills_df, distinct_skills_df, on='Skill', how='inner')\n# Garder uniquement les colonnes 'Job Id' et 'Skill ID'\njob_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le rsultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les donnes Job Id et Skill ID ont t enregistres dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "job_skill_df",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "job_skill_df = merged_df[['Job Id', 'Skill ID']]\n# Enregistrer le rsultat dans un nouveau fichier Excel\noutput_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les donnes Job Id et Skill ID ont t enregistres dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "output_job_skill_excel",
        "kind": 5,
        "importPath": "notoyage_data.skills4",
        "description": "notoyage_data.skills4",
        "peekOfCode": "output_job_skill_excel = 'job_skill_mapping.xlsx'\njob_skill_df.to_excel(output_job_skill_excel, index=False)\nprint(f\"Les donnes Job Id et Skill ID ont t enregistres dans le fichier Excel : {output_job_skill_excel}\")",
        "detail": "notoyage_data.skills4",
        "documentation": {}
    },
    {
        "label": "input_json_file",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "input_json_file = 'extracted_skills.json'  # Remplacez par le nom de votre fichier JSON\nwith open(input_json_file, 'r') as file:\n    json_data = json.load(file)\n# Crer une liste pour stocker les donnes structures\ndata = []\n# Parcourir chaque entre dans le fichier JSON\nfor entry in json_data:\n    job_id = entry[\"Job Id\"]\n    skills = entry[\"Skills\"]\n    # Omettre les deux premiers skills et le dernier",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "data = []\n# Parcourir chaque entre dans le fichier JSON\nfor entry in json_data:\n    job_id = entry[\"Job Id\"]\n    skills = entry[\"Skills\"]\n    # Omettre les deux premiers skills et le dernier\n    filtered_skills = skills[2:-1]  # Exclut les deux premiers et le dernier\n    # Ajouter chaque skill avec l'ID correspondant  la liste de donnes\n    for skill in filtered_skills:\n        data.append({\"Job Id\": job_id, \"Skill\": skill})",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "df = pd.DataFrame(data)\n# Enregistrer les donnes dans un fichier Excel\noutput_excel_file = 'skills_output.xlsx'\ndf.to_excel(output_excel_file, index=False)\nprint(f\"Les donnes ont t transfres dans le fichier Excel : {output_excel_file}\")",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "output_excel_file",
        "kind": 5,
        "importPath": "notoyage_data.skils2",
        "description": "notoyage_data.skils2",
        "peekOfCode": "output_excel_file = 'skills_output.xlsx'\ndf.to_excel(output_excel_file, index=False)\nprint(f\"Les donnes ont t transfres dans le fichier Excel : {output_excel_file}\")",
        "detail": "notoyage_data.skils2",
        "documentation": {}
    },
    {
        "label": "ats_extractor",
        "kind": 2,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "def ats_extractor(text):\n    # Prompt for extracting skills information\n    prompt = \"\"\"\n    You are an AI assistant. Your task is to extract a list of skills mentioned in the text provided by the user. \n    Please list the skills you can identify, separating them with commas.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": text}\n    ]",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "CONFIG_PATH",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "CONFIG_PATH = r\"config.yaml\"\napi_key = None\n# Load the API key from YAML configuration file\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialize the Groq API client\ngroq_client = Groq(api_key=api_key)\ndef ats_extractor(text):\n    # Prompt for extracting skills information",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "api_key = None\n# Load the API key from YAML configuration file\nwith open(CONFIG_PATH) as file:\n    data = yaml.load(file, Loader=yaml.FullLoader)\n    api_key = data['GROQ_API_KEY']\n# Initialize the Groq API client\ngroq_client = Groq(api_key=api_key)\ndef ats_extractor(text):\n    # Prompt for extracting skills information\n    prompt = \"\"\"",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "groq_client",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "groq_client = Groq(api_key=api_key)\ndef ats_extractor(text):\n    # Prompt for extracting skills information\n    prompt = \"\"\"\n    You are an AI assistant. Your task is to extract a list of skills mentioned in the text provided by the user. \n    Please list the skills you can identify, separating them with commas.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": text}",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "excel_file_path",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "excel_file_path = 'fichier_reduit_avec_contrat.xlsx'  # Remplacez par le chemin de votre fichier Excel\ndata = pd.read_excel(excel_file_path)\n# Create a list to store JSON data\njson_data = []\n# Iterate over the rows in the Excel file\nfor index, row in data.iterrows():\n    job_id = row['Job Id']\n    skills_text = row['skills']\n    # Extract skills using the ats_extractor function\n    skills_list = ats_extractor(skills_text)",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "data = pd.read_excel(excel_file_path)\n# Create a list to store JSON data\njson_data = []\n# Iterate over the rows in the Excel file\nfor index, row in data.iterrows():\n    job_id = row['Job Id']\n    skills_text = row['skills']\n    # Extract skills using the ats_extractor function\n    skills_list = ats_extractor(skills_text)\n    # Append data to JSON structure",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "json_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "json_data = []\n# Iterate over the rows in the Excel file\nfor index, row in data.iterrows():\n    job_id = row['Job Id']\n    skills_text = row['skills']\n    # Extract skills using the ats_extractor function\n    skills_list = ats_extractor(skills_text)\n    # Append data to JSON structure\n    json_data.append({\n        \"Job Id\": job_id,",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "output_json_file",
        "kind": 5,
        "importPath": "notoyage_data.transfert10_skills",
        "description": "notoyage_data.transfert10_skills",
        "peekOfCode": "output_json_file = 'extracted_skills.json'\nwith open(output_json_file, 'w') as json_file:\n    json.dump(json_data, json_file, indent=4)\nprint(f\"Les comptences extraites ont t sauvegardes dans {output_json_file}\")",
        "detail": "notoyage_data.transfert10_skills",
        "documentation": {}
    },
    {
        "label": "assign_language",
        "kind": 2,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "def assign_language(country):\n    if country == \"Morocco\":\n        # 90 % Franais, 10 % Anglais\n        if random.random() < 0.9:\n            return 2  # ID de Franais\n        else:\n            return 4  # ID d'Anglais\n    else:\n        # Si le pays n'est pas spcifi, retourner une langue par dfaut\n        return 4  # Par exemple, ID de Arabe",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_contrat.xlsx')\n# Charger les langues depuis un autre fichier Excel ou dfinir manuellement\nlanguages = [\n    {\"id_lang\": 1, \"name\": \"Arabe\"},\n    {\"id_lang\": 2, \"name\": \"Franais\"},\n    {\"id_lang\": 3, \"name\": \"Amazigh\"},\n    {\"id_lang\": 4, \"name\": \"Anglais\"},\n    {\"id_lang\": 13, \"name\": \"Espagnol\"},\n    # Ajoutez d'autres langues si ncessaire\n]",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "languages",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "languages = [\n    {\"id_lang\": 1, \"name\": \"Arabe\"},\n    {\"id_lang\": 2, \"name\": \"Franais\"},\n    {\"id_lang\": 3, \"name\": \"Amazigh\"},\n    {\"id_lang\": 4, \"name\": \"Anglais\"},\n    {\"id_lang\": 13, \"name\": \"Espagnol\"},\n    # Ajoutez d'autres langues si ncessaire\n]\n# Convertir les langues en DataFrame\nlanguages_df = pd.DataFrame(languages)",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "languages_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "languages_df = pd.DataFrame(languages)\n# Fonction pour assigner une langue en fonction du pays\ndef assign_language(country):\n    if country == \"Morocco\":\n        # 90 % Franais, 10 % Anglais\n        if random.random() < 0.9:\n            return 2  # ID de Franais\n        else:\n            return 4  # ID d'Anglais\n    else:",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "data['id_lang']",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "data['id_lang'] = data['Country'].apply(assign_language)\n# Crer un DataFrame pour \"Job Id\" et \"id_lang\"\noutput_df = data[['Job Id', 'id_lang']]\n# Sauvegarder les rsultats dans un fichier Excel\noutput_df.to_excel('job_id_languages.xlsx', index=False)\n# Afficher un aperu des donnes sauvegardes\nprint(output_df.head())",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "output_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert11_langes",
        "description": "notoyage_data.transfert11_langes",
        "peekOfCode": "output_df = data[['Job Id', 'id_lang']]\n# Sauvegarder les rsultats dans un fichier Excel\noutput_df.to_excel('job_id_languages.xlsx', index=False)\n# Afficher un aperu des donnes sauvegardes\nprint(output_df.head())",
        "detail": "notoyage_data.transfert11_langes",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert12_etat",
        "description": "notoyage_data.transfert12_etat",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_contrat.xlsx')\n# Extraire la colonne \"Country\" et supprimer les doublons\nunique_countries = data['Country'].drop_duplicates().reset_index(drop=True)\n# Gnrer un DataFrame avec id_etat et nom de l'tat\nunique_countries_df = pd.DataFrame({\n    'id_etat': range(1, len(unique_countries) + 1),  # Gnrer des ID uniques\n    'nom_etat': unique_countries\n})\n# Sauvegarder le DataFrame dans un fichier Excel\nunique_countries_df.to_excel('unique_countries.xlsx', index=False)",
        "detail": "notoyage_data.transfert12_etat",
        "documentation": {}
    },
    {
        "label": "unique_countries",
        "kind": 5,
        "importPath": "notoyage_data.transfert12_etat",
        "description": "notoyage_data.transfert12_etat",
        "peekOfCode": "unique_countries = data['Country'].drop_duplicates().reset_index(drop=True)\n# Gnrer un DataFrame avec id_etat et nom de l'tat\nunique_countries_df = pd.DataFrame({\n    'id_etat': range(1, len(unique_countries) + 1),  # Gnrer des ID uniques\n    'nom_etat': unique_countries\n})\n# Sauvegarder le DataFrame dans un fichier Excel\nunique_countries_df.to_excel('unique_countries.xlsx', index=False)\n# Afficher un aperu des rsultats\nprint(unique_countries_df.head())",
        "detail": "notoyage_data.transfert12_etat",
        "documentation": {}
    },
    {
        "label": "unique_countries_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert12_etat",
        "description": "notoyage_data.transfert12_etat",
        "peekOfCode": "unique_countries_df = pd.DataFrame({\n    'id_etat': range(1, len(unique_countries) + 1),  # Gnrer des ID uniques\n    'nom_etat': unique_countries\n})\n# Sauvegarder le DataFrame dans un fichier Excel\nunique_countries_df.to_excel('unique_countries.xlsx', index=False)\n# Afficher un aperu des rsultats\nprint(unique_countries_df.head())",
        "detail": "notoyage_data.transfert12_etat",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_etat.xlsx')\n# Extraire les colonnes 'Location' et 'id_etat'\ncities = data[['location', 'id_etat']].drop_duplicates()\n# Supprimer les lignes o 'Location' est vide ou NaN\ncities = cities.dropna(subset=['location'])\n# Gnrer un id_ville unique pour chaque ville\ncities['id_ville'] = range(1, len(cities) + 1)\n# Rorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clart",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities = data[['location', 'id_etat']].drop_duplicates()\n# Supprimer les lignes o 'Location' est vide ou NaN\ncities = cities.dropna(subset=['location'])\n# Gnrer un id_ville unique pour chaque ville\ncities['id_ville'] = range(1, len(cities) + 1)\n# Rorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clart\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities = cities.dropna(subset=['location'])\n# Gnrer un id_ville unique pour chaque ville\ncities['id_ville'] = range(1, len(cities) + 1)\n# Rorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clart\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes\ncities.to_excel('villes_avec_id.xlsx', index=False)\n# Afficher un aperu du rsultat",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities['id_ville']",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities['id_ville'] = range(1, len(cities) + 1)\n# Rorganiser les colonnes\ncities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clart\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes\ncities.to_excel('villes_avec_id.xlsx', index=False)\n# Afficher un aperu du rsultat\nprint(cities.head())",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert13_ville",
        "description": "notoyage_data.transfert13_ville",
        "peekOfCode": "cities = cities[['id_ville', 'id_etat', 'location']]\n# Renommer les colonnes pour plus de clart\ncities.rename(columns={'location': 'nom_ville'}, inplace=True)\n# Sauvegarder le nouveau fichier contenant les villes\ncities.to_excel('villes_avec_id.xlsx', index=False)\n# Afficher un aperu du rsultat\nprint(cities.head())",
        "detail": "notoyage_data.transfert13_ville",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert14_secteur",
        "description": "notoyage_data.transfert14_secteur",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_ville.xlsx')\n# Extraire les secteurs uniques\nsecteurs_uniques = data['Secteur'].dropna().unique()\n# Crer un DataFrame avec un id pour chaque secteur unique\nsecteurs_df = pd.DataFrame({\n    'id_secteur': range(1, len(secteurs_uniques) + 1),\n    'nom_secteur': secteurs_uniques\n})\n# Sauvegarder le fichier avec les secteurs et leurs identifiants\nsecteurs_df.to_excel('secteurs_avec_id.xlsx', index=False)",
        "detail": "notoyage_data.transfert14_secteur",
        "documentation": {}
    },
    {
        "label": "secteurs_uniques",
        "kind": 5,
        "importPath": "notoyage_data.transfert14_secteur",
        "description": "notoyage_data.transfert14_secteur",
        "peekOfCode": "secteurs_uniques = data['Secteur'].dropna().unique()\n# Crer un DataFrame avec un id pour chaque secteur unique\nsecteurs_df = pd.DataFrame({\n    'id_secteur': range(1, len(secteurs_uniques) + 1),\n    'nom_secteur': secteurs_uniques\n})\n# Sauvegarder le fichier avec les secteurs et leurs identifiants\nsecteurs_df.to_excel('secteurs_avec_id.xlsx', index=False)\n# Afficher un aperu des secteurs gnrs pour vrification\nprint(secteurs_df.head())",
        "detail": "notoyage_data.transfert14_secteur",
        "documentation": {}
    },
    {
        "label": "secteurs_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert14_secteur",
        "description": "notoyage_data.transfert14_secteur",
        "peekOfCode": "secteurs_df = pd.DataFrame({\n    'id_secteur': range(1, len(secteurs_uniques) + 1),\n    'nom_secteur': secteurs_uniques\n})\n# Sauvegarder le fichier avec les secteurs et leurs identifiants\nsecteurs_df.to_excel('secteurs_avec_id.xlsx', index=False)\n# Afficher un aperu des secteurs gnrs pour vrification\nprint(secteurs_df.head())",
        "detail": "notoyage_data.transfert14_secteur",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_secteur.xlsx')\n# Extraire les types de travail uniques\nunique_work_types = data['Work Type'].dropna().unique()\n# Crer un DataFrame avec les types de travail et leur identifiant\nwork_type_df = pd.DataFrame(unique_work_types, columns=['nom_type_trav'])\n# Ajouter un identifiant pour chaque type de travail\nwork_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "unique_work_types",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "unique_work_types = data['Work Type'].dropna().unique()\n# Crer un DataFrame avec les types de travail et leur identifiant\nwork_type_df = pd.DataFrame(unique_work_types, columns=['nom_type_trav'])\n# Ajouter un identifiant pour chaque type de travail\nwork_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(work_type_df.head())",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "work_type_df",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "work_type_df = pd.DataFrame(unique_work_types, columns=['nom_type_trav'])\n# Ajouter un identifiant pour chaque type de travail\nwork_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(work_type_df.head())",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "work_type_df['id_type_trav']",
        "kind": 5,
        "importPath": "notoyage_data.transfert15_type_travail",
        "description": "notoyage_data.transfert15_type_travail",
        "peekOfCode": "work_type_df['id_type_trav'] = range(1, len(work_type_df) + 1)\n# Sauvegarder le DataFrame dans un nouveau fichier Excel\nwork_type_df.to_excel('type_trav_avec_id.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(work_type_df.head())",
        "detail": "notoyage_data.transfert15_type_travail",
        "documentation": {}
    },
    {
        "label": "assign_contrat",
        "kind": 2,
        "importPath": "notoyage_data.transfert16_contrat",
        "description": "notoyage_data.transfert16_contrat",
        "peekOfCode": "def assign_contrat(row, cdi_ratio=0.4):\n    work_type = row['Work Type']\n    # Si le type de travail est \"Contract\", il y a une chance de 40% d'tre CDI\n    if work_type == 'Contract':\n        if random.random() < cdi_ratio:  # 40% chance de CDI\n            return 'CDI'\n        else:\n            return 'CDD'\n    # Affecter les autres types directement\n    elif work_type == 'Full-Time':",
        "detail": "notoyage_data.transfert16_contrat",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert16_contrat",
        "description": "notoyage_data.transfert16_contrat",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_telephone_mise_a_jour.xlsx')\n# Fonction pour assigner les valeurs du contrat selon le type de travail\ndef assign_contrat(row, cdi_ratio=0.4):\n    work_type = row['Work Type']\n    # Si le type de travail est \"Contract\", il y a une chance de 40% d'tre CDI\n    if work_type == 'Contract':\n        if random.random() < cdi_ratio:  # 40% chance de CDI\n            return 'CDI'\n        else:\n            return 'CDD'",
        "detail": "notoyage_data.transfert16_contrat",
        "documentation": {}
    },
    {
        "label": "data['contrat']",
        "kind": 5,
        "importPath": "notoyage_data.transfert16_contrat",
        "description": "notoyage_data.transfert16_contrat",
        "peekOfCode": "data['contrat'] = data.apply(assign_contrat, axis=1)\n# Sauvegarder le DataFrame mis  jour dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_contrat.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(data.head())",
        "detail": "notoyage_data.transfert16_contrat",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "data = pd.read_csv('salaries_max_convertis.csv')\n# Supprimer les doublons par entreprise et conserver les premires occurrences de chaque entreprise\nunique_companies = data.drop_duplicates(subset=['Company'])\n# Crer un DataFrame avec les informations demandes\ncompany_data = unique_companies[['Company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']].copy()\n# Renommer les colonnes pour correspondre au format demand\ncompany_data.rename(columns={\n    'Company': 'nom_company',\n    'email': 'email',\n    'URL': 'URL',",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "unique_companies",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "unique_companies = data.drop_duplicates(subset=['Company'])\n# Crer un DataFrame avec les informations demandes\ncompany_data = unique_companies[['Company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']].copy()\n# Renommer les colonnes pour correspondre au format demand\ncompany_data.rename(columns={\n    'Company': 'nom_company',\n    'email': 'email',\n    'URL': 'URL',\n    'telephone': 'telephone'\n}, inplace=True)",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "company_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "company_data = unique_companies[['Company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']].copy()\n# Renommer les colonnes pour correspondre au format demand\ncompany_data.rename(columns={\n    'Company': 'nom_company',\n    'email': 'email',\n    'URL': 'URL',\n    'telephone': 'telephone'\n}, inplace=True)\n# Ajouter un identifiant unique pour chaque entreprise\ncompany_data['id_company'] = range(1, len(company_data) + 1)",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "company_data['id_company']",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "company_data['id_company'] = range(1, len(company_data) + 1)\n# Rorganiser les colonnes pour l'ordre souhait\ncompany_data = company_data[['id_company', 'nom_company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']]\n# Sauvegarder les informations dans un nouveau fichier Excel\ncompany_data.to_csv('entreprises_avec_id.csv', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(company_data.head())",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "company_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert17_entreprise",
        "description": "notoyage_data.transfert17_entreprise",
        "peekOfCode": "company_data = company_data[['id_company', 'nom_company', 'email', 'URL', 'telephone', 'id_secteur', 'id_ville']]\n# Sauvegarder les informations dans un nouveau fichier Excel\ncompany_data.to_csv('entreprises_avec_id.csv', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(company_data.head())",
        "detail": "notoyage_data.transfert17_entreprise",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "data = pd.read_csv('fichier_valide.csv')\n# Slectionner les colonnes souhaites et renommer si ncessaire\nfields_to_keep = {\n    'Job Id': 'Job Id',              # Garder la colonne Job ID\n    'id_type_contrat': 'id_type_contrat',  # Garder la colonne id_type_contrat\n    'id_company': 'id_company',          # Garder la colonne id_company\n    'id_type_trav': 'id_type_trav',      # Garder la colonne id_type_trav\n    'salaire': 'salaire',                 # Renommer Salary en salaire\n    'Experience': 'Experience',          # Garder la colonne Experience\n    'email': 'email_offre'               # Renommer Email en email_offre",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "fields_to_keep",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "fields_to_keep = {\n    'Job Id': 'Job Id',              # Garder la colonne Job ID\n    'id_type_contrat': 'id_type_contrat',  # Garder la colonne id_type_contrat\n    'id_company': 'id_company',          # Garder la colonne id_company\n    'id_type_trav': 'id_type_trav',      # Garder la colonne id_type_trav\n    'salaire': 'salaire',                 # Renommer Salary en salaire\n    'Experience': 'Experience',          # Garder la colonne Experience\n    'email': 'email_offre'               # Renommer Email en email_offre\n}\nnew_data = data[list(fields_to_keep.keys())].rename(columns=fields_to_keep)",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "new_data",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "new_data = data[list(fields_to_keep.keys())].rename(columns=fields_to_keep)\n# Convertir la colonne 'salaire' en un nombre dcimal\nnew_data['salaire'] = (\n    new_data['salaire']  # Travailler sur la colonne salaire\n    .replace({',': ''}, regex=True)  # Supprimer les virgules\n    .astype(float)  # Convertir en nombre dcimal\n)\n# Sauvegarder le nouveau fichier Excel\nnew_data.to_csv('offres_emploi_filtrees2.csv', index=False)\n# Afficher un aperu des premires lignes",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "new_data['salaire']",
        "kind": 5,
        "importPath": "notoyage_data.transfert18_fait",
        "description": "notoyage_data.transfert18_fait",
        "peekOfCode": "new_data['salaire'] = (\n    new_data['salaire']  # Travailler sur la colonne salaire\n    .replace({',': ''}, regex=True)  # Supprimer les virgules\n    .astype(float)  # Convertir en nombre dcimal\n)\n# Sauvegarder le nouveau fichier Excel\nnew_data.to_csv('offres_emploi_filtrees2.csv', index=False)\n# Afficher un aperu des premires lignes\nprint(new_data.head())",
        "detail": "notoyage_data.transfert18_fait",
        "documentation": {}
    },
    {
        "label": "extract_max_salary",
        "kind": 2,
        "importPath": "notoyage_data.transfert1_salaire",
        "description": "notoyage_data.transfert1_salaire",
        "peekOfCode": "def extract_max_salary(salary_range):\n    # Diviser les valeurs en deux parties\n    _, max_salary = salary_range.split('-')\n    # Enlever le symbole $ et le K, puis multiplier par 100 pour obtenir le format dcimal\n    max_decimal = float(max_salary.replace('$', '').replace('K', '')) * 100\n    # Retourner la valeur maximale en format dcimal avec deux chiffres aprs la virgule\n    return f\"{max_decimal:,.2f}\"\n# Appliquer la conversion pour extraire la valeur maximale et crer la colonne 'salaire'\ndata['salaire'] = data['Salary Range'].apply(extract_max_salary)\n# Sauvegarder le rsultat dans un nouveau fichier Excel",
        "detail": "notoyage_data.transfert1_salaire",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert1_salaire",
        "description": "notoyage_data.transfert1_salaire",
        "peekOfCode": "data = pd.read_excel('fichier_reduit.xlsx')\n# Fonction pour convertir la valeur maximale de la plage en dcimal\ndef extract_max_salary(salary_range):\n    # Diviser les valeurs en deux parties\n    _, max_salary = salary_range.split('-')\n    # Enlever le symbole $ et le K, puis multiplier par 100 pour obtenir le format dcimal\n    max_decimal = float(max_salary.replace('$', '').replace('K', '')) * 100\n    # Retourner la valeur maximale en format dcimal avec deux chiffres aprs la virgule\n    return f\"{max_decimal:,.2f}\"\n# Appliquer la conversion pour extraire la valeur maximale et crer la colonne 'salaire'",
        "detail": "notoyage_data.transfert1_salaire",
        "documentation": {}
    },
    {
        "label": "data['salaire']",
        "kind": 5,
        "importPath": "notoyage_data.transfert1_salaire",
        "description": "notoyage_data.transfert1_salaire",
        "peekOfCode": "data['salaire'] = data['Salary Range'].apply(extract_max_salary)\n# Sauvegarder le rsultat dans un nouveau fichier Excel\ndata.to_excel('salaries_max_convertis.xlsx', index=False)\nprint(\"La conversion est termine et les donnes sont sauvegardes dans 'salaries_max_convertis.xlsx'.\")",
        "detail": "notoyage_data.transfert1_salaire",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert2_maroc_villes",
        "description": "notoyage_data.transfert2_maroc_villes",
        "peekOfCode": "data = pd.read_excel('salaries_max_convertis.xlsx')\n# Liste des villes marocaines\nmoroccan_cities = [\n    'Casablanca', 'Rabat', 'Sal', 'Agadir', 'Tanger', 'Fs',\n    'Marrakech', 'Mekns', 'Oujda', 'Kenitra', 'Ttouan', 'Safi',\n    'Mohammedia', 'El Jadida', 'Khouribga', 'Beni Mellal',\n    'Nador', 'Khemisset', 'Settat', 'Larache', 'Guelmim',\n    'Taza', 'Tan-Tan', 'Errachidia', 'Azrou', 'Ouarzazate',\n    'Layoune', 'Al Hoceima', 'Ifrane', 'Asilah'\n]",
        "detail": "notoyage_data.transfert2_maroc_villes",
        "documentation": {}
    },
    {
        "label": "moroccan_cities",
        "kind": 5,
        "importPath": "notoyage_data.transfert2_maroc_villes",
        "description": "notoyage_data.transfert2_maroc_villes",
        "peekOfCode": "moroccan_cities = [\n    'Casablanca', 'Rabat', 'Sal', 'Agadir', 'Tanger', 'Fs',\n    'Marrakech', 'Mekns', 'Oujda', 'Kenitra', 'Ttouan', 'Safi',\n    'Mohammedia', 'El Jadida', 'Khouribga', 'Beni Mellal',\n    'Nador', 'Khemisset', 'Settat', 'Larache', 'Guelmim',\n    'Taza', 'Tan-Tan', 'Errachidia', 'Azrou', 'Ouarzazate',\n    'Layoune', 'Al Hoceima', 'Ifrane', 'Asilah'\n]\n# Slectionner alatoirement 700 indices de lignes dans le dataset\nindices_to_change = random.sample(range(len(data)), 700)",
        "detail": "notoyage_data.transfert2_maroc_villes",
        "documentation": {}
    },
    {
        "label": "indices_to_change",
        "kind": 5,
        "importPath": "notoyage_data.transfert2_maroc_villes",
        "description": "notoyage_data.transfert2_maroc_villes",
        "peekOfCode": "indices_to_change = random.sample(range(len(data)), 700)\n# Modifier les lignes slectionnes avec le pays 'Morocco' et des villes marocaines alatoires\nfor idx in indices_to_change:\n    data.at[idx, 'Country'] = 'Morocco'\n    data.at[idx, 'location'] = random.choice(moroccan_cities)\n# Sauvegarder le dataset modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_maroc.xlsx', index=False)\nprint(\"Modification termine et les donnes sont sauvegardes dans 'fichier_reduit_avec_maroc.xlsx'.\")",
        "detail": "notoyage_data.transfert2_maroc_villes",
        "documentation": {}
    },
    {
        "label": "extract_sector",
        "kind": 2,
        "importPath": "notoyage_data.transfert3_cree_sector",
        "description": "notoyage_data.transfert3_cree_sector",
        "peekOfCode": "def extract_sector(company_profile):\n    try:\n        # Convertir la chane de caractres en dictionnaire\n        profile_dict = ast.literal_eval(company_profile)\n        # Retourner la valeur du secteur\n        return profile_dict.get('Sector', None)  # Renvoie None si 'Sector' n'existe pas\n    except:\n        return None\n# Appliquer la fonction  la colonne 'Company Profile' et crer la nouvelle colonne 'Secteur'\ndata['Secteur'] = data['Company Profile'].apply(extract_sector)",
        "detail": "notoyage_data.transfert3_cree_sector",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert3_cree_sector",
        "description": "notoyage_data.transfert3_cree_sector",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_maroc.xlsx')\n# Fonction pour extraire la valeur du secteur depuis la colonne \"Company Profile\"\ndef extract_sector(company_profile):\n    try:\n        # Convertir la chane de caractres en dictionnaire\n        profile_dict = ast.literal_eval(company_profile)\n        # Retourner la valeur du secteur\n        return profile_dict.get('Sector', None)  # Renvoie None si 'Sector' n'existe pas\n    except:\n        return None",
        "detail": "notoyage_data.transfert3_cree_sector",
        "documentation": {}
    },
    {
        "label": "data['Secteur']",
        "kind": 5,
        "importPath": "notoyage_data.transfert3_cree_sector",
        "description": "notoyage_data.transfert3_cree_sector",
        "peekOfCode": "data['Secteur'] = data['Company Profile'].apply(extract_sector)\n# Sauvegarder le dataset modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_secteur.xlsx', index=False)\nprint(\"Modification termine et les donnes sont sauvegardes dans 'fichier_reduit_avec_secteur.xlsx'.\")",
        "detail": "notoyage_data.transfert3_cree_sector",
        "documentation": {}
    },
    {
        "label": "update_company",
        "kind": 2,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "def update_company(row):\n    country = row['Country']\n    secteur = row['Secteur']  # Rcuprer le secteur\n    if country == 'Morocco' and secteur in companies_by_sector:\n        # Slectionner une entreprise au hasard dans le secteur correspondant\n        company = random.choice(companies_by_sector[secteur])\n        row['Company'] = company  # Mettre  jour la colonne 'Company'\n    return row\n# Appliquer la fonction  chaque ligne du dataframe\ndata = data.apply(update_company, axis=1)",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "companies_by_sector",
        "kind": 5,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "companies_by_sector = {\n    'Energy': ['Nareva Holding', 'OCP Group', 'LafargeHolcim Maroc', 'Managem'],\n    'Consumer Goods': ['Cosumar', 'Sidi Ali', 'Danone Maroc', 'Unilever Maroc'],\n    'Healthcare Services': ['Clinique Internationale de Marrakech', 'Clinique Al Azhar', 'Rseau de sant Al Amal'],\n    'Insurance': ['Attijariwafa Assurance', 'RMA Watanya', 'Saham Assurance'],\n    'Lab Equipment': ['Biopharma', 'Medtech Maroc', 'Pharmatex'],\n    'Financial Services': ['Bank of Africa', 'Attijariwafa Bank', 'BMCE Bank of Africa', 'CIH Bank'],\n    'Healthcare Technology': ['Medisys', 'HemoTech', 'InnovHealth'],\n    'Industrial': ['LafargeHolcim Maroc', 'Maroc Chimie', 'Managem'],\n    'Logistics': ['CTM', 'Groupe BDP International', 'Transports Ocan'],",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_secteur.xlsx')\n# Fonction pour mettre  jour la colonne 'Company' en fonction du secteur et du pays\ndef update_company(row):\n    country = row['Country']\n    secteur = row['Secteur']  # Rcuprer le secteur\n    if country == 'Morocco' and secteur in companies_by_sector:\n        # Slectionner une entreprise au hasard dans le secteur correspondant\n        company = random.choice(companies_by_sector[secteur])\n        row['Company'] = company  # Mettre  jour la colonne 'Company'\n    return row",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert4_sector_maroc",
        "description": "notoyage_data.transfert4_sector_maroc",
        "peekOfCode": "data = data.apply(update_company, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_entreprises_mises_a_jour.xlsx', index=False)\nprint(\"Les entreprises ont t mises  jour dans le fichier 'fichier_reduit_avec_entreprises_mises_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert4_sector_maroc",
        "documentation": {}
    },
    {
        "label": "extract_url",
        "kind": 2,
        "importPath": "notoyage_data.transfert5_url",
        "description": "notoyage_data.transfert5_url",
        "peekOfCode": "def extract_url(company_profile):\n    try:\n        # Convertir le texte JSON en dictionnaire Python\n        profile = json.loads(company_profile)\n        # Extraire l'URL si elle existe\n        return profile.get(\"Website\", \"\")  # Retourne la valeur de \"Website\" ou une chane vide si non trouv\n    except (json.JSONDecodeError, TypeError):\n        # Retourne une chane vide en cas d'erreur de format\n        return \"\"\n# Appliquer la fonction  la colonne 'Company Profile' pour crer la nouvelle colonne 'URL'",
        "detail": "notoyage_data.transfert5_url",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert5_url",
        "description": "notoyage_data.transfert5_url",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_entreprises_mises_a_jour.xlsx')\n# Fonction pour extraire l'URL du champ 'Company Profile'\ndef extract_url(company_profile):\n    try:\n        # Convertir le texte JSON en dictionnaire Python\n        profile = json.loads(company_profile)\n        # Extraire l'URL si elle existe\n        return profile.get(\"Website\", \"\")  # Retourne la valeur de \"Website\" ou une chane vide si non trouv\n    except (json.JSONDecodeError, TypeError):\n        # Retourne une chane vide en cas d'erreur de format",
        "detail": "notoyage_data.transfert5_url",
        "documentation": {}
    },
    {
        "label": "data['URL']",
        "kind": 5,
        "importPath": "notoyage_data.transfert5_url",
        "description": "notoyage_data.transfert5_url",
        "peekOfCode": "data['URL'] = data['Company Profile'].apply(extract_url)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_URL.xlsx', index=False)\nprint(\"La colonne URL a t ajoute et sauvegarde dans le fichier 'fichier_reduit_avec_URL.xlsx'.\")",
        "detail": "notoyage_data.transfert5_url",
        "documentation": {}
    },
    {
        "label": "update_url",
        "kind": 2,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "def update_url(row):\n    company_name = row['Company']\n    # Si l'entreprise existe dans le dictionnaire, on met  jour l'URL, sinon on garde l'ancienne valeur\n    return company_urls.get(company_name, row['URL'])  # Renvoyer l'ancienne URL si l'entreprise n'est pas trouve\n# Appliquer la fonction  la colonne 'Company' pour mettre  jour la colonne 'URL'\ndata['URL'] = data.apply(update_url, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_URL_mis_a_jour.xlsx', index=False)\nprint(\"La colonne URL a t mise  jour et sauvegarde dans le fichier 'fichier_reduit_avec_URL_mis_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_URL.xlsx')\n# Dictionnaire des entreprises et de leurs URLs\ncompany_urls = {\n    \"Nareva Holding\": \"http://www.nareva.ma\",\n    \"Unilever Maroc\": \"https://www.unilever.com\",\n    \"Clinique Internationale de Marrakech\": \"http://www.cim.ma\",\n    \"Attijariwafa Assurance\": \"https://www.attijariwafassurance.ma\",\n    \"Pharmatex\": \"https://www.pharmatex.ma\",\n    \"BMCE Bank of Africa\": \"https://www.bmcebank.ma\",\n    \"HemoTech\": \"http://www.hemotech.com\",",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "company_urls",
        "kind": 5,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "company_urls = {\n    \"Nareva Holding\": \"http://www.nareva.ma\",\n    \"Unilever Maroc\": \"https://www.unilever.com\",\n    \"Clinique Internationale de Marrakech\": \"http://www.cim.ma\",\n    \"Attijariwafa Assurance\": \"https://www.attijariwafassurance.ma\",\n    \"Pharmatex\": \"https://www.pharmatex.ma\",\n    \"BMCE Bank of Africa\": \"https://www.bmcebank.ma\",\n    \"HemoTech\": \"http://www.hemotech.com\",\n    \"Maroc Chimie\": \"http://www.marocchimie.ma\",\n    \"Groupe BDP International\": \"http://www.bdp-group.com\",",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "data['URL']",
        "kind": 5,
        "importPath": "notoyage_data.transfert6_url_maroc",
        "description": "notoyage_data.transfert6_url_maroc",
        "peekOfCode": "data['URL'] = data.apply(update_url, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_URL_mis_a_jour.xlsx', index=False)\nprint(\"La colonne URL a t mise  jour et sauvegarde dans le fichier 'fichier_reduit_avec_URL_mis_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert6_url_maroc",
        "documentation": {}
    },
    {
        "label": "generate_email",
        "kind": 2,
        "importPath": "notoyage_data.transfert7_mail",
        "description": "notoyage_data.transfert7_mail",
        "peekOfCode": "def generate_email(row):\n    # Extraire le prnom et le nom de la colonne 'Contact Person'\n    contact_person = row['Contact Person']\n    if contact_person:  # Si la personne de contact n'est pas vide\n        name_parts = contact_person.split()  # Sparer le prnom et le nom\n        first_name = name_parts[0].lower()  # Prnom en minuscules\n        last_name = name_parts[-1].lower()  # Nom en minuscules\n        # Extraire le nom de l'entreprise\n        company_name = row['Company'].replace(\" \", \"_\").lower()  # Remplacer les espaces par des underscores et mettre en minuscules\n        # Crer l'email",
        "detail": "notoyage_data.transfert7_mail",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert7_mail",
        "description": "notoyage_data.transfert7_mail",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_URL_mis_a_jour.xlsx')\n# Fonction pour gnrer l'email\ndef generate_email(row):\n    # Extraire le prnom et le nom de la colonne 'Contact Person'\n    contact_person = row['Contact Person']\n    if contact_person:  # Si la personne de contact n'est pas vide\n        name_parts = contact_person.split()  # Sparer le prnom et le nom\n        first_name = name_parts[0].lower()  # Prnom en minuscules\n        last_name = name_parts[-1].lower()  # Nom en minuscules\n        # Extraire le nom de l'entreprise",
        "detail": "notoyage_data.transfert7_mail",
        "documentation": {}
    },
    {
        "label": "data['email']",
        "kind": 5,
        "importPath": "notoyage_data.transfert7_mail",
        "description": "notoyage_data.transfert7_mail",
        "peekOfCode": "data['email'] = data.apply(generate_email, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_email.xlsx', index=False)\nprint(\"La colonne email a t ajoute et sauvegarde dans le fichier 'fichier_reduit_avec_email.xlsx'.\")",
        "detail": "notoyage_data.transfert7_mail",
        "documentation": {}
    },
    {
        "label": "generate_telephone",
        "kind": 2,
        "importPath": "notoyage_data.transfert8_telephone",
        "description": "notoyage_data.transfert8_telephone",
        "peekOfCode": "def generate_telephone(row):\n    contact_value = row['Contact']  # Rcuprer la valeur de la colonne Contact\n    if contact_value:\n        # Limiter  20 caractres\n        telephone_value = contact_value[:20]  # Prendre uniquement les 20 premiers caractres\n        return telephone_value\n    return \"\"  # Retourner une chane vide si le champ Contact est vide\n# Appliquer la fonction  chaque ligne du DataFrame pour crer la colonne 'telephone'\ndata['telephone'] = data.apply(generate_telephone, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel",
        "detail": "notoyage_data.transfert8_telephone",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert8_telephone",
        "description": "notoyage_data.transfert8_telephone",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_email.xlsx')\n# Fonction pour crer un tlphone  partir du Contact\ndef generate_telephone(row):\n    contact_value = row['Contact']  # Rcuprer la valeur de la colonne Contact\n    if contact_value:\n        # Limiter  20 caractres\n        telephone_value = contact_value[:20]  # Prendre uniquement les 20 premiers caractres\n        return telephone_value\n    return \"\"  # Retourner une chane vide si le champ Contact est vide\n# Appliquer la fonction  chaque ligne du DataFrame pour crer la colonne 'telephone'",
        "detail": "notoyage_data.transfert8_telephone",
        "documentation": {}
    },
    {
        "label": "data['telephone']",
        "kind": 5,
        "importPath": "notoyage_data.transfert8_telephone",
        "description": "notoyage_data.transfert8_telephone",
        "peekOfCode": "data['telephone'] = data.apply(generate_telephone, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_telephone.xlsx', index=False)\nprint(\"La colonne telephone a t ajoute et sauvegarde dans le fichier 'fichier_reduit_avec_telephone.xlsx'.\")",
        "detail": "notoyage_data.transfert8_telephone",
        "documentation": {}
    },
    {
        "label": "generate_moroccan_phone_number",
        "kind": 2,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "def generate_moroccan_phone_number():\n    # Gnrer un numro marocain au format +212 6XX XXX XXX\n    first_part = \"+212 6\"\n    second_part = str(random.randint(100, 999))  # Trois chiffres pour la seconde partie\n    third_part = str(random.randint(100, 999))  # Trois chiffres pour la troisime partie\n    fourth_part = str(random.randint(100, 999))  # Trois chiffres pour la quatrime partie\n    return f\"{first_part}{second_part} {third_part} {fourth_part}\"\n# Fonction pour mettre  jour la colonne telephone\ndef update_telephone(row):\n    contact_value = row['Contact']  # Rcuprer la valeur de la colonne Contact",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "update_telephone",
        "kind": 2,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "def update_telephone(row):\n    contact_value = row['Contact']  # Rcuprer la valeur de la colonne Contact\n    country_value = row['Country']  # Rcuprer la valeur de la colonne Country\n    if country_value == \"Morocco\":\n        # Si le pays est le Maroc, remplacer le tlphone par un numro marocain\n        return generate_moroccan_phone_number()\n    else:\n        # Si le pays n'est pas le Maroc, limiter la valeur  20 caractres\n        if contact_value:\n            return contact_value[:20]  # Prendre uniquement les 20 premiers caractres",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_telephone.xlsx')\n# Fonction pour gnrer un numro marocain fictif\ndef generate_moroccan_phone_number():\n    # Gnrer un numro marocain au format +212 6XX XXX XXX\n    first_part = \"+212 6\"\n    second_part = str(random.randint(100, 999))  # Trois chiffres pour la seconde partie\n    third_part = str(random.randint(100, 999))  # Trois chiffres pour la troisime partie\n    fourth_part = str(random.randint(100, 999))  # Trois chiffres pour la quatrime partie\n    return f\"{first_part}{second_part} {third_part} {fourth_part}\"\n# Fonction pour mettre  jour la colonne telephone",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "data['telephone']",
        "kind": 5,
        "importPath": "notoyage_data.transfert9_tele2",
        "description": "notoyage_data.transfert9_tele2",
        "peekOfCode": "data['telephone'] = data.apply(update_telephone, axis=1)\n# Sauvegarder le fichier modifi dans un nouveau fichier Excel\ndata.to_excel('fichier_reduit_avec_telephone_mise_a_jour.xlsx', index=False)\nprint(\n    \"La colonne 'telephone' a t mise  jour et sauvegarde dans le fichier 'fichier_reduit_avec_telephone_mise_a_jour.xlsx'.\")",
        "detail": "notoyage_data.transfert9_tele2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.type_trav2",
        "description": "notoyage_data.type_trav2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_secteur.xlsx')\nwork_type_df = pd.read_excel('type_trav_avec_id.xlsx')\n# Fusionner les deux DataFrames sur la colonne 'Work Type' pour ajouter 'id_type_trav'\nmerged_data = pd.merge(data, work_type_df, how='left', left_on='Work Type', right_on='nom_type_trav')\n# Sauvegarder le fichier mis  jour dans un nouveau fichier Excel\nmerged_data.to_excel('fichier_reduit_avec_id_type_trav.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(merged_data.head())",
        "detail": "notoyage_data.type_trav2",
        "documentation": {}
    },
    {
        "label": "work_type_df",
        "kind": 5,
        "importPath": "notoyage_data.type_trav2",
        "description": "notoyage_data.type_trav2",
        "peekOfCode": "work_type_df = pd.read_excel('type_trav_avec_id.xlsx')\n# Fusionner les deux DataFrames sur la colonne 'Work Type' pour ajouter 'id_type_trav'\nmerged_data = pd.merge(data, work_type_df, how='left', left_on='Work Type', right_on='nom_type_trav')\n# Sauvegarder le fichier mis  jour dans un nouveau fichier Excel\nmerged_data.to_excel('fichier_reduit_avec_id_type_trav.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(merged_data.head())",
        "detail": "notoyage_data.type_trav2",
        "documentation": {}
    },
    {
        "label": "merged_data",
        "kind": 5,
        "importPath": "notoyage_data.type_trav2",
        "description": "notoyage_data.type_trav2",
        "peekOfCode": "merged_data = pd.merge(data, work_type_df, how='left', left_on='Work Type', right_on='nom_type_trav')\n# Sauvegarder le fichier mis  jour dans un nouveau fichier Excel\nmerged_data.to_excel('fichier_reduit_avec_id_type_trav.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrifier\nprint(merged_data.head())",
        "detail": "notoyage_data.type_trav2",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "notoyage_data.ville2",
        "description": "notoyage_data.ville2",
        "peekOfCode": "data = pd.read_excel('fichier_reduit_avec_id_etat.xlsx')\n# Charger le fichier des villes avec id_ville et id_etat\ncities = pd.read_excel('villes_avec_id.xlsx')\n# Fusionner les donnes pour ajouter la colonne id_ville\n# en utilisant 'Location' et 'id_etat' comme cl de correspondance\nmerged_data = pd.merge(data, cities, how='left', left_on=['location', 'id_etat'], right_on=['nom_ville', 'id_etat'])\n# Supprimer la colonne 'nom_ville' utilise uniquement pour la correspondance\nmerged_data.drop(columns=['nom_ville'], inplace=True)\n# Sauvegarder le fichier final avec la nouvelle colonne 'id_ville'\nmerged_data.to_excel('fichier_reduit_avec_id_ville.xlsx', index=False)",
        "detail": "notoyage_data.ville2",
        "documentation": {}
    },
    {
        "label": "cities",
        "kind": 5,
        "importPath": "notoyage_data.ville2",
        "description": "notoyage_data.ville2",
        "peekOfCode": "cities = pd.read_excel('villes_avec_id.xlsx')\n# Fusionner les donnes pour ajouter la colonne id_ville\n# en utilisant 'Location' et 'id_etat' comme cl de correspondance\nmerged_data = pd.merge(data, cities, how='left', left_on=['location', 'id_etat'], right_on=['nom_ville', 'id_etat'])\n# Supprimer la colonne 'nom_ville' utilise uniquement pour la correspondance\nmerged_data.drop(columns=['nom_ville'], inplace=True)\n# Sauvegarder le fichier final avec la nouvelle colonne 'id_ville'\nmerged_data.to_excel('fichier_reduit_avec_id_ville.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrification\nprint(merged_data.head())",
        "detail": "notoyage_data.ville2",
        "documentation": {}
    },
    {
        "label": "merged_data",
        "kind": 5,
        "importPath": "notoyage_data.ville2",
        "description": "notoyage_data.ville2",
        "peekOfCode": "merged_data = pd.merge(data, cities, how='left', left_on=['location', 'id_etat'], right_on=['nom_ville', 'id_etat'])\n# Supprimer la colonne 'nom_ville' utilise uniquement pour la correspondance\nmerged_data.drop(columns=['nom_ville'], inplace=True)\n# Sauvegarder le fichier final avec la nouvelle colonne 'id_ville'\nmerged_data.to_excel('fichier_reduit_avec_id_ville.xlsx', index=False)\n# Afficher un aperu des 5 premires lignes pour vrification\nprint(merged_data.head())",
        "detail": "notoyage_data.ville2",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "offre_Process.data.config",
        "description": "offre_Process.data.config",
        "peekOfCode": "api_key = \"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\"\n# Required connection configs for Kafka producer, consumer, and admin\nserveur_kafka=\"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nusername=\"OM3FCB4RLKF3L2AQ\"\npassword=\"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\ntopic=\"offres_trav\"",
        "detail": "offre_Process.data.config",
        "documentation": {}
    },
    {
        "label": "envoyer_vers_kafka",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):\n    try:\n        # Configuration du producteur Kafka avec SASL_SSL\n        producer = KafkaProducer(\n            bootstrap_servers=[serveur_kafka],\n            value_serializer=lambda m: json.dumps(m).encode('utf-8'),\n            security_protocol=\"SASL_SSL\",\n            sasl_mechanism=\"PLAIN\",\n            sasl_plain_username=api_key,\n            sasl_plain_password=api_secret,",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "simulation_offres",
        "kind": 2,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "def simulation_offres(api_key, api_secret, serveur_kafka, topic):\n    exemples_cv = [\n        \"Candidat expert en Python et SQL, avec exprience dans le cloud AWS.\",\n        \"Candidat avec des comptences avances en Java, Spark, et Hadoop.\",\n        \"Ingnieur logiciel avec 5 ans d'exprience en C++, Docker, et Kubernetes.\",\n        \"tudiant en data science matrisant R, Tableau et machine learning.\",\n        \"Dveloppeur front-end avec expertise en JavaScript, React, et TypeScript.\"\n    ]\n    while True:\n        try:",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "compteur_offres",
        "kind": 5,
        "importPath": "offre_Process.data.producer",
        "description": "offre_Process.data.producer",
        "peekOfCode": "compteur_offres = 0\nserveur_kafka=cfg.serveur_kafka\ntopic=cfg.topic\napi_key=cfg.username\napi_secret=cfg.password\ndef envoyer_vers_kafka(serveur_kafka, topic, message, api_key, api_secret):\n    try:\n        # Configuration du producteur Kafka avec SASL_SSL\n        producer = KafkaProducer(\n            bootstrap_servers=[serveur_kafka],",
        "detail": "offre_Process.data.producer",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "def create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\n            \"spark.sql.catalog.spark_catalog\",\n            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "def create_empty_delta_table(spark: SparkSession, schema: StructType, path: str, partition_cols: Optional[List[str]] = None):\n    try:\n        DeltaTable.forPath(spark, path)\n        print(f\"Delta Table already exists at path: {path}\")\n    except Exception:\n        custom_builder = (\n            DeltaTable.createIfNotExists(spark)\n            .location(path)\n            .addColumns(schema)\n        )",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n    print(\"Data written to Delta Table\")\n# Initialiser Spark\nspark = create_or_get_spark(app_name=\"kafka_to_delta\", packages=PACKAGES)\n# Configurer la connexion Azure\nspark.conf.set(\n    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n    ADLS_ACCOUNT_KEY,\n)",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = os.getenv(\"ADLS_STORAGE_ACCOUNT_NAME\")\nADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\n# Configuration Kafka Consumer\nconsumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "ADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\n# Configuration Kafka Consumer\nconsumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "ADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\n# Configuration Kafka Consumer\nconsumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "ADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\n# Configuration Kafka Consumer\nconsumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\n# Configuration Kafka Consumer\nconsumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "KAFKA_GROUP_ID",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "KAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\n# Configuration Kafka Consumer\nconsumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\n# Configuration Spark",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "consumer = KafkaConsumer(\n    'offres_travail',\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True\n)\n# Configuration Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",\n]\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .config(\"spark.jars.packages\", jars)",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "offre_Process.spark.extractor",
        "description": "offre_Process.spark.extractor",
        "peekOfCode": "spark = create_or_get_spark(app_name=\"kafka_to_delta\", packages=PACKAGES)\n# Configurer la connexion Azure\nspark.conf.set(\n    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n    ADLS_ACCOUNT_KEY,\n)\nprint(\"Spark Session Created\")\n# Boucle principale pour lire et traiter les messages Kafka\nfor message in consumer:\n    try:",
        "detail": "offre_Process.spark.extractor",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Crer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n        .config(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\")\n        .master(\"local[*]\")",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    \"\"\"Crer une table Delta vide si elle n'existe pas.\"\"\"\n    try:\n        DeltaTable.forPath(spark, path)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    \"\"\"Sauvegarder les donnes dans une Delta Table.\"\"\"\n    df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n    print(\"Data written to Delta Table.\")\ndef process_message(spark: SparkSession, message: str):\n    \"\"\"Traiter un message Kafka et l'enregistrer dans une table Delta.\"\"\"\n    try:\n        job_posting = json.loads(message)\n        df = spark.createDataFrame([job_posting], schema=OFFRE_SCHEMA)\n        save_to_delta(df, OUTPUT_PATH)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "process_message",
        "kind": 2,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "def process_message(spark: SparkSession, message: str):\n    \"\"\"Traiter un message Kafka et l'enregistrer dans une table Delta.\"\"\"\n    try:\n        job_posting = json.loads(message)\n        df = spark.createDataFrame([job_posting], schema=OFFRE_SCHEMA)\n        save_to_delta(df, OUTPUT_PATH)\n    except Exception as e:\n        print(f\"Error processing message: {e}\")\n# Configurer le consommateur Kafka pour Confluent\nconsumer = KafkaConsumer(",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = os.getenv(\"ADLS_STORAGE_ACCOUNT_NAME\")\nADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_ACCOUNT_KEY = os.getenv(\"ADLS_ACCOUNT_KEY\")\nADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_CONTAINER_NAME = os.getenv(\"ADLS_CONTAINER_NAME\")\nADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "ADLS_FOLDER_PATH = os.getenv(\"ADLS_FOLDER_PATH\")\nKAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\")\nKAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_GROUP_ID",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_GROUP_ID = os.getenv(\"KAFKA_GROUP_ID\")\nKAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_API_KEY",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_API_KEY = os.getenv(\"KAFKA_API_KEY\")\nKAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_API_SECRET",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_API_SECRET = os.getenv(\"KAFKA_API_SECRET\")\nKAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "KAFKA_TOPIC",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "KAFKA_TOPIC = os.getenv(\"KAFKA_TOPIC\")\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Crer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "offre_Process.spark.sparkStreaming",
        "description": "offre_Process.spark.sparkStreaming",
        "peekOfCode": "consumer = KafkaConsumer(\n    KAFKA_TOPIC,\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_plain_username=KAFKA_API_KEY,\n    sasl_plain_password=KAFKA_API_SECRET,",
        "detail": "offre_Process.spark.sparkStreaming",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns\ndef get_average_word_vector(tokens, model):\n    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if not word_vectors:\n        return np.zeros(model.vector_size)\n    return np.mean(word_vectors, axis=0)",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "get_average_word_vector",
        "kind": 2,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "def get_average_word_vector(tokens, model):\n    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if not word_vectors:\n        return np.zeros(model.vector_size)\n    return np.mean(word_vectors, axis=0)\n# Fonction pour extraire du texte des fichiers PDF\ndef extract_text_from_pdfs(upload_folder):\n    resumes = []\n    for filename in os.listdir(upload_folder):\n        if filename.endswith('.pdf'):",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdfs",
        "kind": 2,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "def extract_text_from_pdfs(upload_folder):\n    resumes = []\n    for filename in os.listdir(upload_folder):\n        if filename.endswith('.pdf'):\n            filepath = os.path.join(upload_folder, filename)\n            doc = fitz.open(filepath)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n            resumes.append((filename, text))",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "def index():\n    if request.method == 'POST':\n        if 'resume_folder' in request.files:\n            # Traiter le tlchargement des fichiers PDF\n            resume_folder = request.files.getlist('resume_folder')\n            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n            for file in resume_folder:\n                file.save(os.path.join(app.config['UPLOAD_FOLDER'], file.filename))\n            # Extraire les textes des PDF\n            resumes = extract_text_from_pdfs(app.config['UPLOAD_FOLDER'])",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "job_description",
        "kind": 2,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "def job_description():\n    if request.method == 'POST':\n        job_description = request.form['job_description']\n        # Charger les CV  partir du fichier CSV\n        df = pd.read_csv('resumes.csv')\n        job_tokens = preprocess_text(job_description)\n        job_vector = get_average_word_vector(job_tokens, model)\n        similarity_list = []\n        for index, row in df.iterrows():\n            resume_tokens = preprocess_text(row['Resume_str'])",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "app = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = 'uploads/'\n# Charger le modle Word2Vec\nmodel = gensim.models.Word2Vec.load(\"D:/mmoire/resume_word2vec.model\")\n# Fonction de prtraitement\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "app.config['UPLOAD_FOLDER']",
        "kind": 5,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "app.config['UPLOAD_FOLDER'] = 'uploads/'\n# Charger le modle Word2Vec\nmodel = gensim.models.Word2Vec.load(\"D:/mmoire/resume_word2vec.model\")\n# Fonction de prtraitement\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns\ndef get_average_word_vector(tokens, model):",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Recruiter_App.app",
        "description": "Recruiter_App.app",
        "peekOfCode": "model = gensim.models.Word2Vec.load(\"D:/mmoire/resume_word2vec.model\")\n# Fonction de prtraitement\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns\ndef get_average_word_vector(tokens, model):\n    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if not word_vectors:",
        "detail": "Recruiter_App.app",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns\ndef get_average_word_vector(tokens, model):\n    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if not word_vectors:\n        return np.zeros(model.vector_size)\n    return np.mean(word_vectors, axis=0)",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "get_average_word_vector",
        "kind": 2,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "def get_average_word_vector(tokens, model):\n    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if not word_vectors:\n        return np.zeros(model.vector_size)\n    return np.mean(word_vectors, axis=0)\n# Fonction pour extraire du texte des fichiers PDF\ndef extract_text_from_pdfs(upload_folder):\n    resumes = []\n    for filename in os.listdir(upload_folder):\n        if filename.endswith('.pdf'):",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdfs",
        "kind": 2,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "def extract_text_from_pdfs(upload_folder):\n    resumes = []\n    for filename in os.listdir(upload_folder):\n        if filename.endswith('.pdf'):\n            filepath = os.path.join(upload_folder, filename)\n            doc = fitz.open(filepath)\n            text = \"\"\n            for page in doc:\n                text += page.get_text()\n            resumes.append((filename, text))",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "def index():\n    if request.method == 'POST':\n        if 'resume_folder' in request.files:\n            # Traiter le tlchargement des fichiers PDF\n            resume_folder = request.files.getlist('resume_folder')\n            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n            for file in resume_folder:\n                file.save(os.path.join(app.config['UPLOAD_FOLDER'], file.filename))\n            # Extraire les textes des PDF\n            resumes = extract_text_from_pdfs(app.config['UPLOAD_FOLDER'])",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "job_description",
        "kind": 2,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "def job_description():\n    if request.method == 'POST':\n        job_description = request.form['job_description']\n        # Charger les CV  partir du fichier CSV\n        df = pd.read_csv('resumes.csv')\n        job_tokens = preprocess_text(job_description)\n        job_vector = get_average_word_vector(job_tokens, model)\n        similarity_list = []\n        for index, row in df.iterrows():\n            resume_tokens = preprocess_text(row['Resume_str'])",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "app = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = 'uploads/'\n# Charger le modle Word2Vec\nmodel = gensim.models.Word2Vec.load(\"D:/mmoire/resume_word2vec.model\")\n# Fonction de prtraitement\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "app.config['UPLOAD_FOLDER']",
        "kind": 5,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "app.config['UPLOAD_FOLDER'] = 'uploads/'\n# Charger le modle Word2Vec\nmodel = gensim.models.Word2Vec.load(\"D:/mmoire/resume_word2vec.model\")\n# Fonction de prtraitement\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns\ndef get_average_word_vector(tokens, model):",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Recruiter_App.app1",
        "description": "Recruiter_App.app1",
        "peekOfCode": "model = gensim.models.Word2Vec.load(\"D:/mmoire/resume_word2vec.model\")\n# Fonction de prtraitement\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    return tokens\n# Fonction pour obtenir les embeddings moyenns\ndef get_average_word_vector(tokens, model):\n    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if not word_vectors:",
        "detail": "Recruiter_App.app1",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": "Recruiter_App.app2",
        "description": "Recruiter_App.app2",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    return text\n# Fonction pour tokeniser le texte\ndef tokenize_text(text):\n    tokens = word_tokenize(text.lower())\n    return tokens",
        "detail": "Recruiter_App.app2",
        "documentation": {}
    },
    {
        "label": "tokenize_text",
        "kind": 2,
        "importPath": "Recruiter_App.app2",
        "description": "Recruiter_App.app2",
        "peekOfCode": "def tokenize_text(text):\n    tokens = word_tokenize(text.lower())\n    return tokens\n# Fonction pour obtenir le vecteur moyen d'un document\ndef get_average_vector(model, tokens):\n    vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(model.vector_size)",
        "detail": "Recruiter_App.app2",
        "documentation": {}
    },
    {
        "label": "get_average_vector",
        "kind": 2,
        "importPath": "Recruiter_App.app2",
        "description": "Recruiter_App.app2",
        "peekOfCode": "def get_average_vector(model, tokens):\n    vectors = [model.wv[token] for token in tokens if token in model.wv]\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(model.vector_size)\n# Interface Streamlit\nst.title(\"Slection des Top 10 Candidats Base sur Word2Vec\")\n# Tlchargement de la description du poste\njob_description = st.text_area(\"Description de poste\", height=150)",
        "detail": "Recruiter_App.app2",
        "documentation": {}
    },
    {
        "label": "job_description",
        "kind": 5,
        "importPath": "Recruiter_App.app2",
        "description": "Recruiter_App.app2",
        "peekOfCode": "job_description = st.text_area(\"Description de poste\", height=150)\n# Tlchargement des CVs\nuploaded_files = st.file_uploader(\"Tlchargez les CVs au format PDF\", accept_multiple_files=True, type=\"pdf\")\nif uploaded_files and job_description:\n    st.write(f\"Nombre de CVs tlchargs : {len(uploaded_files)}\")\n    # Extraction et tokenisation de la description du poste\n    job_tokens = tokenize_text(job_description)\n    # Traitement des CVs\n    resume_texts = [extract_text_from_pdf(file) for file in uploaded_files]\n    resume_tokens = [tokenize_text(text) for text in resume_texts]",
        "detail": "Recruiter_App.app2",
        "documentation": {}
    },
    {
        "label": "uploaded_files",
        "kind": 5,
        "importPath": "Recruiter_App.app2",
        "description": "Recruiter_App.app2",
        "peekOfCode": "uploaded_files = st.file_uploader(\"Tlchargez les CVs au format PDF\", accept_multiple_files=True, type=\"pdf\")\nif uploaded_files and job_description:\n    st.write(f\"Nombre de CVs tlchargs : {len(uploaded_files)}\")\n    # Extraction et tokenisation de la description du poste\n    job_tokens = tokenize_text(job_description)\n    # Traitement des CVs\n    resume_texts = [extract_text_from_pdf(file) for file in uploaded_files]\n    resume_tokens = [tokenize_text(text) for text in resume_texts]\n    # Entranement du modle Word2Vec sur l'ensemble des textes\n    all_tokens = job_tokens + [token for tokens in resume_tokens for token in tokens]",
        "detail": "Recruiter_App.app2",
        "documentation": {}
    },
    {
        "label": "get_pdf_text",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.converteur",
        "description": "Recruitment_chatBot.recruitboot.converteur",
        "peekOfCode": "def get_pdf_text(pdf_docs):\n    text = \"\"\n    for pdf in pdf_docs:\n        # Vrifiez si le chemin du fichier PDF est valide\n        if not os.path.exists(pdf):\n            print(f\"Le fichier {pdf} n'existe pas.\")\n            continue  # Passez au fichier suivant si celui-ci n'existe pas\n        try:\n            pdf_reader = PdfReader(pdf)\n            # Parcourir toutes les pages du PDF et extraire le texte",
        "detail": "Recruitment_chatBot.recruitboot.converteur",
        "documentation": {}
    },
    {
        "label": "get_text_chunks",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.ingest",
        "description": "Recruitment_chatBot.recruitboot.ingest",
        "peekOfCode": "def get_text_chunks(text):\n    text_splitter = CharacterTextSplitter(\n        separator=\"\\n\",\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    chunks = text_splitter.split_text(text)\n    return chunks\ndef get_vectorstore(text_chunks):",
        "detail": "Recruitment_chatBot.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "get_vectorstore",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.ingest",
        "description": "Recruitment_chatBot.recruitboot.ingest",
        "peekOfCode": "def get_vectorstore(text_chunks):\n    # Charger les embeddings Hugging Face\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    # Crer le vecteur FAISS  partir des textes et des embeddings\n    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n    return vectorstore",
        "detail": "Recruitment_chatBot.recruitboot.ingest",
        "documentation": {}
    },
    {
        "label": "create_conversation_chain",
        "kind": 2,
        "importPath": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "description": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "peekOfCode": "def create_conversation_chain(vectorstore):\n    if not isinstance(vectorstore, FAISS):\n        raise TypeError(\"Le vectorstore doit tre un objet de type FAISS.\")\n    # Initialisation du LLM avec HuggingFaceHub\n    llm = HuggingFaceHub(\n        repo_id=\"google/flan-t5-large\",\n        huggingfacehub_api_token=\"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\",\n        model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n    )\n    # Configuration de la mmoire pour conserver l'historique des conversations",
        "detail": "Recruitment_chatBot.recruitboot.retrieval_generation",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "def index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel  invoke() pour obtenir la rponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 2,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "def chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel  invoke() pour obtenir la rponse\n        result = conversation_chain.invoke({\"question\": input})\n        print(\"Response: \", result)\n        # Retourner uniquement la rponse textuelle sans l'objet complet\n        answer = result[\"answer\"] if \"answer\" in result else \"Dsol, je n'ai pas pu trouver une rponse.\"\n        return jsonify({\"response\": answer})  # Retourner la rponse du chatbot",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "app = Flask(__name__)\nload_dotenv()\n# Fonction de gnration de la chane conversationnelle (pour intgrer LangChain)\n# Traitement du PDF\npdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Cration de la chane de conversation\nconversation_chain = create_conversation_chain(vectorstore)",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "pdf_docs",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "pdf_docs = [r\"C:\\Users\\HP\\OneDrive\\Desktop\\bigData_project\\E-Commerce-Chatbot\\data\\hamza.pdf\"]\ntext = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Cration de la chane de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "text = get_pdf_text(pdf_docs)\nchunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Cration de la chane de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "chunks = get_text_chunks(text)\nvectorstore = get_vectorstore(chunks)\n# Cration de la chane de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "vectorstore = get_vectorstore(chunks)\n# Cration de la chane de conversation\nconversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "conversation_chain",
        "kind": 5,
        "importPath": "Recruitment_chatBot.app",
        "description": "Recruitment_chatBot.app",
        "peekOfCode": "conversation_chain = create_conversation_chain(vectorstore)\n@app.route(\"/\")\ndef index():\n    return render_template('chat.html')\n@app.route(\"/get\", methods=[\"POST\"])\ndef chat():\n    msg = request.form[\"msg\"]\n    input = msg  # Le message utilisateur\n    try:\n        # Appel  invoke() pour obtenir la rponse",
        "detail": "Recruitment_chatBot.app",
        "documentation": {}
    },
    {
        "label": "CandidatDataLoader",
        "kind": 6,
        "importPath": "segmentationSpark.reader_data",
        "description": "segmentationSpark.reader_data",
        "peekOfCode": "class CandidatDataLoader:\n    def __init__(self, jdbc_url=\"jdbc:postgresql://localhost:5433/cond_db\"):\n        \"\"\"\n        Initialise la classe pour charger les donnes depuis PostgreSQL.\n        :param jdbc_url: URL de connexion JDBC  la base de donnes PostgreSQL.\n        \"\"\"\n        self.jdbc_url = jdbc_url\n        self.query = \"\"\"\n        SELECT \n            c.full_name AS candidate_name,",
        "detail": "segmentationSpark.reader_data",
        "documentation": {}
    },
    {
        "label": "CandidatSegmentation",
        "kind": 6,
        "importPath": "segmentationSpark.segmentation",
        "description": "segmentationSpark.segmentation",
        "peekOfCode": "class CandidatSegmentation:\n    def __init__(self, df, k=3, seed=42):\n        \"\"\"\n        Initialise la classe pour la segmentation des candidats.\n        :param df: DataFrame Spark contenant les donnes transformes.\n        :param k: Nombre de clusters pour KMeans.\n        :param seed: Seed pour la reproductibilit.\n        \"\"\"\n        self.df = df\n        self.k = k",
        "detail": "segmentationSpark.segmentation",
        "documentation": {}
    },
    {
        "label": "CandidatTransformer",
        "kind": 6,
        "importPath": "segmentationSpark.transformer",
        "description": "segmentationSpark.transformer",
        "peekOfCode": "class CandidatTransformer:\n    def __init__(self, df):\n        \"\"\"\n        Initialise la classe avec un DataFrame Spark.\n        :param df: DataFrame Spark contenant les donnes.\n        \"\"\"\n        self.df = df\n    def build_pipeline(self):\n        \"\"\"\n        Construit un pipeline pour transformer les donnes.",
        "detail": "segmentationSpark.transformer",
        "documentation": {}
    },
    {
        "label": "batch_vectors",
        "kind": 2,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "def batch_vectors(vectors, batch_size):\n    \"\"\"\n    Divise une liste de vecteurs en lots plus petits.\n    \"\"\"\n    for i in range(0, len(vectors), batch_size):\n        yield vectors[i:i + batch_size]\n# Taille maximale des lots (ajustez si ncessaire)\nbatch_size = 100  # Taille de lot initiale, adapte  vos besoins et  vos tests.\n# --- Insrer les offres ---\noffre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "account",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "account = \"phztxrc-go36107\"\nuser = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "user",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "user = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "password",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "password = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "role",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "role = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "warehouse",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "warehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "database",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "database = \"my_project_database\"\nschema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "schema",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "schema = \"my_project_schema\"\n# Connexion  Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "conn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema\n)\n# Requte pour les offres",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "query_offres",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "query_offres = \"\"\"\n SELECT o.offre_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        o.experience_dur\n FROM my_project_database.my_project_schema.offre_fait o\n JOIN my_project_database.my_project_schema.offre_comp oc ON o.offre_id = oc.offre_id\n JOIN my_project_database.my_project_schema.competence cmp ON oc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_offr lo ON o.offre_id = lo.offre_id",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "query_candidats",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "query_candidats = \"\"\"\n SELECT c.candidat_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        c.nbr_years_exp\n FROM my_project_database.my_project_schema.candidat_fait c\n LEFT JOIN my_project_database.my_project_schema.candidat_comp cc ON c.candidat_id = cc.candidat_id\n LEFT JOIN my_project_database.my_project_schema.competence cmp ON cc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_cand lc ON c.candidat_id = lc.candidat_id",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n# Trier les comptences et les langues pour chaque offre\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les comptences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les comptences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n# Trier les comptences et les langues pour chaque offre\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les comptences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les comptences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les comptences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les comptences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Trier les comptences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les comptences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les comptences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les comptences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['FORMATION']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['FORMATION']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n# Supprimer les doublons dans les comptences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Gnrer des embeddings pour les offres",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Gnrer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Gnrer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['COMPETENCES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Gnrer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['LANGUES']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n# Gnrer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['description']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Gnrer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "embeddings_offres",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "embeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Gnrer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vrifier le rsultat",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_offres['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Gnrer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vrifier le rsultat\nprint(df_offres.head())\nprint(df_candidats.head())",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "#df_offres['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n# Gnrer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vrifier le rsultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['description']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vrifier le rsultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# tape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "embeddings_candidats",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "embeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vrifier le rsultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# tape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "df_candidats['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "df_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vrifier le rsultat\nprint(df_offres.head())\nprint(df_candidats.head())\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# tape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone\n# Crez une instance de Pinecone avec votre cl API\napi_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "#df_candidats['embedding']",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# tape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone\n# Crez une instance de Pinecone avec votre cl API\napi_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"\npc = pinecone.Pinecone(api_key=api_key)\n# Vrifiez si l'index existe dj\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "api_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"\npc = pinecone.Pinecone(api_key=api_key)\n# Vrifiez si l'index existe dj\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:\n        # Crez l'index si ncessaire\n        pc.create_index(\n            name='jobcandidates',\n            dimension=384,  # Adaptez la dimension selon le modle utilis\n            metric='cosine',  # Vous pouvez choisir la mtrique approprie",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "pc",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "pc = pinecone.Pinecone(api_key=api_key)\n# Vrifiez si l'index existe dj\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:\n        # Crez l'index si ncessaire\n        pc.create_index(\n            name='jobcandidates',\n            dimension=384,  # Adaptez la dimension selon le modle utilis\n            metric='cosine',  # Vous pouvez choisir la mtrique approprie\n            spec=ServerlessSpec(",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "index = pc.Index(\"jobcandidates\")\ndef batch_vectors(vectors, batch_size):\n    \"\"\"\n    Divise une liste de vecteurs en lots plus petits.\n    \"\"\"\n    for i in range(0, len(vectors), batch_size):\n        yield vectors[i:i + batch_size]\n# Taille maximale des lots (ajustez si ncessaire)\nbatch_size = 100  # Taille de lot initiale, adapte  vos besoins et  vos tests.\n# --- Insrer les offres ---",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "batch_size = 100  # Taille de lot initiale, adapte  vos besoins et  vos tests.\n# --- Insrer les offres ---\noffre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]\n# Vrifiez la dimension des vecteurs\nfor vector_id, vector_values in offre_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n# Insrer les vecteurs en lots\nfor batch in batch_vectors(offre_vectors, batch_size):",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "offre_vectors",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "offre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]\n# Vrifiez la dimension des vecteurs\nfor vector_id, vector_values in offre_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n# Insrer les vecteurs en lots\nfor batch in batch_vectors(offre_vectors, batch_size):\n    try:\n        index.upsert(vectors=batch)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "candidat_vectors",
        "kind": 5,
        "importPath": "snowflake_to_pinecone.snowflake_to_pinecone",
        "description": "snowflake_to_pinecone.snowflake_to_pinecone",
        "peekOfCode": "candidat_vectors = [(str(row.CANDIDAT_ID), row.embedding) for _, row in df_candidats.iterrows()]\n# Vrifiez la dimension des vecteurs\nfor vector_id, vector_values in candidat_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n# Insrer les vecteurs en lots\nfor batch in batch_vectors(candidat_vectors, batch_size):\n    try:\n        index.upsert(vectors=batch)",
        "detail": "snowflake_to_pinecone.snowflake_to_pinecone",
        "documentation": {}
    },
    {
        "label": "generer_offre_emploi",
        "kind": 2,
        "importPath": "generator_offre",
        "description": "generator_offre",
        "peekOfCode": "def generer_offre_emploi(api_key, prompt, resume_data):\n    try:\n        from groq import Groq  # Import du client Groq\n        groq_client = Groq(api_key=api_key)\n        # Messages pour le modle\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"Tu es un assistant charg de gnrer des offres d'emploi bases sur un modle structur.\"\n            },",
        "detail": "generator_offre",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "generator_offre",
        "description": "generator_offre",
        "peekOfCode": "api_key = \"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\"\nresume_data = \"Candidat expert en Python et SQL, avec exprience dans le cloud AWS.\"\nprompt = \"Gnrer une offre d'emploi sous format texte structur.\"\noffre = generer_offre_emploi(api_key, prompt, resume_data)\nprint(offre)",
        "detail": "generator_offre",
        "documentation": {}
    },
    {
        "label": "resume_data",
        "kind": 5,
        "importPath": "generator_offre",
        "description": "generator_offre",
        "peekOfCode": "resume_data = \"Candidat expert en Python et SQL, avec exprience dans le cloud AWS.\"\nprompt = \"Gnrer une offre d'emploi sous format texte structur.\"\noffre = generer_offre_emploi(api_key, prompt, resume_data)\nprint(offre)",
        "detail": "generator_offre",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "generator_offre",
        "description": "generator_offre",
        "peekOfCode": "prompt = \"Gnrer une offre d'emploi sous format texte structur.\"\noffre = generer_offre_emploi(api_key, prompt, resume_data)\nprint(offre)",
        "detail": "generator_offre",
        "documentation": {}
    },
    {
        "label": "offre",
        "kind": 5,
        "importPath": "generator_offre",
        "description": "generator_offre",
        "peekOfCode": "offre = generer_offre_emploi(api_key, prompt, resume_data)\nprint(offre)",
        "detail": "generator_offre",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Crer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n        .config(\"fs.abfss.impl\", \"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\")\n        .master(\"local[*]\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    \"\"\"Crer une table Delta vide si elle n'existe pas.\"\"\"\n    try:\n        DeltaTable.forPath(spark, path)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    \"\"\"Sauvegarder les donnes dans une Delta Table.\"\"\"\n    df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n    print(\"Data written to Delta Table.\")\ndef process_message(spark: SparkSession, message: str):\n    \"\"\"Traiter un message Kafka et l'enregistrer dans une table Delta.\"\"\"\n    try:\n        job_posting = json.loads(message)\n        df = spark.createDataFrame([job_posting], schema=OFFRE_SCHEMA)\n        save_to_delta(df, OUTPUT_PATH)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "process_message",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def process_message(spark: SparkSession, message: str):\n    \"\"\"Traiter un message Kafka et l'enregistrer dans une table Delta.\"\"\"\n    try:\n        job_posting = json.loads(message)\n        df = spark.createDataFrame([job_posting], schema=OFFRE_SCHEMA)\n        save_to_delta(df, OUTPUT_PATH)\n    except Exception as e:\n        print(f\"Error processing message: {e}\")\n# Configurer le consommateur Kafka pour Confluent\nconsumer = KafkaConsumer(",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "OFFRE_SCHEMA",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "OFFRE_SCHEMA = StructType([\n    StructField(\"titre_du_poste\", StringType(), True),\n    StructField(\"societe\", StringType(), True),\n    StructField(\"competences\", ArrayType(StringType()), True),\n    StructField(\"lieu\", StringType(), True),\n    StructField(\"type_offre\", StringType(), True),\n    StructField(\"dure\", StringType(), True),\n    StructField(\"type_de_contrat\", StringType(), True),\n    StructField(\"email\", StringType(), True),\n    StructField(\"telephone\", StringType(), True),",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "GROQ_API_KEY",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "GROQ_API_KEY = \"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\"\nADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # A remplacer par votre cl\nADLS_CONTAINER_NAME = \"postes\"\nADLS_FOLDER_PATH = \"offres_trav\"\nKAFKA_BOOTSTRAP_SERVERS = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nKAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # A remplacer par votre cl\nADLS_CONTAINER_NAME = \"postes\"\nADLS_FOLDER_PATH = \"offres_trav\"\nKAFKA_BOOTSTRAP_SERVERS = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nKAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "ADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # A remplacer par votre cl\nADLS_CONTAINER_NAME = \"postes\"\nADLS_FOLDER_PATH = \"offres_trav\"\nKAFKA_BOOTSTRAP_SERVERS = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nKAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "ADLS_CONTAINER_NAME = \"postes\"\nADLS_FOLDER_PATH = \"offres_trav\"\nKAFKA_BOOTSTRAP_SERVERS = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nKAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "ADLS_FOLDER_PATH = \"offres_trav\"\nKAFKA_BOOTSTRAP_SERVERS = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nKAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\nKAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "KAFKA_GROUP_ID",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "KAFKA_GROUP_ID = \"groupe_traitement\"\nKAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "KAFKA_API_KEY",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "KAFKA_API_KEY = \"OM3FCB4RLKF3L2AQ\"\nKAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "KAFKA_API_SECRET",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "KAFKA_API_SECRET = \"TaIh3NYZANuLKfatv3dHcQLFaigVQvIdG+uY9Sma/eFIPzMXCWvdojhc6Q1+/BWK\"\nKAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "KAFKA_TOPIC",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "KAFKA_TOPIC = \"offres_trav\"\nOUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Crer une session Spark avec Delta et Azure.\"\"\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "OUTPUT_PATH = f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{ADLS_FOLDER_PATH}\"\n# Packages requis pour Spark\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Crer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n]\ndef create_or_get_spark(app_name: str, packages: List[str]) -> SparkSession:\n    \"\"\"Crer une session Spark avec Delta et Azure.\"\"\"\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "consumer",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "consumer = KafkaConsumer(\n    KAFKA_TOPIC,\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    group_id=KAFKA_GROUP_ID,\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_plain_username=KAFKA_API_KEY,\n    sasl_plain_password=KAFKA_API_SECRET,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYSPARK_PYTHON\"]",
        "kind": 5,
        "importPath": "seyp",
        "description": "seyp",
        "peekOfCode": "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\nfrom pyspark import SparkConf, SparkContext\n# Crer une configuration Spark\nconf = SparkConf().setAppName(\"WordCount\").setMaster(\"spark://localhost:7077\")  # Set the correct Spark master URL\nsc = SparkContext(conf=conf)\n# Vous pouvez maintenant utiliser sc pour excuter votre travail Spark\n# Crer un RDD avec une liste de mots\ndata = sc.parallelize([\"Spark\", \"Scala\",\"Scala\", \"Hadoop\", \"Big Data\", \"Spark\"])\n# Compter le nombre d'occurrences de chaque mot\nword_counts = data.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)",
        "detail": "seyp",
        "documentation": {}
    },
    {
        "label": "conf",
        "kind": 5,
        "importPath": "seyp",
        "description": "seyp",
        "peekOfCode": "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"spark://localhost:7077\")  # Set the correct Spark master URL\nsc = SparkContext(conf=conf)\n# Vous pouvez maintenant utiliser sc pour excuter votre travail Spark\n# Crer un RDD avec une liste de mots\ndata = sc.parallelize([\"Spark\", \"Scala\",\"Scala\", \"Hadoop\", \"Big Data\", \"Spark\"])\n# Compter le nombre d'occurrences de chaque mot\nword_counts = data.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n# Afficher les rsultats\nfor word, count in word_counts.collect():\n    print(f\"{word}: {count}\")",
        "detail": "seyp",
        "documentation": {}
    },
    {
        "label": "sc",
        "kind": 5,
        "importPath": "seyp",
        "description": "seyp",
        "peekOfCode": "sc = SparkContext(conf=conf)\n# Vous pouvez maintenant utiliser sc pour excuter votre travail Spark\n# Crer un RDD avec une liste de mots\ndata = sc.parallelize([\"Spark\", \"Scala\",\"Scala\", \"Hadoop\", \"Big Data\", \"Spark\"])\n# Compter le nombre d'occurrences de chaque mot\nword_counts = data.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n# Afficher les rsultats\nfor word, count in word_counts.collect():\n    print(f\"{word}: {count}\")\n# Arrter le contexte Spark",
        "detail": "seyp",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "seyp",
        "description": "seyp",
        "peekOfCode": "data = sc.parallelize([\"Spark\", \"Scala\",\"Scala\", \"Hadoop\", \"Big Data\", \"Spark\"])\n# Compter le nombre d'occurrences de chaque mot\nword_counts = data.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n# Afficher les rsultats\nfor word, count in word_counts.collect():\n    print(f\"{word}: {count}\")\n# Arrter le contexte Spark\nsc.stop()",
        "detail": "seyp",
        "documentation": {}
    },
    {
        "label": "word_counts",
        "kind": 5,
        "importPath": "seyp",
        "description": "seyp",
        "peekOfCode": "word_counts = data.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n# Afficher les rsultats\nfor word, count in word_counts.collect():\n    print(f\"{word}: {count}\")\n# Arrter le contexte Spark\nsc.stop()",
        "detail": "seyp",
        "documentation": {}
    },
    {
        "label": "create_or_get_spark",
        "kind": 2,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "def create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)\n    spark = (\n        SparkSession.builder.appName(app_name)\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .config(\"spark.jars.packages\", jars)\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "create_empty_delta_table",
        "kind": 2,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "def create_empty_delta_table(\n    spark: SparkSession,\n    schema: StructType,\n    path: str,\n    partition_cols: Optional[List[str]] = None,\n    enable_cdc: Optional[bool] = False,\n):\n    # Vrifier si la table Delta existe\n    try:\n        delta_table = DeltaTable.forPath(spark, path)",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "save_to_delta",
        "kind": 2,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "def save_to_delta(df: DataFrame, output_path: str):\n    # Sauvegarder les donnes traites dans la table Delta avec la fusion de schmas\n    df.write.format(\"delta\") \\\n        .mode(\"append\") \\\n        .option(\"mergeSchema\", \"true\") \\\n        .save(output_path)\n    print(\"Data written to Delta Table\")\n# ===================================================================================\n#                           MAIN ENTRYPOINT\n# ===================================================================================",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PYSPARK_PIN_THREAD\"]",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n# Dfinir le schma pour le JSON\nPERSON_SCHEMA = StructType([\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "PERSON_SCHEMA",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "PERSON_SCHEMA = StructType([\n    StructField(\"last_name\", StringType(), True),\n    StructField(\"full_name\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"address\", StructType([\n        StructField(\"formatted_location\", StringType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"region\", StringType(), True),\n        StructField(\"country\", StringType(), True),\n        StructField(\"postal_code\", StringType(), True)",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_STORAGE_ACCOUNT_NAME",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_STORAGE_ACCOUNT_NAME = \"dataoffre\"\nADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_ACCOUNT_KEY",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_ACCOUNT_KEY = \"1eNXm2As1DuaMeSt2Yjegn22fFCvIUa8nBhknayEyTgfBZb6xEEyZhnvl9OiGT7U4O7cFrygjBE/+ASt1hkNQQ==\"  # Add your ADLS account key here  # Add your ADLS account key here\nADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_CONTAINER_NAME",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_CONTAINER_NAME = \"offres\"\nADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "ADLS_FOLDER_PATH",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "ADLS_FOLDER_PATH = \"offres_trav\"\nOUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "OUTPUT_PATH = (\n    f\"abfss://{ADLS_CONTAINER_NAME}@{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n    + ADLS_FOLDER_PATH\n)\n# Required Spark packages\nPACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "PACKAGES",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "PACKAGES = [\n    \"io.delta:delta-spark_2.12:3.0.0\",\n    \"org.apache.hadoop:hadoop-azure:3.3.6\",\n    \"org.apache.hadoop:hadoop-azure-datalake:3.3.6\",\n    \"org.apache.hadoop:hadoop-common:3.3.6\",\n]\ndef create_or_get_spark(\n    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n) -> SparkSession:\n    jars = \",\".join(packages)",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "spark = create_or_get_spark(\n    app_name=\"json_to_delta\", packages=PACKAGES, cluster_manager=\"local[*]\"\n)\n# Configurer la connexion  Azure\nspark.conf.set(\n    f\"fs.azure.account.key.{ADLS_STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n    ADLS_ACCOUNT_KEY,\n)\nprint(\"Spark Session Created\")\n# Lire les donnes JSON depuis la variable",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "spark_TO_delta(inspiratio)",
        "description": "spark_TO_delta(inspiratio)",
        "peekOfCode": "df = spark.read.json(spark.sparkContext.parallelize([data]), schema=PERSON_SCHEMA)\nprint(\"JSON data loaded\")\n# Traiter les donnes\nprint(\"Data processed\")\n# Crer une table Delta vide (si elle n'existe pas encore)\ncreate_empty_delta_table(\n    spark=spark,\n    schema=PERSON_SCHEMA,\n    path=OUTPUT_PATH,\n    partition_cols=[\"first_name\"],",
        "detail": "spark_TO_delta(inspiratio)",
        "documentation": {}
    }
]