{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:18:58.079076Z","iopub.execute_input":"2024-12-12T20:18:58.079632Z","iopub.status.idle":"2024-12-12T20:18:59.078342Z","shell.execute_reply.started":"2024-12-12T20:18:58.079586Z","shell.execute_reply":"2024-12-12T20:18:59.077455Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install numpy==1.26\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:19:03.592140Z","iopub.execute_input":"2024-12-12T20:19:03.592677Z","iopub.status.idle":"2024-12-12T20:19:17.010219Z","shell.execute_reply.started":"2024-12-12T20:19:03.592645Z","shell.execute_reply":"2024-12-12T20:19:17.009357Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.26\n  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\ndask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\npylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\npylibraft 24.10.0 requires nvidia-cublas, which is not installed.\npylibraft 24.10.0 requires nvidia-curand, which is not installed.\npylibraft 24.10.0 requires nvidia-cusolver, which is not installed.\npylibraft 24.10.0 requires nvidia-cusparse, which is not installed.\nucxx 0.40.0 requires libucxx==0.40.*, which is not installed.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.0 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\nblis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.0 which is incompatible.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nrmm 24.10.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\nxarray 2024.11.0 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\npip install sentence-transformers==3.3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:19:24.015046Z","iopub.execute_input":"2024-12-12T20:19:24.015784Z","iopub.status.idle":"2024-12-12T20:19:32.886334Z","shell.execute_reply.started":"2024-12-12T20:19:24.015743Z","shell.execute_reply":"2024-12-12T20:19:32.885295Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers==3.3\n  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (1.26.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.3) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.3) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.3) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers==3.3) (1.3.0)\nDownloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install transformers==4.47\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:19:40.710381Z","iopub.execute_input":"2024-12-12T20:19:40.710766Z","iopub.status.idle":"2024-12-12T20:19:59.753266Z","shell.execute_reply.started":"2024-12-12T20:19:40.710732Z","shell.execute_reply":"2024-12-12T20:19:59.752154Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.47\n  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (1.26.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers==4.47)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.47) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (2024.6.2)\nDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install tensorflow==2.15\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:20:03.209917Z","iopub.execute_input":"2024-12-12T20:20:03.210749Z","iopub.status.idle":"2024-12-12T20:21:06.048953Z","shell.execute_reply.started":"2024-12-12T20:20:03.210718Z","shell.execute_reply":"2024-12-12T20:21:06.047437Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.15\n  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (18.1.1)\nCollecting ml-dtypes~=0.2.0 (from tensorflow==2.15)\n  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.26.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (4.12.2)\nCollecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15)\n  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.37.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.62.2)\nCollecting tensorboard<2.16,>=2.15 (from tensorflow==2.15)\n  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.43.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.30.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.6)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15) (3.1.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\nDownloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wrapt, ml-dtypes, keras, tensorboard, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.16.0\n    Uninstalling wrapt-1.16.0:\n      Successfully uninstalled wrapt-1.16.0\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.3.2\n    Uninstalling ml-dtypes-0.3.2:\n      Successfully uninstalled ml-dtypes-0.3.2\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.6.0 which is incompatible.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\ntensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 wrapt-1.14.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install pinecone-client\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:21:16.454843Z","iopub.execute_input":"2024-12-12T20:21:16.455235Z","iopub.status.idle":"2024-12-12T20:21:25.642468Z","shell.execute_reply.started":"2024-12-12T20:21:16.455200Z","shell.execute_reply":"2024-12-12T20:21:25.641390Z"}},"outputs":[{"name":"stdout","text":"Collecting pinecone-client\n  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (2024.6.2)\nCollecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (1.26.18)\nDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nInstalling collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\nSuccessfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install snowflake-connector-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:21:31.331935Z","iopub.execute_input":"2024-12-12T20:21:31.332667Z","iopub.status.idle":"2024-12-12T20:21:41.012650Z","shell.execute_reply.started":"2024-12-12T20:21:31.332617Z","shell.execute_reply":"2024-12-12T20:21:41.011586Z"}},"outputs":[{"name":"stdout","text":"Collecting snowflake-connector-python\n  Downloading snowflake_connector_python-3.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (1.16.0)\nRequirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (42.0.8)\nRequirement already satisfied: pyOpenSSL<25.0.0,>=22.0.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (24.0.0)\nRequirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2.8.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2024.1)\nRequirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2.32.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (21.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2024.6.2)\nRequirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (4.12.2)\nRequirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.15.1)\nRequirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2.4.0)\nRequirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.11.0)\nRequirement already satisfied: tomlkit in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (0.13.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0->snowflake-connector-python) (1.26.18)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->snowflake-connector-python) (3.1.2)\nDownloading snowflake_connector_python-3.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: asn1crypto, snowflake-connector-python\nSuccessfully installed asn1crypto-1.5.1 snowflake-connector-python-3.12.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import gc\nimport torch\n\n# Forcez le nettoyage de la mémoire GPU\ntorch.cuda.empty_cache()\n\n# Collecte de la mémoire inutilisée\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:21:58.335546Z","iopub.execute_input":"2024-12-12T20:21:58.335923Z","iopub.status.idle":"2024-12-12T20:22:01.437738Z","shell.execute_reply.started":"2024-12-12T20:21:58.335890Z","shell.execute_reply":"2024-12-12T20:22:01.436966Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Extraire les données de Snowflake\nimport snowflake.connector\nimport pandas as pd\n\n# Détails de connexion à Snowflake\naccount = \"phztxrc-go36107\"\nuser = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema\n)\n\n# Requête pour les offres\nquery_offres = \"\"\"\n SELECT o.offre_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        o.experience_dur\n FROM my_project_database.my_project_schema.offre_fait o\n JOIN my_project_database.my_project_schema.offre_comp oc ON o.offre_id = oc.offre_id\n JOIN my_project_database.my_project_schema.competence cmp ON oc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_offr lo ON o.offre_id = lo.offre_id\n LEFT JOIN my_project_database.my_project_schema.langue lng ON lo.langue_id = lng.langue_id\n LEFT JOIN my_project_database.my_project_schema.formation f ON o.formation_id = f.formation_id\n GROUP BY o.offre_id, f.nom, o.experience_dur\n\"\"\"\n\n# Requête pour les candidats\nquery_candidats = \"\"\"\n SELECT c.candidat_id ,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        c.nbr_years_exp\n FROM my_project_database.my_project_schema.candidat_fait c\n LEFT JOIN my_project_database.my_project_schema.candidat_comp cc ON c.candidat_id = cc.candidat_id\n LEFT JOIN my_project_database.my_project_schema.competence cmp ON cc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_cand lc ON c.candidat_id = lc.candidat_id\n LEFT JOIN my_project_database.my_project_schema.langue lng ON lc.langue_id = lng.langue_id\n LEFT JOIN my_project_database.my_project_schema.formation f ON c.formation_id = f.formation_id\n GROUP BY c.candidat_id, f.nom, c.nbr_years_exp\n\"\"\"\n\n# Exécution des requêtes et transfert des résultats dans des DataFrames\ntry:\n    df_offres = pd.read_sql(query_offres, conn)\n    print(\"Données des offres chargées avec succès.\")\n    print(df_offres.head())  # Affiche un aperçu des premières lignes\n    print(df_offres.columns)\n\n    df_candidats = pd.read_sql(query_candidats, conn)\n    print(\"Données des candidats chargées avec succès.\")\n    print(df_candidats.head())  # Affiche un aperçu des premières lignes\n    df_candidats = df_candidats.sample(frac=0.5, random_state=42)\n    print('2',df_candidats.head())\n    print(df_candidats.columns)\n\nexcept Exception as e:\n    print(\"Erreur lors de l'exécution des requêtes :\", e)\nfinally:\n    # Fermeture de la connexion\n    conn.close()\n    print(\"Connexion à Snowflake fermée.\")\n\n# from sqlalchemy import create_engine\n# import pandas as pd\n#\n# # Chaîne de connexion SQLAlchemy\n# engine = create_engine(\n#     'snowflake://SAID:SaidKHALID2002!@phztxrc-go36107.snowflakecomputing.com/my_project_database/my_project_schema?role=dev_role&warehouse=projet_warehouse'\n# )\n#\n# # Établir une connexion explicite à partir de l'objet engine\n# with engine.connect() as connection:\n#     # Charger les données des offres\n#     query_offres = \"\"\"\n#     SELECT o.offre_id,\n#            LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n#            LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n#            f.nom AS formation,\n#            o.experience_dur\n#     FROM my_project_database.my_project_schema.offre_fait o\n#     JOIN my_project_database.my_project_schema.offre_comp oc ON o.offre_id = oc.offre_id\n#     JOIN my_project_database.my_project_schema.competence cmp ON oc.competence_id = cmp.competence_id\n#     LEFT JOIN my_project_database.my_project_schema.langue_offr lo ON o.offre_id = lo.offre_id\n#     LEFT JOIN my_project_database.my_project_schema.langue lng ON lo.langue_id = lng.langue_id\n#     LEFT JOIN my_project_database.my_project_schema.formation f ON o.formation_id = f.formation_id\n#     GROUP BY o.offre_id, f.nom, o.experience_dur\n#     \"\"\"\n#     df_offres = pd.read_sql(query_offres, connection)\n#\n#     # Charger les données des candidats\n#     query_candidats = \"\"\"\n#     SELECT c.candidat_id,\n#            LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n#            LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n#            f.nom AS formation,\n#            c.nbr_years_exp\n#     FROM my_project_database.my_project_schema.candidat_fait c\n#     LEFT JOIN my_project_database.my_project_schema.candidat_comp cc ON c.candidat_id = cc.candidat_id\n#     LEFT JOIN my_project_database.my_project_schema.competence cmp ON cc.competence_id = cmp.competence_id\n#     LEFT JOIN my_project_database.my_project_schema.langue_cand lc ON c.candidat_id = lc.candidat_id\n#     LEFT JOIN my_project_database.my_project_schema.langue lng ON lc.langue_id = lng.langue_id\n#     LEFT JOIN my_project_database.my_project_schema.formation f ON c.formation_id = f.formation_id\n#     GROUP BY c.candidat_id, f.nom, c.nbr_years_exp\n#     \"\"\"\n#     df_candidats = pd.read_sql(query_candidats, connection)\n#\n# # Afficher les DataFrames pour vérification\n# print(\"Offres :\")\n# print(df_offres.head())\n# print(\"\\nCandidats :\")\n# print(df_candidats.head())\n\n# Étape 2 : Générer des embeddings pour les offres et les candidats\nfrom sentence_transformers import SentenceTransformer\n\n\n# Charger le modèle avec support GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n# Trier les compétences et les langues pour chaque offre\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n\n# Trier les compétences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\n\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n\n\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n\n\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n\n# Générer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n\n# Générer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())\n\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Étape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone\n# Créez une instance de Pinecone avec votre clé API\napi_key = \"pcsk_vUaKS_3PK35kGth5rcmSKZkihFFuaS7B44xzMycHCnot1s9Czf1WE8iXSPZg4nDph81Ak\"\n\npc = pinecone.Pinecone(api_key=api_key)\n# Vérifiez si l'index existe déjà\nif 'job' not in pc.list_indexes().names():\n    try:\n        # Créez l'index si nécessaire\n        pc.create_index(\n            name='job',\n            dimension=384,  # Adaptez la dimension selon le modèle utilisé\n            metric='cosine',  # Vous pouvez choisir la métrique appropriée\n            spec=ServerlessSpec(\n                cloud='aws',  # Spécifiez le fournisseur de cloud\n                region='us-east-1'  # Essayez une autre région AWS\n            )\n        )\n        print(\"Index 'job' créé avec succès.\")\n    except PineconeApiException as e:\n        print(f\"Erreur lors de la création de l'index : {e}\")\nelse:\n    print(\"L'index 'job' existe déjà.\")\n\n# Vérifiez si l'index existe déjà\nif 'candidates' not in pc.list_indexes().names():\n    try:\n        # Créez l'index si nécessaire\n        pc.create_index(\n            name='candidates',\n            dimension=384,  # Adaptez la dimension selon le modèle utilisé\n            metric='cosine',  # Vous pouvez choisir la métrique appropriée\n            spec=ServerlessSpec(\n                cloud='aws',  # Spécifiez le fournisseur de cloud\n                region='us-east-1'  # Essayez une autre région AWS\n            )\n        )\n        print(\"Index 'candidates' créé avec succès.\")\n    except PineconeApiException as e:\n        print(f\"Erreur lors de la création de l'index : {e}\")\nelse:\n    print(\"L'index 'candidates' existe déjà.\")\n\n# Accédez à l'index\nindex_job = pc.Index(\"job\")\nindex_candidates = pc.Index(\"candidates\")\n\n\ndef batch_vectors(vectors, batch_size):\n    \"\"\"\n    Divise une liste de vecteurs en lots plus petits.\n    \"\"\"\n    for i in range(0, len(vectors), batch_size):\n        yield vectors[i:i + batch_size]\n\n# Taille maximale des lots (ajustez si nécessaire)\nbatch_size = 100  # Taille de lot initiale, adaptée à vos besoins et à vos tests.\n\n# --- Insérer les offres ---\noffre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]\n\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in offre_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(offre_vectors, batch_size):\n    try:\n        index_job.upsert(vectors=batch)\n    except pinecone.PineconeApiException as e:\n        print(f\"Erreur lors de l'insertion d'un lot d'offres : {e}\")\n\n# --- Insérer les candidats ---\ncandidat_vectors = [(str(row.CANDIDAT_ID), row.embedding) for _, row in df_candidats.iterrows()]\n\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in candidat_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(candidat_vectors, batch_size):\n    try:\n        index_candidates.upsert(vectors=batch)\n    except pinecone.PineconeApiException as e:\n        print(f\"Erreur lors de l'insertion d'un lot de candidats : {e}\")\n\nprint(\"Tous les embeddings ont été insérés dans Pinecone.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:22:53.396660Z","iopub.execute_input":"2024-12-12T20:22:53.397151Z","iopub.status.idle":"2024-12-12T21:27:37.590521Z","shell.execute_reply.started":"2024-12-12T20:22:53.397118Z","shell.execute_reply":"2024-12-12T21:27:37.589655Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1493679725.py:59: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df_offres = pd.read_sql(query_offres, conn)\n","output_type":"stream"},{"name":"stdout","text":"Données des offres chargées avec succès.\n           OFFRE_ID                                        COMPETENCES  \\\n0  1684338730094059           Structural analysis, AutoCAD proficiency   \n1  2509368638876743  Multimedia design, Curriculum development, Lea...   \n2  1317753417770859  Tax treaties and agreements, Cross-border tax ...   \n3  1340864434062859  Strategic planning and execution, Budgeting an...   \n4  2781263026180916  Schema markup, XML sitemaps, Website crawl ana...   \n\n  LANGUES         FORMATION EXPERIENCE_DUR  \n0  French           Finance  2 to 13 Years  \n1  French  Computer Science   2 to 9 Years  \n2  French           Tourism  5 to 15 Years  \n3  French         Marketing  2 to 15 Years  \n4  French          Agronomy  0 to 10 Years  \nIndex(['OFFRE_ID', 'COMPETENCES', 'LANGUES', 'FORMATION', 'EXPERIENCE_DUR'], dtype='object')\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1493679725.py:64: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df_candidats = pd.read_sql(query_candidats, conn)\n","output_type":"stream"},{"name":"stdout","text":"Données des candidats chargées avec succès.\n   CANDIDAT_ID                                        COMPETENCES  \\\n0      1680367   Social Research Methods, Anthropology, Sociology   \n1      1680618                           Sociology, Social Policy   \n2      1681676  Social Research Methods, Sociology, Community ...   \n3      1679140  Political Science, Anthropology, Social Policy...   \n4      1678676  Social Research Methods, Social Policy, Cultur...   \n\n                            LANGUES        FORMATION  NBR_YEARS_EXP  \n0           Arabic, Français, stage  Social Sciences             14  \n1   Français, Arabic, French, stage  Social Sciences              4  \n2                           English  Social Sciences             14  \n3  stage, English, Français, Arabic  Social Sciences              0  \n4  stage, Français, English, Arabic  Social Sciences             19  \n2          CANDIDAT_ID                                        COMPETENCES  \\\n554339        636169                          Control Systems, Robotics   \n1602454        14343                                       Data Science   \n561830       1846887  Search Engine Optimization (SEO), Data Analyti...   \n691565       1008283                                    Web Development   \n1497105      1499013       Supply Chain Management, Nutritional Science   \n\n                                 LANGUES               FORMATION  \\\n554339                            French  Mechanical Engineering   \n1602454                           French                    ensa   \n561830   Français, stage, Arabic, French       Digital Marketing   \n691565                   English, French              Multimedia   \n1497105          Arabic, stage, Français               Agri-food   \n\n         NBR_YEARS_EXP  \n554339               8  \n1602454             16  \n561830               9  \n691565              19  \n1497105             15  \nIndex(['CANDIDAT_ID', 'COMPETENCES', 'LANGUES', 'FORMATION', 'NBR_YEARS_EXP'], dtype='object')\nConnexion à Snowflake fermée.\n","output_type":"stream"},{"name":"stderr","text":"2024-12-12 20:23:22.432169: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-12 20:23:22.432249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-12 20:23:22.434027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a17bb60681649d9bd8ba9e39cad88d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b426a4da48d546fb88d02c34fb5c8543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c9d399e3724831bc39efc590684a83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7486fa923a4f7bbbe89c4eb0dc6272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039b6b50feb84b37ad0ea1fb08aba550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb33e5227abe4c2a9a951276b48c93f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa564ced26b45698b7b1827e29f21f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fdeb7e7f71840af887dea560bb8ad7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6eb4e209f1a46378ddaf2ad2de81cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad7e0712e3e64e12b4165462aa31ba94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f16c75f6f924b09b2b740cfd149ffd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8180faf468164e03a517cafc20cc4a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32639 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125c10b20b44412680f0407082769cae"}},"metadata":{}},{"name":"stdout","text":"           OFFRE_ID                                        COMPETENCES  \\\n0  1684338730094059           autocad proficiency, structural analysis   \n1  2509368638876743  curriculum development, elearning tools, learn...   \n2  1317753417770859  client relationship management, communication ...   \n3  1340864434062859  budgeting and financial analysis, leadership a...   \n4  2781263026180916  schema markup, website crawl analysis, xml sit...   \n\n  LANGUES         FORMATION EXPERIENCE_DUR  \\\n0  french           finance  2 to 13 Years   \n1  french  computer science   2 to 9 Years   \n2  french           tourism  5 to 15 Years   \n3  french         marketing  2 to 15 Years   \n4  french          agronomy  0 to 10 Years   \n\n                                         description  \\\n0  autocad proficiency, structural analysis, fren...   \n1  curriculum development, elearning tools, learn...   \n2  client relationship management, communication ...   \n3  budgeting and financial analysis, leadership a...   \n4  schema markup, website crawl analysis, xml sit...   \n\n                                           embedding  \n0  [0.018703944981098175, -0.035757407546043396, ...  \n1  [0.06362176686525345, -0.06815682351589203, 0....  \n2  [0.026105279102921486, -0.05329383164644241, 0...  \n3  [0.04067850112915039, 0.0003607260005082935, -...  \n4  [-0.015540128573775291, 0.036959242075681686, ...  \n         CANDIDAT_ID                                        COMPETENCES  \\\n554339        636169                          control systems, robotics   \n1602454        14343                                       data science   \n561830       1846887  brand management, data analytics, email market...   \n691565       1008283                                    web development   \n1497105      1499013       nutritional science, supply chain management   \n\n                                 LANGUES               FORMATION  \\\n554339                            french  mechanical engineering   \n1602454                           french                    ensa   \n561830   arabic, français, french, stage       digital marketing   \n691565                   english, french              multimedia   \n1497105          arabic, français, stage               agri-food   \n\n         NBR_YEARS_EXP                                        description  \\\n554339               8  control systems, robotics, french, mechanical ...   \n1602454             16                     data science, french, ensa, 16   \n561830               9  brand management, data analytics, email market...   \n691565              19   web development, english, french, multimedia, 19   \n1497105             15  nutritional science, supply chain management, ...   \n\n                                                 embedding  \n554339   [-0.05619995296001434, -0.11029313504695892, 0...  \n1602454  [-0.03004864975810051, -0.03702419996261597, 0...  \n561830   [0.0032004083041101694, -0.0545366145670414, -...  \n691565   [0.0031996991019695997, -0.12822626531124115, ...  \n1497105  [-0.006709852255880833, -0.03647216409444809, ...  \nIndex 'job' créé avec succès.\nIndex 'candidates' créé avec succès.\nTous les embeddings ont été insérés dans Pinecone.\n","output_type":"stream"}],"execution_count":9}]}