{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:47:26.322582Z","iopub.execute_input":"2024-12-09T20:47:26.323018Z","iopub.status.idle":"2024-12-09T20:47:26.642040Z","shell.execute_reply.started":"2024-12-09T20:47:26.322974Z","shell.execute_reply":"2024-12-09T20:47:26.641413Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install numpy==1.26\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:47:32.044093Z","iopub.execute_input":"2024-12-09T20:47:32.045191Z","iopub.status.idle":"2024-12-09T20:47:44.924241Z","shell.execute_reply.started":"2024-12-09T20:47:32.045152Z","shell.execute_reply":"2024-12-09T20:47:44.923420Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.26\n  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\ndask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\npylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\npylibraft 24.10.0 requires nvidia-cublas, which is not installed.\npylibraft 24.10.0 requires nvidia-curand, which is not installed.\npylibraft 24.10.0 requires nvidia-cusolver, which is not installed.\npylibraft 24.10.0 requires nvidia-cusparse, which is not installed.\nucxx 0.40.0 requires libucxx==0.40.*, which is not installed.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.0 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\nblis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.0 which is incompatible.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nrmm 24.10.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\nxarray 2024.11.0 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\npip install sentence-transformers==3.3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:47:55.309249Z","iopub.execute_input":"2024-12-09T20:47:55.310232Z","iopub.status.idle":"2024-12-09T20:48:03.894129Z","shell.execute_reply.started":"2024-12-09T20:47:55.310177Z","shell.execute_reply":"2024-12-09T20:48:03.892996Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers==3.3\n  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==3.3) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==3.3) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.3) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (1.26.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==3.3) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.3) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.3) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.3) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers==3.3) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers==3.3) (1.3.0)\nDownloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install transformers==4.47\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:48:12.905913Z","iopub.execute_input":"2024-12-09T20:48:12.906255Z","iopub.status.idle":"2024-12-09T20:48:31.855573Z","shell.execute_reply.started":"2024-12-09T20:48:12.906225Z","shell.execute_reply":"2024-12-09T20:48:31.854638Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.47\n  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (1.26.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers==4.47)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.47) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47) (2024.6.2)\nDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install tensorflow==2.15\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:48:35.512470Z","iopub.execute_input":"2024-12-09T20:48:35.512825Z","iopub.status.idle":"2024-12-09T20:49:35.035241Z","shell.execute_reply.started":"2024-12-09T20:48:35.512794Z","shell.execute_reply":"2024-12-09T20:49:35.034064Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.15\n  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (18.1.1)\nCollecting ml-dtypes~=0.2.0 (from tensorflow==2.15)\n  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.26.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (4.12.2)\nCollecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15)\n  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (0.37.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (1.62.2)\nCollecting tensorboard<2.16,>=2.15 (from tensorflow==2.15)\n  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.43.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.30.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.6)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15) (3.1.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\nDownloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wrapt, ml-dtypes, keras, tensorboard, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.16.0\n    Uninstalling wrapt-1.16.0:\n      Successfully uninstalled wrapt-1.16.0\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.3.2\n    Uninstalling ml-dtypes-0.3.2:\n      Successfully uninstalled ml-dtypes-0.3.2\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.6.0 which is incompatible.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\ntensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 wrapt-1.14.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install pinecone-client\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:49:45.267637Z","iopub.execute_input":"2024-12-09T20:49:45.267982Z","iopub.status.idle":"2024-12-09T20:49:55.079301Z","shell.execute_reply.started":"2024-12-09T20:49:45.267950Z","shell.execute_reply":"2024-12-09T20:49:55.078262Z"}},"outputs":[{"name":"stdout","text":"Collecting pinecone-client\n  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (2024.6.2)\nCollecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (1.26.18)\nDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nInstalling collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\nSuccessfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install snowflake-connector-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:50:02.955075Z","iopub.execute_input":"2024-12-09T20:50:02.955898Z","iopub.status.idle":"2024-12-09T20:50:12.230617Z","shell.execute_reply.started":"2024-12-09T20:50:02.955860Z","shell.execute_reply":"2024-12-09T20:50:12.229526Z"}},"outputs":[{"name":"stdout","text":"Collecting snowflake-connector-python\n  Downloading snowflake_connector_python-3.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (1.16.0)\nRequirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (42.0.8)\nRequirement already satisfied: pyOpenSSL<25.0.0,>=22.0.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (24.0.0)\nRequirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2.8.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2024.1)\nRequirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2.32.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (21.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2024.6.2)\nRequirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (4.12.2)\nRequirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.15.1)\nRequirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (2.4.0)\nRequirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (3.11.0)\nRequirement already satisfied: tomlkit in /opt/conda/lib/python3.10/site-packages (from snowflake-connector-python) (0.13.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0->snowflake-connector-python) (1.26.18)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->snowflake-connector-python) (3.1.2)\nDownloading snowflake_connector_python-3.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: asn1crypto, snowflake-connector-python\nSuccessfully installed asn1crypto-1.5.1 snowflake-connector-python-3.12.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Doit retourner True si GPU CUDA est disponible\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:14:59.356206Z","iopub.execute_input":"2024-12-09T20:14:59.356584Z","iopub.status.idle":"2024-12-09T20:15:02.440089Z","shell.execute_reply.started":"2024-12-09T20:14:59.356549Z","shell.execute_reply":"2024-12-09T20:15:02.439180Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import gc\nimport torch\n\n# Forcez le nettoyage de la mémoire GPU\ntorch.cuda.empty_cache()\n\n# Collecte de la mémoire inutilisée\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:50:45.221022Z","iopub.execute_input":"2024-12-09T20:50:45.221399Z","iopub.status.idle":"2024-12-09T20:50:48.225433Z","shell.execute_reply.started":"2024-12-09T20:50:45.221339Z","shell.execute_reply":"2024-12-09T20:50:48.224615Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Extraire les données de Snowflake\nimport snowflake.connector\nimport pandas as pd\n\n# Détails de connexion à Snowflake\naccount = \"phztxrc-go36107\"\nuser = \"SAID\"\npassword = \"SaidKHALID2002!\"\nrole = \"dev_role\"\nwarehouse = \"projet_warehouse\"\ndatabase = \"my_project_database\"\nschema = \"my_project_schema\"\n\n# Connexion à Snowflake\nconn = snowflake.connector.connect(\n    account=account,\n    user=user,\n    password=password,\n    role=role,\n    warehouse=warehouse,\n    database=database,\n    schema=schema\n)\n\n# Requête pour les offres\nquery_offres = \"\"\"\n SELECT o.offre_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        o.experience_dur\n FROM my_project_database.my_project_schema.offre_fait o\n JOIN my_project_database.my_project_schema.offre_comp oc ON o.offre_id = oc.offre_id\n JOIN my_project_database.my_project_schema.competence cmp ON oc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_offr lo ON o.offre_id = lo.offre_id\n LEFT JOIN my_project_database.my_project_schema.langue lng ON lo.langue_id = lng.langue_id\n LEFT JOIN my_project_database.my_project_schema.formation f ON o.formation_id = f.formation_id\n GROUP BY o.offre_id, f.nom, o.experience_dur\n\"\"\"\n\n# Requête pour les candidats\nquery_candidats = \"\"\"\n SELECT c.candidat_id,\n        LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n        LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n        f.nom AS formation,\n        c.nbr_years_exp\n FROM my_project_database.my_project_schema.candidat_fait c\n LEFT JOIN my_project_database.my_project_schema.candidat_comp cc ON c.candidat_id = cc.candidat_id\n LEFT JOIN my_project_database.my_project_schema.competence cmp ON cc.competence_id = cmp.competence_id\n LEFT JOIN my_project_database.my_project_schema.langue_cand lc ON c.candidat_id = lc.candidat_id\n LEFT JOIN my_project_database.my_project_schema.langue lng ON lc.langue_id = lng.langue_id\n LEFT JOIN my_project_database.my_project_schema.formation f ON c.formation_id = f.formation_id\n GROUP BY c.candidat_id, f.nom, c.nbr_years_exp\n\"\"\"\n\n# Exécution des requêtes et transfert des résultats dans des DataFrames\ntry:\n    df_offres = pd.read_sql(query_offres, conn)\n    print(\"Données des offres chargées avec succès.\")\n    print(df_offres.head())  # Affiche un aperçu des premières lignes\n    print(df_offres.columns)\n\n    df_candidats = pd.read_sql(query_candidats, conn)\n    print(\"Données des candidats chargées avec succès.\")\n    print(df_candidats.head())  # Affiche un aperçu des premières lignes\n    df_candidats = df_candidats.sample(frac=0.5, random_state=42)\n    print('2',df_candidats.head())\n    print(df_candidats.columns)\n\nexcept Exception as e:\n    print(\"Erreur lors de l'exécution des requêtes :\", e)\nfinally:\n    # Fermeture de la connexion\n    conn.close()\n    print(\"Connexion à Snowflake fermée.\")\n\n# from sqlalchemy import create_engine\n# import pandas as pd\n#\n# # Chaîne de connexion SQLAlchemy\n# engine = create_engine(\n#     'snowflake://SAID:SaidKHALID2002!@phztxrc-go36107.snowflakecomputing.com/my_project_database/my_project_schema?role=dev_role&warehouse=projet_warehouse'\n# )\n#\n# # Établir une connexion explicite à partir de l'objet engine\n# with engine.connect() as connection:\n#     # Charger les données des offres\n#     query_offres = \"\"\"\n#     SELECT o.offre_id,\n#            LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n#            LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n#            f.nom AS formation,\n#            o.experience_dur\n#     FROM my_project_database.my_project_schema.offre_fait o\n#     JOIN my_project_database.my_project_schema.offre_comp oc ON o.offre_id = oc.offre_id\n#     JOIN my_project_database.my_project_schema.competence cmp ON oc.competence_id = cmp.competence_id\n#     LEFT JOIN my_project_database.my_project_schema.langue_offr lo ON o.offre_id = lo.offre_id\n#     LEFT JOIN my_project_database.my_project_schema.langue lng ON lo.langue_id = lng.langue_id\n#     LEFT JOIN my_project_database.my_project_schema.formation f ON o.formation_id = f.formation_id\n#     GROUP BY o.offre_id, f.nom, o.experience_dur\n#     \"\"\"\n#     df_offres = pd.read_sql(query_offres, connection)\n#\n#     # Charger les données des candidats\n#     query_candidats = \"\"\"\n#     SELECT c.candidat_id,\n#            LISTAGG(DISTINCT cmp.nom, ', ') AS competences,\n#            LISTAGG(DISTINCT lng.nom, ', ') AS langues,\n#            f.nom AS formation,\n#            c.nbr_years_exp\n#     FROM my_project_database.my_project_schema.candidat_fait c\n#     LEFT JOIN my_project_database.my_project_schema.candidat_comp cc ON c.candidat_id = cc.candidat_id\n#     LEFT JOIN my_project_database.my_project_schema.competence cmp ON cc.competence_id = cmp.competence_id\n#     LEFT JOIN my_project_database.my_project_schema.langue_cand lc ON c.candidat_id = lc.candidat_id\n#     LEFT JOIN my_project_database.my_project_schema.langue lng ON lc.langue_id = lng.langue_id\n#     LEFT JOIN my_project_database.my_project_schema.formation f ON c.formation_id = f.formation_id\n#     GROUP BY c.candidat_id, f.nom, c.nbr_years_exp\n#     \"\"\"\n#     df_candidats = pd.read_sql(query_candidats, connection)\n#\n# # Afficher les DataFrames pour vérification\n# print(\"Offres :\")\n# print(df_offres.head())\n# print(\"\\nCandidats :\")\n# print(df_candidats.head())\n\n# Étape 2 : Générer des embeddings pour les offres et les candidats\nfrom sentence_transformers import SentenceTransformer\n\n\n# Charger le modèle avec support GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n# Trier les compétences et les langues pour chaque offre\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n\n# Trier les compétences et les langues pour chaque candidat\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(x.split(', '))) if pd.notnull(x) else '')\n# Uniformiser la casse pour les compétences, langues, formation, etc.\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].str.lower()\ndf_offres['LANGUES'] = df_offres['LANGUES'].str.lower()\ndf_offres['FORMATION'] = df_offres['FORMATION'].str.lower()\n\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].str.lower()\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].str.lower()\ndf_candidats['FORMATION'] = df_candidats['FORMATION'].str.lower()\n\n\n# Supprimer les doublons dans les compétences et langues\ndf_offres['COMPETENCES'] = df_offres['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_offres['LANGUES'] = df_offres['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n\ndf_candidats['COMPETENCES'] = df_candidats['COMPETENCES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\ndf_candidats['LANGUES'] = df_candidats['LANGUES'].apply(lambda x: ', '.join(sorted(set(x.split(', ')))) if x else '')\n\n\n# Remplacer les valeurs manquantes\ndf_offres.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'EXPERIENCE_DUR': 0}, inplace=True)\ndf_candidats.fillna({'COMPETENCES': '', 'LANGUES': '', 'FORMATION': '', 'NBR_YEARS_EXP': 0}, inplace=True)\n\n# Générer des embeddings pour les offres\ndf_offres['description'] = df_offres['COMPETENCES'] + \", \" + df_offres['LANGUES'] + \", \" + df_offres['FORMATION'] + \", \" + df_offres['EXPERIENCE_DUR'].astype(str)\nembeddings_offres = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_offres['embedding'] = [embedding.tolist() for embedding in embeddings_offres]\n\n#df_offres['embedding'] = model.encode(df_offres['description'].tolist(), show_progress_bar=True)\n\n# Générer des embeddings pour les candidats\ndf_candidats['description'] = df_candidats['COMPETENCES'] + \", \" + df_candidats['LANGUES'] + \", \" + df_candidats['FORMATION'] + \", \" + df_candidats['NBR_YEARS_EXP'].astype(str)\nembeddings_candidats = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n\n# Assurez-vous que les embeddings sont bien sous forme de liste de tableaux 1D\ndf_candidats['embedding'] = [embedding.tolist() for embedding in embeddings_candidats]\n# Vérifier le résultat\nprint(df_offres.head())\nprint(df_candidats.head())\n\n#df_candidats['embedding'] = model.encode(df_candidats['description'].tolist(), show_progress_bar=True)\n# Étape 3 : Charger les embeddings dans Pinecone\nfrom pinecone import Pinecone, ServerlessSpec, PineconeApiException\nimport pinecone\n# Créez une instance de Pinecone avec votre clé API\napi_key = \"pcsk_Bek39_GoSxAyBEKDYpgNTADG9gjATrshAMneRiTtY9cEET1LvmsH4mCqyYoYxazS1cPp2\"\n\npc = pinecone.Pinecone(api_key=api_key)\n# Vérifiez si l'index existe déjà\nif 'jobcandidates' not in pc.list_indexes().names():\n    try:\n        # Créez l'index si nécessaire\n        pc.create_index(\n            name='jobcandidates',\n            dimension=384,  # Adaptez la dimension selon le modèle utilisé\n            metric='cosine',  # Vous pouvez choisir la métrique appropriée\n            spec=ServerlessSpec(\n                cloud='aws',  # Spécifiez le fournisseur de cloud\n                region='us-east-1'  # Essayez une autre région AWS\n            )\n        )\n        print(\"Index 'job_candidates' créé avec succès.\")\n    except PineconeApiException as e:\n        print(f\"Erreur lors de la création de l'index : {e}\")\nelse:\n    print(\"L'index 'job_candidates' existe déjà.\")\n\n\n# Accédez à l'index\nindex = pc.Index(\"jobcandidates\")\n\n\ndef batch_vectors(vectors, batch_size):\n    \"\"\"\n    Divise une liste de vecteurs en lots plus petits.\n    \"\"\"\n    for i in range(0, len(vectors), batch_size):\n        yield vectors[i:i + batch_size]\n\n# Taille maximale des lots (ajustez si nécessaire)\nbatch_size = 100  # Taille de lot initiale, adaptée à vos besoins et à vos tests.\n\n# --- Insérer les offres ---\noffre_vectors = [(str(row.OFFRE_ID), row.embedding) for _, row in df_offres.iterrows()]\n\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in offre_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(offre_vectors, batch_size):\n    try:\n        index.upsert(vectors=batch)\n    except pinecone.PineconeApiException as e:\n        print(f\"Erreur lors de l'insertion d'un lot d'offres : {e}\")\n\n# --- Insérer les candidats ---\ncandidat_vectors = [(str(row.CANDIDAT_ID), row.embedding) for _, row in df_candidats.iterrows()]\n\n# Vérifiez la dimension des vecteurs\nfor vector_id, vector_values in candidat_vectors:\n    if len(vector_values) != 384:\n        print(f\"Erreur : le vecteur avec l'ID {vector_id} a une dimension incorrecte ({len(vector_values)})\")\n        break\n\n# Insérer les vecteurs en lots\nfor batch in batch_vectors(candidat_vectors, batch_size):\n    try:\n        index.upsert(vectors=batch)\n    except pinecone.PineconeApiException as e:\n        print(f\"Erreur lors de l'insertion d'un lot de candidats : {e}\")\n\nprint(\"Tous les embeddings ont été insérés dans Pinecone.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:50:55.201358Z","iopub.execute_input":"2024-12-09T20:50:55.201823Z","iopub.status.idle":"2024-12-09T21:52:33.226211Z","shell.execute_reply.started":"2024-12-09T20:50:55.201794Z","shell.execute_reply":"2024-12-09T21:52:33.225408Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3123254283.py:59: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df_offres = pd.read_sql(query_offres, conn)\n","output_type":"stream"},{"name":"stdout","text":"Données des offres chargées avec succès.\n           OFFRE_ID                                        COMPETENCES  \\\n0   992312330492282  Electronic health records (EHR), Medication ad...   \n1  2725484753519878                        Negotiation, Sales strategy   \n2  2781263026180916  Schema markup, XML sitemaps, Website crawl ana...   \n3   546201581084317  Art techniques, Lesson planning, Classroom man...   \n4   985132019340003                    Contract negotiation, Budgeting   \n\n   LANGUES           FORMATION EXPERIENCE_DUR  \n0   French            Agronomy  1 to 10 Years  \n1   French  Energy Engineering  4 to 14 Years  \n2   French            Agronomy  0 to 10 Years  \n3   French           Aerospace  0 to 13 Years  \n4  English            Agronomy  2 to 14 Years  \nIndex(['OFFRE_ID', 'COMPETENCES', 'LANGUES', 'FORMATION', 'EXPERIENCE_DUR'], dtype='object')\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/3123254283.py:64: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df_candidats = pd.read_sql(query_candidats, conn)\n","output_type":"stream"},{"name":"stdout","text":"Données des candidats chargées avec succès.\n   CANDIDAT_ID                                        COMPETENCES  \\\n0      1677495  Sociology, Social Policy, Social Research Methods   \n1      1677568                   Political Science, Social Policy   \n2      1677825        Political Science, Social Policy, Sociology   \n3      1678730  Social Policy, Political Science, Social Resea...   \n4      1683359                            Sociology, Anthropology   \n\n                            LANGUES        FORMATION  NBR_YEARS_EXP  \n0                   English, French  Social Sciences              6  \n1                            French  Social Sciences             14  \n2  stage, Français, Arabic, English  Social Sciences             17  \n3           Français, stage, Arabic  Social Sciences              5  \n4                           English  Social Sciences              4  \n2          CANDIDAT_ID                                        COMPETENCES  \\\n554339       1220519  Environmental Impact Assessment, Petroleum Geo...   \n1602454        35828  Data Science, C++, Network Security, Cybersecu...   \n561830        842236                                       Broadcasting   \n691565       1747672  Supply Chain Management, Quality Engineering, ...   \n1497105       540497  Logistics Strategy, Procurement, Transportatio...   \n\n                                 LANGUES               FORMATION  \\\n554339                           English   Petroleum Engineering   \n1602454                 English, Spanish        Computer Science   \n561830                   French, English             Audiovisual   \n691565   stage, French, Français, Arabic  Industrial Engineering   \n1497105                           French               Logistics   \n\n         NBR_YEARS_EXP  \n554339              16  \n1602454              7  \n561830              14  \n691565              15  \n1497105              0  \nIndex(['CANDIDAT_ID', 'COMPETENCES', 'LANGUES', 'FORMATION', 'NBR_YEARS_EXP'], dtype='object')\nConnexion à Snowflake fermée.\n","output_type":"stream"},{"name":"stderr","text":"2024-12-09 20:51:10.084558: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-09 20:51:10.084628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-09 20:51:10.086483: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608965a2400c47f18bd8995e97c410eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc534c6fb0742e7a39765710c51b18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5a5dd6688f8438e8dd89d22532e7093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1235e7c933ba4505871b90ee2cc1baef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"507da3d1a456462098a18d346f578d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1008e598fb34fb0ae5e3dfad82f8adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219f5c8f6d7d43feb53c9a339c416511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdf5ac5a9d342b387dcc34e2ff163a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0093ddf350ae4992a9f8872ff7f7b423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ea51480f834d03a5b20451396b3e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68ddaf5a1f8d4680a9c05794092d564e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e5aa8570524281bf741d14c1fccf74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32639 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc91c09464a43fba06c4fd407a598c7"}},"metadata":{}},{"name":"stdout","text":"           OFFRE_ID                                        COMPETENCES  \\\n0   992312330492282  electronic health records (ehr), medication ad...   \n1  2725484753519878                        negotiation, sales strategy   \n2  2781263026180916  schema markup, website crawl analysis, xml sit...   \n3   546201581084317  art techniques, classroom management, lesson p...   \n4   985132019340003                    budgeting, contract negotiation   \n\n   LANGUES           FORMATION EXPERIENCE_DUR  \\\n0   french            agronomy  1 to 10 Years   \n1   french  energy engineering  4 to 14 Years   \n2   french            agronomy  0 to 10 Years   \n3   french           aerospace  0 to 13 Years   \n4  english            agronomy  2 to 14 Years   \n\n                                         description  \\\n0  electronic health records (ehr), medication ad...   \n1  negotiation, sales strategy, french, energy en...   \n2  schema markup, website crawl analysis, xml sit...   \n3  art techniques, classroom management, lesson p...   \n4  budgeting, contract negotiation, english, agro...   \n\n                                           embedding  \n0  [-0.05156804621219635, 0.013881652615964413, 0...  \n1  [-0.01942281611263752, 0.0338139533996582, 0.0...  \n2  [-0.015540128573775291, 0.036959242075681686, ...  \n3  [0.043583281338214874, -0.015960481017827988, ...  \n4  [0.032527823001146317, 0.01234655175358057, 0....  \n         CANDIDAT_ID                                        COMPETENCES  \\\n554339       1220519  environmental impact assessment, petroleum geo...   \n1602454        35828  c++, cybersecurity, data science, network secu...   \n561830        842236                                       broadcasting   \n691565       1747672  manufacturing systems, quality engineering, su...   \n1497105       540497  logistics strategy, procurement, transportatio...   \n\n                                 LANGUES               FORMATION  \\\n554339                           english   petroleum engineering   \n1602454                 english, spanish        computer science   \n561830                   english, french             audiovisual   \n691565   arabic, français, french, stage  industrial engineering   \n1497105                           french               logistics   \n\n         NBR_YEARS_EXP                                        description  \\\n554339              16  environmental impact assessment, petroleum geo...   \n1602454              7  c++, cybersecurity, data science, network secu...   \n561830              14     broadcasting, english, french, audiovisual, 14   \n691565              15  manufacturing systems, quality engineering, su...   \n1497105              0  logistics strategy, procurement, transportatio...   \n\n                                                 embedding  \n554339   [0.02378643862903118, -0.016589544713497162, 0...  \n1602454  [-0.02394198067486286, -0.08858750760555267, 0...  \n561830   [0.04431282356381416, -0.08915052562952042, 0....  \n691565   [-0.006844489835202694, -0.05697472393512726, ...  \n1497105  [0.020007222890853882, -0.03318599611520767, -...  \nL'index 'job_candidates' existe déjà.\nTous les embeddings ont été insérés dans Pinecone.\n","output_type":"stream"}],"execution_count":9}]}